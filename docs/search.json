[
  {
    "objectID": "00-fundamentals/index.html",
    "href": "00-fundamentals/index.html",
    "title": "Fundamentals",
    "section": "",
    "text": "Before jumping into network science details, we need to cover some fundamentals. I assume that most of the contents here are well known to you–we will be brief–but I want to ensure we are all on the same page."
  },
  {
    "objectID": "00-fundamentals/index.html#getting-help",
    "href": "00-fundamentals/index.html#getting-help",
    "title": "Fundamentals",
    "section": "1.1 Getting help",
    "text": "1.1 Getting help\nUnlike other languages, R’s documentation is highly reliable. The Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN must pass a series of tests to ensure the quality of the code, including the documentation.\nTo get help on a function, we can use the help() function. For example, if we wanted to get help on the mean() function, we would do:\n\nhelp(\"mean\")"
  },
  {
    "objectID": "00-fundamentals/index.html#naming-conventions",
    "href": "00-fundamentals/index.html#naming-conventions",
    "title": "Fundamentals",
    "section": "1.2 Naming conventions",
    "text": "1.2 Naming conventions\nR has a set of naming conventions that we should follow to avoid confusion. The most important ones are:\n\nUse lowercase letters (optional)\nUse underscores to separate words (optional)\nDo not start with a number\nDo not use special characters\nDo not use reserved words\n\n\n\n\n\n\n\nQuestion\n\n\n\nOf the following list, which are valid names and which are valid but to be avoided?\n_my.var\nmy.var\nmy_var\nmyVar\nmyVar1\n1myVar\nmy var\nmy-var"
  },
  {
    "objectID": "00-fundamentals/index.html#assignment",
    "href": "00-fundamentals/index.html#assignment",
    "title": "Fundamentals",
    "section": "1.3 Assignment",
    "text": "1.3 Assignment\nIn R, we have two (four) ways of assigning values to objects: the &lt;- and = binary operators2. Although both are equivalent, the former is the preferred way of assigning values to objects since the latter can be confused with function arguments.\n\nx &lt;- 1\nx = 1\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between the following two assignments? Use the help function to find out.\nx &lt;- 1\nx &lt;&lt;- 1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are other ways in which you can assign values to objects?"
  },
  {
    "objectID": "00-fundamentals/index.html#using-functions-and-piping",
    "href": "00-fundamentals/index.html#using-functions-and-piping",
    "title": "Fundamentals",
    "section": "1.4 Using functions and piping",
    "text": "1.4 Using functions and piping\nIn R, we use functions to perform operations on objects. Functions are implemented as function_name ( argument_1 , argument_2 , ... ). For example, the mean() function takes a vector of numbers and returns the mean of the values:\n\nx &lt;- c(1, 2, 3) # The c() function creates a vector\nmean(x)\n## [1] 2\n\nFurthermore, we can use the pipe operator (|&gt;) to improve readability. The pipe operator takes the output of the left-hand side expression and passes it as the first argument of the right-hand side expression. Our previous example could be rewritten as:\n\nc(1, 2, 3) |&gt; mean()\n## [1] 2"
  },
  {
    "objectID": "00-fundamentals/index.html#data-structures",
    "href": "00-fundamentals/index.html#data-structures",
    "title": "Fundamentals",
    "section": "1.5 Data structures",
    "text": "1.5 Data structures\nAtomic types are the minimal building blocks of R. They are logical, integer, double, character, complex, raw:\n\nx_logical   &lt;- TRUE\nx_integer   &lt;- 1L\nx_double    &lt;- 1.0\nx_character &lt;- \"a\"\nx_complex   &lt;- 1i\nx_raw       &lt;- charToRaw(\"a\")\n\nUnlike other languages, we do not need to declare the data type before creating the object; R will infer it from the value.\n\n\n\n\n\n\nPro-tip\n\n\n\nAdding the L suffix to the value is good practice when dealing with integers. Some R packages like data.table (Barrett, Dowle, and Srinivasan 2023) have internal checks that will throw an error if you are not explicit about the data type.\n\n\nThe next type is the vector. A vector is a collection of elements of the same type. The most common way to create a vector is with the c() function:\n\nx_integer &lt;- c(1, 2, 3)\nx_double  &lt;- c(1.0, 2.0, 3.0)\nx_logical &lt;- c(TRUE, FALSE, TRUE)\n# etc.\n\nR will coerce the data types to the most general type. For example, if we mix integers and doubles, R will coerce the integers into doubles. The coercion order is logical &lt; integer &lt; double &lt; character\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is the coercion order logical &lt; integer &lt; double &lt; character?\n\n\nThe next data structure is the list. A list is a collection of elements of any type. We can create a list with the list() function:\n\nx_list       &lt;- list(1, 2.0, TRUE, \"a\")\nx_list_named &lt;- list(a = 1, b = 2.0, c = TRUE, d = \"a\")\n\nTo access elements in a list, we have two options: by position or by name, the latter only if the elements are named:\n\nx_list[[1]]\n## [1] 1\nx_list_named[[\"a\"]]\n## [1] 1\nx_list_named$a\n## [1] 1\n\nAfter lists, we have matrices. A matrix is a collection of elements of the same type arranged in a two-dimensional grid. We can create a matrix with the matrix() function:\n\nx_matrix &lt;- matrix(1:9, nrow = 3, ncol = 3)\nx_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n\n# We can access elements in a matrix by row column, or position:\nx_matrix[1, 2]\n## [1] 4\nx_matrix[cbind(1, 2)]\n## [1] 4\nx_matrix[4]\n## [1] 4\n\n\n\n\n\n\n\nMatrix is a vector\n\n\n\nMatrices in R are vectors with dimensions. In base R, matrices are stored in column-major order. This means that the elements are stored column by column. This is important to know when we are accessing elements in a matrix\n\n\nThe two last data structures are arrays and data frames. An array is a collection of elements of the same type arranged in a multi-dimensional grid. We can create an array with the array() function:\n\nx_array &lt;- array(1:27, dim = c(3, 3, 3))\n\n# We can access elements in an array by row, column, and dimension, or\n# position:\nx_array[1, 2, 3]\n## [1] 22\nx_array[cbind(1, 2, 3)]\n## [1] 22\nx_array[22]\n## [1] 22\n\nData frames are the most common data structure in R. In principle, these objects are lists of vectors of the same length, each vector representing a column. Columns (lists) in data frames can be of different types, but elements in each column must be of the same type. We can create a data frame with the data.frame() function:\n\nx_data_frame &lt;- data.frame(\n  a = 1:3,\n  b = c(\"a\", \"b\", \"c\"),\n  c = c(TRUE, FALSE, TRUE)\n)\n\n# We can access elements in a data frame by row, column, or position:\nx_data_frame[1, 2]\n## [1] \"a\"\nx_data_frame[cbind(1, 2)]\n## [1] \"a\"\nx_data_frame$b[1]    # Like a list\n## [1] \"a\"\nx_data_frame[[2]][1] # Like a list too\n## [1] \"a\""
  },
  {
    "objectID": "00-fundamentals/index.html#functions",
    "href": "00-fundamentals/index.html#functions",
    "title": "Fundamentals",
    "section": "1.6 Functions",
    "text": "1.6 Functions\nFunctions are the most important building blocks of R. A function is a set of instructions that takes one or more inputs and returns one or more outputs. We can create a function with the function() function:\n\n# This function has two arguments (y is optional)\nf &lt;- function(x, y = 1) {\n  x + 1\n}\n\nf(1)\n## [1] 2\n\nStarting with R 4, we can use the lambda syntax to create functions:\n\nf &lt;- \\(x, y) x + 1\n\nf(1)\n## [1] 2"
  },
  {
    "objectID": "00-fundamentals/index.html#control-flow",
    "href": "00-fundamentals/index.html#control-flow",
    "title": "Fundamentals",
    "section": "1.7 Control flow",
    "text": "1.7 Control flow\nControl flow statements allow us to control the execution of the code. The most common control flow statements are if, for, while, and repeat. We can create a control flow statement with the if(), for(), while(), and repeat() functions:\n\n# if\nif (TRUE) {\n  \"a\"\n} else {\n  \"b\"\n}\n## [1] \"a\"\n\n# for\nfor (i in 1:3) {\n  cat(\"This is the number \", i, \"\\n\")\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n# while\ni &lt;- 1\nwhile (i &lt;= 3) {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n# repeat\ni &lt;- 1\nrepeat {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n  if (i &gt; 3) {\n    break\n  }\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3"
  },
  {
    "objectID": "00-fundamentals/index.html#r-packages",
    "href": "00-fundamentals/index.html#r-packages",
    "title": "Fundamentals",
    "section": "1.8 R packages",
    "text": "1.8 R packages\nR is so powerful because of its extensions. R extensions (different from other programming languages) are called packages. Packages are collections of functions, data, and documentation that provide additional functionality to R. Although anyone can create and distribute R packages to other users, the Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN are thoroughly tested, so generally, their quality is high.\nTo install R packages, we use the install.packages() function; to load them, we use the library() function. For example, the following code chunk installs the ergm package and loads it:\n\ninstall.packages(\"ergm\")\nlibrary(ergm)"
  },
  {
    "objectID": "00-fundamentals/index.html#hypothesis-testing",
    "href": "00-fundamentals/index.html#hypothesis-testing",
    "title": "Fundamentals",
    "section": "2.1 Hypothesis testing",
    "text": "2.1 Hypothesis testing\nAccording to Wikipedia\n\nA statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis. More generally, hypothesis testing allows us to make probabilistic statements about population parameters. More informally, hypothesis testing is the processes of making decisions under uncertainty. Typically, hypothesis testing procedures involve a user selected tradeoff between false positives and false negatives. – Wiki\n\nIn a nutshell, hypothesis testing is performed by following these steps:\n\nState the null and alternative hypotheses. In general, the null hypothesis is a statement about the population parameter that challenges our research question; for example, given the question of whether two networks are different, the null hypothesis would be that the two networks are the same.\nCompute the corresponding test statistic. It is a data function that reduces the information to a single number.\nCompare the observed test statistic with the distribution of the test statistic under the null hypothesis. The sometimes infamous p-value: ``[…] the probability that the chosen test statistic would have been at least as large as its observed value if every model assumption were correct, including the test hypothesis.’’ (Greenland et al. 2016) 3\n\n\nReport the observed effect and p-value, i.e., \\Pr(t \\in H_0)\n\nWe usually say that we either reject the null hypothesis or fail to reject it (we never accept the null hypothesis,) but, in my view, it is always better to talk about it in terms of “suggests evidence for” or “suggests evidence against.”\nWe will illustrate statistical concepts more concretely in the next section."
  },
  {
    "objectID": "00-fundamentals/index.html#probability-distributions",
    "href": "00-fundamentals/index.html#probability-distributions",
    "title": "Fundamentals",
    "section": "3.1 Probability distributions",
    "text": "3.1 Probability distributions\nR has a standard way of naming probability functions. The naming structure is [type of function][distribution], where [type of function] can be d for density, p for cumulative distribution function, q for quantile function, and r for random generation. For example, the normal distribution has the following functions:\n\ndnorm(0, mean = 0, sd = 1)\n## [1] 0.3989423\npnorm(0, mean = 0, sd = 1)\n## [1] 0.5\nqnorm(0.5, mean = 0, sd = 1)\n## [1] 0\n\nNow, if we wanted to know what is the probability of observing a value smaller than -2 comming from a standard normal distribution, we would do:\n\npnorm(-2, mean = 0, sd = 1)\n## [1] 0.02275013\n\nCurrently, R has a wide range of probability distributions implemented.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many probability distributions are implemented in R’s stats package?"
  },
  {
    "objectID": "00-fundamentals/index.html#random-number-generation",
    "href": "00-fundamentals/index.html#random-number-generation",
    "title": "Fundamentals",
    "section": "3.2 Random number generation",
    "text": "3.2 Random number generation\nRandom numbers, and more precisely, pseudo-random numbers, are a vital component of statistical programming. Pure randomness is hard to come by, and so we rely on pseudo-random number generators (PRNGs) to generate random numbers. These generators are deterministic algorithms that produce sequences of numbers we can then use to generate random samples from probability distributions. Because of the latter, PRNGs need a starting point called the seed. As a statistical computing program, R has a variety of PRNGs. As suggested in the previous subsection, we can generate random numbers from a probability distribution with the r function. In what follows, we will draw random numbers from a few distributions and plot histograms of the results:\n\nset.seed(1)\n\n# Saving the current graphical parameters\nop &lt;- par(mfrow = c(2,2))\nrnorm(1000) |&gt; hist(main = \"Normal distribution\")\nrunif(1000) |&gt; hist(main = \"Uniform distribution\")\nrpois(1000, lambda = 1) |&gt; hist(main = \"Poisson distribution\")\nrbinom(1000, size = 10, prob = 0.1) |&gt; hist(main = \"Binomial distribution\")\n\n\n\npar(op)"
  },
  {
    "objectID": "00-fundamentals/index.html#simulations-and-sampling",
    "href": "00-fundamentals/index.html#simulations-and-sampling",
    "title": "Fundamentals",
    "section": "3.3 Simulations and sampling",
    "text": "3.3 Simulations and sampling\nSimulations are front and center in statistical programming. We can use them to test the properties of statistical methods, generate data, and perform statistical inference. The following example uses the sample function in R to compute the bootstrap standard error of the mean (see Casella and Berger 2021):\n\nset.seed(1)\nx &lt;- rnorm(1000)\n\n# Bootstrap standard error of the mean\nn &lt;- length(x)\nB &lt;- 1000\n\n# We will store the results in a vector\nres &lt;- numeric(B)\n\nfor (i in 1:B) {\n  # Sample with replacement\n  res[i] &lt;- sample(x, size = n, replace = TRUE) |&gt;\n    mean()\n}\n\n# Plot the results\nhist(res, main = \"Bootstrap standard error of the mean\")\n\n\n\n\nSince the previous example is rather extensive, let us review it in detail.\n\nset.seed(1) sets the seed of the PRNG to 1. It ensures we get the same results every time we run the code.\nrnorm() generates a sample of 1,000 standard-normal values.\nn &lt;- length(x) stores the length of the vector in the n variable.\nB &lt;- 1000 stores the number of bootstrap samples in the B variable.\nres &lt;- numeric(B) creates a vector of length B to store the results.\nfor (i in 1:B) is a for loop that iterates from 1 to B.\nres[i] &lt;- sample(x, size = n, replace = TRUE) |&gt; mean() samples n values from x with replacement and computes the mean of the sample.\nThe pipe operator (|&gt;) passes the output of the left-hand side expression as the first argument of the right-hand side expression.\nhist(res, main = \"Bootstrap standard error of the mean\") plots the histogram of the results.\n\n\n\n\n\n\n\nQuestion\n\n\n\nSimulating convolutions: Using what you have learned about statistical functions in R, simulate the convolution of two normal distributions, one with (\\mu, \\sigma^2) = (-3, 1) and the other with (\\mu, \\sigma^2) = (2, 2). Plot the histogram of the results. Draw 1,000 samples.\n\n\nCode\nset.seed(1)\nx &lt;- rnorm(1000, mean = -3, sd = 1)\ny &lt;- rnorm(1000, mean = 2, sd = 2)\nz &lt;- x + y\n\nhist(z)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nBimodal distribution: Using the previous two normal distributions, simulate a bimodal distribution where the probability of sampling from the first distribution is 0.3 and the probability of sampling from the second distribution is 0.7. Plot the histogram of the results. (Hint: use a combination of runif() and ifelse()).\n\n\nCode\nz &lt;- ifelse(runif(1000) &lt; 0.3, x, y)\ndensity(z) |&gt; plot()"
  },
  {
    "objectID": "00-fundamentals/index.html#footnotes",
    "href": "00-fundamentals/index.html#footnotes",
    "title": "Fundamentals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough, not for network science in general.↩︎\nIn mathematics and computer science, a binary operator is a function that takes two arguments. In R, binary operators are implemented as variable 1 [operator] variable 2. For example, 1 + 2 is a binary operation.↩︎\nThe discussion about interpreting p-values and hypothesis testing is vast and relevant. Although we will not review this here, I recommend looking into the work of Andrew Gelman Gelman (2018).↩︎"
  },
  {
    "objectID": "01-intro-to-ergms/index.html",
    "href": "01-intro-to-ergms/index.html",
    "title": "Introduction to ERGMs",
    "section": "",
    "text": "Have you ever wondered if the friend of your friend is your friend? Or if the people you consider to be your friends feel the same about you? Or if age is related to whether you seek advice from others? All these (and many others certainly more creative) questions can be answered using Exponential-Family Random Graph Models."
  },
  {
    "objectID": "01-intro-to-ergms/index.html#the-logistic-distribution",
    "href": "01-intro-to-ergms/index.html#the-logistic-distribution",
    "title": "Introduction to ERGMs",
    "section": "3.1 The logistic distribution",
    "text": "3.1 The logistic distribution\nLet’s start by stating the result: Conditioning on all graphs that are not y_{ij}, the probability of a tie Y_{ij} is distributed Logistic; formally:\n\nP_{\\mathcal{Y}, \\bm{\\theta}}(Y_{ij}=1 | \\bm{y}_{-ij}) = \\frac{1}{1 + \\exp \\left(\\bm{\\theta}^{\\mathbf{t}}\\delta_{ij}(\\bm{y}){}\\right)},\n\nwhere \\delta_{ij}(\\bm{y}){}\\equiv s_{ij}^+(\\bm{y}) - s_{ij}^-(\\bm{y}) is the change statistic, and s_{ij}^+(\\bm{y}) and s_{ij}^-(\\bm{y}) are the statistics of the graph with and without the tie Y_{ij}, respectively.\nThe importance of this result is two-fold: (a) we can use this equation to interpret fitted models in the context of a single graph (like using odds,) and (b) we can use this equation to simulate from the model, without touching the normalizing constant."
  },
  {
    "objectID": "01-intro-to-ergms/index.html#the-ratio-of-loglikelihoods",
    "href": "01-intro-to-ergms/index.html#the-ratio-of-loglikelihoods",
    "title": "Introduction to ERGMs",
    "section": "3.2 The ratio of loglikelihoods",
    "text": "3.2 The ratio of loglikelihoods\nThe second significant result is that the ratio of loglikelihoods can be approximated through simulation. It is based on the following observation by Geyer and Thompson (1992):\n\n\\frac{\\kappa(\\bm{\\theta})}{\\kappa(\\bm{\\theta}_0)} = \\mathbb{E}_{\\mathcal{Y}, \\bm{\\theta}_0}\\left((\\bm{\\theta} - \\bm{\\theta}_0)s(\\bm{y})^{\\mathbf{t}}\\right),\n\nUsing the latter, we can approximate the following loglikelihood ratio:\n\\begin{align*}\nl(\\bm{\\theta}) - l(\\bm{\\theta}_0) = & (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}) - \\log\\left[\\frac{\\kappa(\\bm{\\theta})}{\\kappa(\\bm{\\theta}_0)}\\right]\\\\\n\\approx & (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}) - \\log\\left[M^{-1}\\sum_{\\bm{y}^{(m)}} (\\bm{\\theta} - \\bm{\\theta}_0)^{\\mathbf{t}}s(\\bm{y}^{(m)})\\right]\n\\end{align*}\nWhere \\bm{\\theta}_0 is an arbitrary vector of parameters, and \\bm{y}^{(m)} are sampled from the distribution P_{\\mathcal{Y}, \\bm{\\theta}_0}. In the words of Geyer and Thompson (1992), “[…] it is possible to approximate \\bm{\\theta} by using simulations from one distribution P_{\\mathcal{Y}, \\bm{\\theta}_0} no matter which \\bm{\\theta}_0 in the parameter space is.” This result has been key for the MC-MLE algorithm (see Hunter, Krivitsky, and Schweinberger (2012)), which has been front and center in the ergm R package.2"
  },
  {
    "objectID": "01-intro-to-ergms/index.html#inspect-the-data",
    "href": "01-intro-to-ergms/index.html#inspect-the-data",
    "title": "Introduction to ERGMs",
    "section": "4.1 Inspect the data",
    "text": "4.1 Inspect the data\nFor the sake of time, we will not take the time to investigate our network properly. However, you should always do so. Make sure you do descriptive statistics (density, centrality, modularity, etc.), check missing values, isolates (disconnected nodes), and inspect your data visually through “notepad” and visualizations before jumping into your ERG model."
  },
  {
    "objectID": "01-intro-to-ergms/index.html#start-with-endogenous-effects-first",
    "href": "01-intro-to-ergms/index.html#start-with-endogenous-effects-first",
    "title": "Introduction to ERGMs",
    "section": "4.2 Start with endogenous effects first",
    "text": "4.2 Start with endogenous effects first\nThe step is to check whether we can fit an ERGM or not. We can do so with the Bernoulli graph:\n\nmodel_1 &lt;- ergm(Y ~ edges)\nsummary(model_1)\n\nCall:\nergm(formula = Y ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.78885    0.06833      0  -55.45   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 13724  on 9900  degrees of freedom\n Residual Deviance:  2102  on 9899  degrees of freedom\n \nAIC: 2104  BIC: 2112  (Smaller is better. MC Std. Err. = 0)\n\n\nIt is rare to see a model in which the edgecount is not significant. The next term we will add is reciprocity (mutual in the ergm package)\n\nmodel_2 &lt;- ergm(Y ~ edges + mutual) \nsummary(model_2)\n## Call:\n## ergm(formula = Y ~ edges + mutual)\n## \n## Monte Carlo Maximum Likelihood Results:\n## \n##        Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \n## edges  -3.93277    0.07833      0 -50.205   &lt;1e-04 ***\n## mutual  2.15152    0.31514      0   6.827   &lt;1e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 13724  on 9900  degrees of freedom\n##  Residual Deviance:  2064  on 9898  degrees of freedom\n##  \n## AIC: 2068  BIC: 2083  (Smaller is better. MC Std. Err. = 0.8083)\n\nAs expected, reciprocity is significant (we made it like this!.) Notwithstanding, there is a difference between this model and the previous one. This model was not fitted using MLE. Instead, since the reciprocity term involves more than one tie, the model cannot be reduced to a Logistic regression, so it needs to be estimated using one of the other available estimation methods in the ergm package.\nThe model starts gaining complexity as we add higher-order terms involving more ties. An infamous example is the number of triangles. Although highly important for social sciences, including triangle terms in your ERGMs results in a degenerate model (when the MCMC chain jumps between empty and fully connected graphs). One exception is if you deal with small networks. To address this, Snijders et al. (2006) and Hunter (2007) introduced various new terms that significantly reduce the risk of degeneracy. Here, we will illustrate the use of the term “geometrically weighted dyadic shared partner” (gwdsp,) which Prof. David Hunter proposed. The gwdsp term is akin to triadic closure but reduces the chances of degeneracy.\n\n# Fitting two more models (output message suppressed)\nmodel_3 &lt;- ergm(Y ~ edges + mutual + gwdsp(.5, fixed = TRUE))\n# model_4 &lt;- ergm(Y ~ edges + triangles) # bad idea\n\nRight after fitting a model, we want to inspect the results. An excellent tool for this is the R package texreg (Leifeld 2013):\nlibrary(texreg)\nknitreg(list(model_1, model_2, model_3))\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\nedges\n\n\n-3.79***\n\n\n-3.93***\n\n\n-3.84***\n\n\n\n\n \n\n\n(0.07)\n\n\n(0.08)\n\n\n(0.21)\n\n\n\n\nmutual\n\n\n \n\n\n2.15***\n\n\n2.14***\n\n\n\n\n \n\n\n \n\n\n(0.32)\n\n\n(0.29)\n\n\n\n\ngwdsp.OTP.fixed.0.5\n\n\n \n\n\n \n\n\n-0.02\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.05)\n\n\n\n\nAIC\n\n\n2104.43\n\n\n2068.22\n\n\n2071.94\n\n\n\n\nBIC\n\n\n2111.63\n\n\n2082.62\n\n\n2093.54\n\n\n\n\nLog Likelihood\n\n\n-1051.22\n\n\n-1032.11\n\n\n-1032.97\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nSo far, model_2 is winning. We will continue with this model."
  },
  {
    "objectID": "01-intro-to-ergms/index.html#lets-add-a-little-bit-of-structure",
    "href": "01-intro-to-ergms/index.html#lets-add-a-little-bit-of-structure",
    "title": "Introduction to ERGMs",
    "section": "4.3 Let’s add a little bit of structure",
    "text": "4.3 Let’s add a little bit of structure\nNow that we only have a model featuring endogenous terms, we can add vertex/edge-covariates. Starting with \"fav_music\", there are a couple of different ways to use this node feature:\n\nDirectly through homophily (assortative mixing): Using the nodematch term, we can control for the propensity of individuals to connect based on shared music taste.\nHomophily (v2): We could activate the option diff = TRUE using the same term. By doing this, the homophily term is operationalized differently, adding as many terms as options in the vertex attribute.\nMixing: We can use the term nodemix for individuals’ tendency to mix between musical tastes.\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn this context, what would be the different hypotheses behind each decision?\n\n\n\nmodel_4 &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\"))\nmodel_5 &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\", diff = TRUE))\nmodel_6 &lt;- ergm(Y ~ edges + mutual + nodemix(\"fav_music\"))\n\nNow, let’s inspect what we have so far:\nknitreg(list(`2` = model_2, `4` = model_4, `5` = model_5, `6` = model_6))\n\n\n\nStatistical models\n\n\n\n\n \n\n\n2\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\nedges\n\n\n-3.93***\n\n\n-4.29***\n\n\n-4.29***\n\n\n-3.56***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.11)\n\n\n(0.11)\n\n\n(0.24)\n\n\n\n\nmutual\n\n\n2.15***\n\n\n1.99***\n\n\n2.00***\n\n\n2.02***\n\n\n\n\n \n\n\n(0.32)\n\n\n(0.30)\n\n\n(0.30)\n\n\n(0.30)\n\n\n\n\nnodematch.fav_music\n\n\n \n\n\n0.85***\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n(0.14)\n\n\n \n\n\n \n\n\n\n\nnodematch.fav_music.jazz\n\n\n \n\n\n \n\n\n0.74**\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.25)\n\n\n \n\n\n\n\nnodematch.fav_music.pop\n\n\n \n\n\n \n\n\n0.82***\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.18)\n\n\n \n\n\n\n\nnodematch.fav_music.rock\n\n\n \n\n\n \n\n\n0.87***\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.17)\n\n\n \n\n\n\n\nmix.fav_music.pop.jazz\n\n\n \n\n\n \n\n\n \n\n\n-0.54\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.34)\n\n\n\n\nmix.fav_music.rock.jazz\n\n\n \n\n\n \n\n\n \n\n\n-0.75*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.38)\n\n\n\n\nmix.fav_music.jazz.pop\n\n\n \n\n\n \n\n\n \n\n\n-0.60\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.35)\n\n\n\n\nmix.fav_music.pop.pop\n\n\n \n\n\n \n\n\n \n\n\n0.10\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.27)\n\n\n\n\nmix.fav_music.rock.pop\n\n\n \n\n\n \n\n\n \n\n\n-0.50\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.31)\n\n\n\n\nmix.fav_music.jazz.rock\n\n\n \n\n\n \n\n\n \n\n\n-1.40**\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.44)\n\n\n\n\nmix.fav_music.pop.rock\n\n\n \n\n\n \n\n\n \n\n\n-0.86*\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.33)\n\n\n\n\nmix.fav_music.rock.rock\n\n\n \n\n\n \n\n\n \n\n\n0.15\n\n\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n(0.27)\n\n\n\n\nAIC\n\n\n2068.22\n\n\n2030.85\n\n\n2033.57\n\n\n2036.37\n\n\n\n\nBIC\n\n\n2082.62\n\n\n2052.45\n\n\n2069.57\n\n\n2108.38\n\n\n\n\nLog Likelihood\n\n\n-1032.11\n\n\n-1012.42\n\n\n-1011.79\n\n\n-1008.19\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\nAlthough model 5 has a higher loglikelihood, using AIC or BIC suggests model 4 is a better candidate. For the sake of time, we will jump ahead and add nodematch(\"female\") as the last term of our model. The next step is to assess (a) convergence and (b) goodness-of-fit.\nmodel_final &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\") + nodematch(\"female\"))\n\n# Printing the pretty table\nknitreg(list(`2` = model_2, `4` = model_4, `Final` = model_final))\n\n\n\nStatistical models\n\n\n\n\n \n\n\n2\n\n\n4\n\n\nFinal\n\n\n\n\n\n\nedges\n\n\n-3.93***\n\n\n-4.29***\n\n\n-3.95***\n\n\n\n\n \n\n\n(0.08)\n\n\n(0.11)\n\n\n(0.12)\n\n\n\n\nmutual\n\n\n2.15***\n\n\n1.99***\n\n\n1.86***\n\n\n\n\n \n\n\n(0.32)\n\n\n(0.30)\n\n\n(0.33)\n\n\n\n\nnodematch.fav_music\n\n\n \n\n\n0.85***\n\n\n0.81***\n\n\n\n\n \n\n\n \n\n\n(0.14)\n\n\n(0.14)\n\n\n\n\nnodematch.female\n\n\n \n\n\n \n\n\n-0.74***\n\n\n\n\n \n\n\n \n\n\n \n\n\n(0.15)\n\n\n\n\nAIC\n\n\n2068.22\n\n\n2030.85\n\n\n2002.18\n\n\n\n\nBIC\n\n\n2082.62\n\n\n2052.45\n\n\n2030.98\n\n\n\n\nLog Likelihood\n\n\n-1032.11\n\n\n-1012.42\n\n\n-997.09\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05"
  },
  {
    "objectID": "01-intro-to-ergms/index.html#evaluate-your-results",
    "href": "01-intro-to-ergms/index.html#evaluate-your-results",
    "title": "Introduction to ERGMs",
    "section": "4.4 Evaluate your results",
    "text": "4.4 Evaluate your results\n\n4.4.1 Convergence\nAs our model was fitted using MCMC, we must ensure the chains converged. We can use the mcmc.diagnostics function from the ergm package to check model convergence. This function looks at the last set of simulations of the MCMC model and generates various diagnostics for the user.\nUnder the hood, the fitting algorithm generates a stream of networks based on those parameters for each new proposed set of model parameters. The last stream of networks is thus simulated using the final state of the model. The mcmc.diagnostics function takes that stream of networks and plots the sequence of the sufficient statistics included in the model. A converged model should show a stationary statistics sequence, moving around a fixed value without (a) becoming stuck at any point and (b) chaining the tendency. This model shows both:\n\nmcmc.diagnostics(model_final, which   = c(\"plots\"))\n\n\n\n\n\n\n\n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nNow that we know our model was good enough to represent the observed statistics (sample them, actually,) let’s see how good it is at capturing other features of the network that were not included in the model.\n\n\n4.4.2 Goodness-of-fit\nThis would be the last step in the sequence of steps to fit an ERGM. As we mentioned before, the idea of Goodness-of-fit [GOF] in ERG models is to see how well our model captures other properties of the graph that were not included in the model. By default, the gof function from the ergm package computes GOF for:\n\nThe model statistics.\nThe outdegree distribution.\nThe indegree distribution.\nThe distribution of edge-wise shared partners.\nThe distribution of the geodesic distances (shortest path).\n\nThe process of evaluating GOF is relatively straightforward. Using samples from the posterior distribution, we check whether the observed statistics from above are covered (fall within the CI) of our model. If they do, we say that the model has a good fit. Otherwise, if we observe significant anomalies, we return to the bench and try to improve our model.\nAs with all simulated data, our gof() call shows that our selected model was an excellent choice for the observed graph:\n\ngof_final &lt;- gof(model_final)\nprint(gof_final)\n## \n## Goodness-of-fit for in-degree \n## \n##           obs min  mean max MC p-value\n## idegree0   11   4 11.00  25       1.00\n## idegree1   22  16 23.86  33       0.74\n## idegree2   23  19 26.43  41       0.56\n## idegree3   30  11 20.47  33       0.08\n## idegree4   10   5 10.81  19       0.88\n## idegree5    3   1  4.66  12       0.70\n## idegree6    1   0  1.91   7       0.86\n## idegree7    0   0  0.60   4       1.00\n## idegree8    0   0  0.18   3       1.00\n## idegree9    0   0  0.06   1       1.00\n## idegree10   0   0  0.01   1       1.00\n## idegree12   0   0  0.01   1       1.00\n## \n## Goodness-of-fit for out-degree \n## \n##           obs min  mean max MC p-value\n## odegree0   10   4 10.75  19       0.90\n## odegree1   30  12 24.16  33       0.22\n## odegree2   23  18 26.76  40       0.52\n## odegree3   17  12 19.71  29       0.64\n## odegree4   10   5 11.05  19       0.80\n## odegree5    8   0  4.89  10       0.22\n## odegree6    2   0  1.96   6       1.00\n## odegree7    0   0  0.52   2       1.00\n## odegree8    0   0  0.15   2       1.00\n## odegree9    0   0  0.04   1       1.00\n## odegree11   0   0  0.01   1       1.00\n## \n## Goodness-of-fit for edgewise shared partner \n## \n##          obs min   mean max MC p-value\n## esp.OTP0 212 180 210.99 240       0.96\n## esp.OTP1   7   3  11.27  24       0.40\n## esp.OTP2   0   0   0.27   2       1.00\n## \n## Goodness-of-fit for minimum geodesic distance \n## \n##      obs  min    mean  max MC p-value\n## 1    219  184  222.53  264       0.96\n## 2    449  320  469.83  626       0.76\n## 3    790  507  863.71 1284       0.68\n## 4   1151  688 1277.86 2016       0.76\n## 5   1245  767 1409.86 2122       0.56\n## 6   1043  698 1163.97 1546       0.46\n## 7    745  347  771.20 1119       0.86\n## 8    434   87  442.32  810       1.00\n## 9    198   19  231.86  631       0.88\n## 10    80    2  115.88  435       0.92\n## 11    10    0   58.27  328       0.50\n## 12     1    0   29.23  204       0.54\n## 13     0    0   14.39  136       0.80\n## 14     0    0    6.85  104       1.00\n## 15     0    0    3.26   76       1.00\n## 16     0    0    1.51   48       1.00\n## 17     0    0    0.72   29       1.00\n## 18     0    0    0.34   18       1.00\n## 19     0    0    0.16   11       1.00\n## 20     0    0    0.08    8       1.00\n## 21     0    0    0.05    5       1.00\n## 22     0    0    0.02    2       1.00\n## 23     0    0    0.01    1       1.00\n## Inf 3535 1150 2816.09 5550       0.40\n## \n## Goodness-of-fit for model statistics \n## \n##                     obs min   mean max MC p-value\n## edges               219 184 222.53 264       0.96\n## mutual               16  10  16.88  26       0.88\n## nodematch.fav_music 123  90 124.93 165       0.90\n## nodematch.female     72  54  71.71  97       1.00\n\nIt is easier to see the results using the plot function:\n\n# Plotting the result (5 figures)\nop &lt;- par(mfrow = c(3,2))\nplot(gof_final)\npar(op)"
  },
  {
    "objectID": "01-intro-to-ergms/index.html#footnotes",
    "href": "01-intro-to-ergms/index.html#footnotes",
    "title": "Introduction to ERGMs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile ERG Models can be used to predict individual ties (which is another way of describing them), the focus is on the processes that give origin to the network.↩︎\nAlthough later versions of the ergm package use the contrastive divergence by Krivitsky (2017).↩︎"
  },
  {
    "objectID": "02-advanced-ergms/index.html",
    "href": "02-advanced-ergms/index.html",
    "title": "Advanced ERGMs: Constraints",
    "section": "",
    "text": "For this section, we will divein into ERGM constranints. Using constraints, you will be able to modify the sampling space of the model to things such as:\n\nPool (multilevel) models.\nAccounting for data generating process.\nMake your model behave (with caution.)\nEven fit Discrete-Exponential Family Models (DEFM.)\n\nWe will start with formally understanding what does constraining the space mean and then continue with a couple of examples."
  },
  {
    "objectID": "02-advanced-ergms/index.html#footnotes",
    "href": "02-advanced-ergms/index.html#footnotes",
    "title": "Advanced ERGMs: Constraints",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Laura Koehly who came up with this complicated model.↩︎\nAfter writing this example, it became apparent the use of the Log() transformation function may not be ideal. Since many terms used in ERGMs can be zero, e.g., triangles, the term Log(~ ostar(2)) is undefined when ostar(2) = 0. In practice, the ERGM package sets a lower limit for the log of 0, so, instead of having Log(0) ~ -Inf, they set it to be a really large negative number. This causes all sorts of issues to the estimates; in our example, an overestimation of the population parameter and a positive log-likelihood. Therefore, I wouldn’t recommend using this transformation too often.↩︎"
  },
  {
    "objectID": "03-new-topics/index.html#overview",
    "href": "03-new-topics/index.html#overview",
    "title": "New topics in network modeling",
    "section": "Overview",
    "text": "Overview\n\nWith more data and computing resources, the things that we can ask and do with networks are becoming increasingly (even more) exciting and complex.\nIn this section, I will introduce some of the latest advancements and forthcoming topics in network modeling."
  },
  {
    "objectID": "03-new-topics/index.html#mutli-ergms",
    "href": "03-new-topics/index.html#mutli-ergms",
    "title": "New topics in network modeling",
    "section": "Mutli-ERGMs",
    "text": "Mutli-ERGMs\n\n\n\nIn Krivitsky, Coletti, and Hens (2023a), the authors present a start-to-finish pooled ERGM example featuring heterogeneous data sources.\nThey increase power and allow exploring heterogeneous effects across types/classes of networks."
  },
  {
    "objectID": "03-new-topics/index.html#statistical-power-of-soam",
    "href": "03-new-topics/index.html#statistical-power-of-soam",
    "title": "New topics in network modeling",
    "section": "Statistical power of SOAM",
    "text": "Statistical power of SOAM\n\n\nSOAM Stadtfeld et al. (2020) proposes ways to perform power analysis for Siena models. At the center of their six-step approach is simulation."
  },
  {
    "objectID": "03-new-topics/index.html#bayesian-alaam",
    "href": "03-new-topics/index.html#bayesian-alaam",
    "title": "New topics in network modeling",
    "section": "Bayesian ALAAM",
    "text": "Bayesian ALAAM\nEver wondered how to model influence exclusively?\n\n\n\nThe Auto-Logistic Actor Attribute Model [ALAAM] is a model that allows us to do just that.\nKoskinen and Daraganova (2022) extends the ALAAM model to a Bayesian framework.\n\n\n\nIt provides greater flexibility to accommodate more complicated models and add extensions such as hierarchical models."
  },
  {
    "objectID": "03-new-topics/index.html#relational-event-models",
    "href": "03-new-topics/index.html#relational-event-models",
    "title": "New topics in network modeling",
    "section": "Relational Event Models",
    "text": "Relational Event Models\n\nREMs are great for modeling sequences of ties (instead of panel or cross-sectional.)\nButts et al. (2023) provides a general overview of Relational Event Models [REMs,] new methods, and future steps.\n\n\nFigure 3 reproduced from Brandenberger (2020)"
  },
  {
    "objectID": "03-new-topics/index.html#big-ergms",
    "href": "03-new-topics/index.html#big-ergms",
    "title": "New topics in network modeling",
    "section": "Big ERGMs",
    "text": "Big ERGMs\n\n\n\nERGMs In A. Stivala, Robins, and Lomi (2020), a new method is proposed to estimate large ERGMs (featuring millions of nodes).\n\n\n\n\n\nPartial map of the Internet based on the January 15, 2005 data found on opte.org. – Wiki"
  },
  {
    "objectID": "03-new-topics/index.html#exponential-random-network-models",
    "href": "03-new-topics/index.html#exponential-random-network-models",
    "title": "New topics in network modeling",
    "section": "Exponential Random Network Models",
    "text": "Exponential Random Network Models\n\nWang, Fellows, and Handcock recently published a re-introduction of the ERNM framework (Wang, Fellows, and Handcock 2023).\nERNMs generalize ERGMs to incorporate behavior and are the cross-sectional causing of SIENA models.\n\n\\begin{align*}\n\\text{ER\\textbf{G}M}: & P_{\\mathcal{Y}, \\bm{\\theta}}(\\bm{Y}=\\bm{y} | \\bm{X}=\\bm{x}) \\\\\n\\text{ER\\textbf{N}M}: & P_{\\mathcal{Y}, \\bm{\\theta}}(\\bm{Y}=\\bm{y}, \\bm{X}=\\bm{x})\n\\end{align*}"
  },
  {
    "objectID": "03-new-topics/index.html#ergmitos-small-ergms",
    "href": "03-new-topics/index.html#ergmitos-small-ergms",
    "title": "New topics in network modeling",
    "section": "ERGMitos: Small ERGMs",
    "text": "ERGMitos: Small ERGMs\n\nERGMitos1 (Vega Yon, Slaughter, and Haye 2021) leverage small network sizes to use exact statistics.\n\n\n\n\nFive small networks from the ergmito R package\n\n\nFrom the Spanish suffix meaning small."
  },
  {
    "objectID": "03-new-topics/index.html#discrete-exponential-family-models",
    "href": "03-new-topics/index.html#discrete-exponential-family-models",
    "title": "New topics in network modeling",
    "section": "Discrete Exponential-family Models",
    "text": "Discrete Exponential-family Models\n\n\n\nERGMs are a particular case of Random Markov fields.\nWe can use the ERGM framework for modeling vectors of binary outcomes, e.g., the consumption of \\{tobacco, MJ, alcohol\\}"
  },
  {
    "objectID": "03-new-topics/index.html#power-analysis-in-ergms",
    "href": "03-new-topics/index.html#power-analysis-in-ergms",
    "title": "New topics in network modeling",
    "section": "Power analysis in ERGMs",
    "text": "Power analysis in ERGMs\n\nUsing conditional ERGMs (closely related to constrained), we can do power analysis for network samples (Vega Yon 2023).\n\n\nReproduced from Krivitsky, Coletti, and Hens (2023b)"
  },
  {
    "objectID": "03-new-topics/index.html#two-step-estimation-ergms",
    "href": "03-new-topics/index.html#two-step-estimation-ergms",
    "title": "New topics in network modeling",
    "section": "Two-step estimation ERGMs",
    "text": "Two-step estimation ERGMs\n\n\nConditioning the ERGM on an observed statistic “drops” the associated coefficient.\nHypothesis: As n increases, conditional ERGM estimates are consistent with the full model:\n\n\n\nSimulation study trying to demonstrate the concept (Work in progress)"
  },
  {
    "objectID": "03-new-topics/index.html#thanks",
    "href": "03-new-topics/index.html#thanks",
    "title": "New topics in network modeling",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "03-new-topics/index.html#bonus-track-why-network-scientists-dont-use-ergms",
    "href": "03-new-topics/index.html#bonus-track-why-network-scientists-dont-use-ergms",
    "title": "New topics in network modeling",
    "section": "Bonus track: Why network scientists don’t use ERGMs?",
    "text": "Bonus track: Why network scientists don’t use ERGMs?\n\nAttempts to overcome these problems by extending the blockmodel have focused particularly on the use of (more complicated) p∗ or exponential random graph models, but while these are conceptually appealing, they quickly lose the analytic tractability of the original blockmodel as their complexity increases.\n– Karrer and Newman (2011)"
  },
  {
    "objectID": "03-new-topics/index.html#references",
    "href": "03-new-topics/index.html#references",
    "title": "New topics in network modeling",
    "section": "References",
    "text": "References\n\n\nNew topics in network modeling – https://ggvy.cl\n\n\n\nBrandenberger, Laurence. 2020. “Interdependencies in Conflict Dynamics: Analyzing Endogenous Patterns in Conflict Event Data Using Relational Event Models.” In Computational Conflict Research, edited by Emanuel Deutschmann, Jan Lorenz, Luis G. Nardin, Davide Natalini, and Adalbert F. X. Wilhelm, 67–80. Computational Social Sciences. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-29333-8_4.\n\n\nButts, Carter T., Alessandro Lomi, Tom A. B. Snijders, and Christoph Stadtfeld. 2023. “Relational Event Models in Network Science.” Network Science 11 (2): 175–83. https://doi.org/10.1017/nws.2023.9.\n\n\nKarrer, Brian, and M. E. J. Newman. 2011. “Stochastic Blockmodels and Community Structure in Networks.” Physical Review E 83 (1): 016107. https://doi.org/10.1103/PhysRevE.83.016107.\n\n\nKoskinen, Johan, and Galina Daraganova. 2022. “Bayesian Analysis of Social Influence.” Journal of the Royal Statistical Society Series A: Statistics in Society 185 (4): 1855–81. https://doi.org/10.1111/rssa.12844.\n\n\nKrivitsky, Pavel N., Pietro Coletti, and Niel Hens. 2023a. “A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks.” Journal of the American Statistical Association 0 (0): 1–21. https://doi.org/10.1080/01621459.2023.2242627.\n\n\n———. 2023b. “Rejoinder to Discussion of ‘A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’.” Journal of the American Statistical Association 118 (544): 2235–38. https://doi.org/10.1080/01621459.2023.2280383.\n\n\nStadtfeld, Christoph, Tom A. B. Snijders, Christian Steglich, and Marijtje van Duijn. 2020. “Statistical Power in Longitudinal Network Studies.” Sociological Methods & Research 49 (4): 1103–32. https://doi.org/10.1177/0049124118769113.\n\n\nStivala, Alex D., H. Colin Gallagher, David A. Rolls, Peng Wang, and Garry L. Robins. 2020. “Using Sampled Network Data With The Autologistic Actor Attribute Model.” arXiv. https://doi.org/10.48550/arXiv.2002.00849.\n\n\nStivala, Alex, Garry Robins, and Alessandro Lomi. 2020. “Exponential Random Graph Model Parameter Estimation for Very Large Directed Networks.” PLoS ONE 15 (1): 1–23. https://doi.org/10.1371/journal.pone.0227804.\n\n\nVega Yon, George G. 2023. “Power and Multicollinearity in Small Networks: A Discussion of ‘Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’ by Krivitsky, Coletti & Hens.” Journal Of The American Statistical Association.\n\n\nVega Yon, George G., Andrew Slaughter, and Kayla de la Haye. 2021. “Exponential Random Graph Models for Little Networks.” Social Networks 64 (August 2020): 225–38. https://doi.org/10.1016/j.socnet.2020.07.005.\n\n\nWang, Zeyi, Ian E. Fellows, and Mark S. Handcock. 2023. “Understanding Networks with Exponential-Family Random Network Models.” Social Networks, August, S0378873323000497. https://doi.org/10.1016/j.socnet.2023.07.003."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to the first version of ChiSocNet’s Summer School! This website was created for the ERGMs component of the school. Here, you will find three subpages with the following content: (1) Fundamentals about R and Statistics, (2) Introduction to ERGMs, (3) Advanced ERGMs, and (4) New topics in Network Modeling.\n\n1 References\nAlthough not all about ERGMs, the following list of references is an excellent resource for statistical inference in network science.\n\n\n“1.1: Basic Definitions and Concepts.” 2014. Statistics LibreTexts. https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/01%3A_Introduction_to_Statistics/1.01%3A_Basic_Definitions_and_Concepts.\n\n\nBarrett, Tyson, Matt Dowle, and Arun Srinivasan. 2023. Data.table: Extension of ‘Data.frame‘. https://r-datatable.com.\n\n\nBell, Brooke M., Donna Spruijt-Metz, George G. Vega Yon, Abu S. Mondol, Ridwan Alam, Meiyi Ma, Ifat Emi, John Lach, John A. Stankovic, and Kayla De La Haye. 2019. “Sensing Eating Mimicry Among Family Members.” Translational Behavioral Medicine. https://doi.org/10.1093/tbm/ibz051.\n\n\nBrandenberger, Laurence. 2020. “Interdependencies in Conflict Dynamics: Analyzing Endogenous Patterns in Conflict Event Data Using Relational Event Models.” In Computational Conflict Research, edited by Emanuel Deutschmann, Jan Lorenz, Luis G. Nardin, Davide Natalini, and Adalbert F. X. Wilhelm, 67–80. Computational Social Sciences. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-29333-8_4.\n\n\nBrooks, S., A. Gelman, G. Jones, and X. L. Meng. 2011. Handbook of Markov Chain Monte Carlo. CRC Press.\n\n\nButts, Carter T., Alessandro Lomi, Tom A. B. Snijders, and Christoph Stadtfeld. 2023. “Relational Event Models in Network Science.” Network Science 11 (2): 175–83. https://doi.org/10.1017/nws.2023.9.\n\n\nCasella, George, and Roger L. Berger. 2021. Statistical Inference. Cengage Learning.\n\n\nFellows, Ian E. 2012. “Exponential Family Random Network Models.” ProQuest Dissertations and Theses. PhD thesis. https://login.ezproxy.lib.utah.edu/login?url=https://www.proquest.com/dissertations-theses/exponential-family-random-network-models/docview/1221548720/se-2.\n\n\nFrank, O, and David Strauss. 1986. “Markov graphs.” Journal of the American Statistical Association 81 (395): 832–42. https://doi.org/10.2307/2289017.\n\n\nGelman, Andrew. 2018. “The Failure of Null Hypothesis Significance Testing When Studying Incremental Changes, and What to Do About It.” Personality and Social Psychology Bulletin 44 (1): 16–23. https://doi.org/10.1177/0146167217729162.\n\n\nGeyer, Charles J., and Elizabeth A. Thompson. 1992. “Constrained Monte Carlo Maximum Likelihood for Dependent Data.” Journal of the Royal Statistical Society. Series B (Methodological) 54 (3): 657–99. https://www.jstor.org/stable/2345852.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. “Statistical Tests, P Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nHandcock, Mark S., David R. Hunter, Carter T. Butts, Steven M. Goodreau, Pavel N. Krivitsky, and Martina Morris. 2023. Ergm: Fit, Simulate and Diagnose Exponential-Family Models for Networks. The Statnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.\n\n\nHaye, Kayla de la, Heesung Shin, George G. Vega Yon, and Thomas W. Valente. 2019. “Smoking Diffusion Through Networks of Diverse, Urban American Adolescents over the High School Period.” Journal of Health and Social Behavior. https://doi.org/10.1177/0022146519870521.\n\n\nHolland, Paul W., and Samuel Leinhardt. 1981. “An exponential family of probability distributions for directed graphs.” Journal of the American Statistical Association 76 (373): 33–50. https://doi.org/10.2307/2287037.\n\n\nHunter, David R. 2007. “Curved Exponential Family Models for Social Networks.” Social Networks 29 (2): 216–30. https://doi.org/10.1016/j.socnet.2006.08.005.\n\n\nHunter, David R., Mark S. Handcock, Carter T. Butts, Steven M. Goodreau, and Martina Morris. 2008. “ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.” Journal of Statistical Software 24 (3): 1–29. https://doi.org/10.18637/jss.v024.i03.\n\n\nHunter, David R., Pavel N. Krivitsky, and Michael Schweinberger. 2012. “Computational Statistical Methods for Social Network Models.” Journal of Computational and Graphical Statistics 21 (4): 856–82. https://doi.org/10.1080/10618600.2012.732921.\n\n\nKarrer, Brian, and M. E. J. Newman. 2011. “Stochastic Blockmodels and Community Structure in Networks.” Physical Review E 83 (1): 016107. https://doi.org/10.1103/PhysRevE.83.016107.\n\n\nKoskinen, Johan, and Galina Daraganova. 2022. “Bayesian Analysis of Social Influence.” Journal of the Royal Statistical Society Series A: Statistics in Society 185 (4): 1855–81. https://doi.org/10.1111/rssa.12844.\n\n\nKoskinen, Johan, Pete Jones, Darkhan Medeuov, Artem Antonyuk, Kseniia Puzyreva, and Nikita Basov. 2023. “Analysing Networks of Networks.” Social Networks 74 (July): 102–17. https://doi.org/10.1016/j.socnet.2023.02.002.\n\n\nKoskinen, Johan, Peng Wang, Garry Robins, and Philippa Pattison. 2018. “Outliers and Influential Observations in Exponential Random Graph Models.” Psychometrika 83 (4): 809–30. https://doi.org/10.1007/s11336-018-9635-8.\n\n\nKrivitsky, Pavel N. 2012. “Exponential-Family Random Graph Models for Valued Networks.” Electronic Journal of Statistics 6: 1100–1128. https://doi.org/10.1214/12-EJS696.\n\n\n———. 2017. “Using Contrastive Divergence to Seed Monte Carlo MLE for Exponential-Family Random Graph Models.” Computational Statistics & Data Analysis 107 (March): 149–61. https://doi.org/10.1016/j.csda.2016.10.015.\n\n\n———. 2023. Ergm.multi: Fit, Simulate and Diagnose Exponential-Family Models for Multiple or Multilayer Networks. The Statnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.multi.\n\n\nKrivitsky, Pavel N., Pietro Coletti, and Niel Hens. 2023a. “A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks.” Journal of the American Statistical Association 0 (0): 1–21. https://doi.org/10.1080/01621459.2023.2242627.\n\n\n———. 2023b. “Rejoinder to Discussion of ‘A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’.” Journal of the American Statistical Association 118 (544): 2235–38. https://doi.org/10.1080/01621459.2023.2280383.\n\n\nKrivitsky, Pavel N., David R. Hunter, Martina Morris, and Chad Klumb. 2023. “Ergm 4: New Features for Analyzing Exponential-Family Random Graph Models.” Journal of Statistical Software 105 (January): 1–44. https://doi.org/10.18637/jss.v105.i06.\n\n\nLeifeld, Philip. 2013. “Texreg : Conversion of Statistical Model Output in R to L A T E X and HTML Tables.” Journal of Statistical Software 55 (8). https://doi.org/10.18637/jss.v055.i08.\n\n\nLeSage, James P. 2008. “An Introduction to Spatial Econometrics.” Revue d’économie Industrielle 123 (123): 19–44. https://doi.org/10.4000/rei.3887.\n\n\nLeSage, James P., and R. Kelley Pace. 2014. “The Biggest Myth in Spatial Econometrics.” Econometrics 2 (4): 217–49. https://doi.org/10.2139/ssrn.1725503.\n\n\nLusher, Dean, Johan Koskinen, and Garry Robins. 2013. Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications. Cambridge University Press.\n\n\nMilo, R., S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. 2002. “Network Motifs: Simple Building Blocks of Complex Networks.” Science 298 (5594): 824–27. https://doi.org/10.1126/science.298.5594.824.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobins, Garry, Philippa Pattison, and Peter Elliott. 2001. “Network Models for Social Influence Processes.” Psychometrika 66 (2): 161–89. https://doi.org/10.1007/BF02294834.\n\n\nRobins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007. “An introduction to exponential random graph (p*) models for social networks.” Social Networks 29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002.\n\n\nRoger Bivand. 2022. “R Packages for Analyzing Spatial Data: A Comparative Case Study with Areal Data.” Geographical Analysis 54 (3): 488–518. https://doi.org/10.1111/gean.12319.\n\n\nSlaughter, Andrew J., and Laura M. Koehly. 2016. “Multilevel Models for Social Networks: Hierarchical Bayesian Approaches to Exponential Random Graph Modeling.” Social Networks 44: 334–45. https://doi.org/10.1016/j.socnet.2015.11.002.\n\n\nSnijders, Tom A B, Philippa E Pattison, Garry L Robins, and Mark S Handcock. 2006. “New specifications for exponential random graph models.” Sociological Methodology 36 (1): 99–153. https://doi.org/10.1111/j.1467-9531.2006.00176.x.\n\n\nSnijders, Tom A. B., and Stephen P. Borgatti. 1999. “Non-Parametric Standard Errors and Tests for Network Statistics.” Connections 22 (2): 1–10.\n\n\nStadtfeld, Christoph, Tom A. B. Snijders, Christian Steglich, and Marijtje van Duijn. 2020. “Statistical Power in Longitudinal Network Studies.” Sociological Methods & Research 49 (4): 1103–32. https://doi.org/10.1177/0049124118769113.\n\n\nStivala, Alex D., H. Colin Gallagher, David A. Rolls, Peng Wang, and Garry L. Robins. 2020. “Using Sampled Network Data With The Autologistic Actor Attribute Model.” arXiv. https://doi.org/10.48550/arXiv.2002.00849.\n\n\nStivala, Alex, Garry Robins, and Alessandro Lomi. 2020. “Exponential Random Graph Model Parameter Estimation for Very Large Directed Networks.” PLoS ONE 15 (1): 1–23. https://doi.org/10.1371/journal.pone.0227804.\n\n\nTanaka, Kyosuke, and George G. Vega Yon. 2024. “Imaginary Network Motifs: Structural Patterns of False Positives and Negatives in Social Networks.” Social Networks 78 (July): 65–80. https://doi.org/10.1016/j.socnet.2023.11.005.\n\n\nValente, Thomas W., and George G. Vega Yon. 2020. “Diffusion/Contagion Processes on Social Networks.” Health Education & Behavior 47 (2): 235–48. https://doi.org/10.1177/1090198120901497.\n\n\nValente, Thomas W., Heather Wipfli, and George G. Vega Yon. 2019. “Network Influences on Policy Implementation: Evidence from a Global Health Treaty.” Social Science and Medicine. https://doi.org/10.1016/j.socscimed.2019.01.008.\n\n\nVega Yon, George. 2020. ergmito: Exponential Random Graph Models for Small Networks. CRAN. https://cran.r-project.org/package=ergmito.\n\n\nVega Yon, George G. 2023. “Power and Multicollinearity in Small Networks: A Discussion of ‘Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’ by Krivitsky, Coletti & Hens.” Journal Of The American Statistical Association.\n\n\nVega Yon, George G., Andrew Slaughter, and Kayla de la Haye. 2021. “Exponential Random Graph Models for Little Networks.” Social Networks 64 (August 2020): 225–38. https://doi.org/10.1016/j.socnet.2020.07.005.\n\n\nWang, Zeyi, Ian E. Fellows, and Mark S. Handcock. 2023. “Understanding Networks with Exponential-Family Random Network Models.” Social Networks, August, S0378873323000497. https://doi.org/10.1016/j.socnet.2023.07.003.\n\n\nWasserman, Stanley, and Philippa Pattison. 1996. “Logit models and logistic regressions for social networks: I. An introduction to Markov graphs and p*.” Psychometrika 61 (3): 401–25. https://doi.org/10.1007/BF02294547.\n\n\n\n\n2 Disclaimer\nThis is an ongoing project. The course is being developed and will be updated as we go. If you have any comments or suggestions, please let me know. The generation of the course materials was assisted by AI tools, namely, GitHub copilot.\n\n\n3 Code of Conduct\nPlease note that the networks-udd2024 project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms."
  }
]