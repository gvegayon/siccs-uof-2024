[
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Home",
    "section": "Introduction",
    "text": "Introduction\nIf you are reading this, it is because you know that networks are everywhere. Network science is a rapidly growing field that has been applied to many different disciplines, from biology to sociology, from computer science to physics. In this course, we will go over advanced network science topics; particularly, statistical inference in networks. The course contents are:\n\nOverview of statistical inference.\nIntroduction to network science inference.\nMotif detection.\nGlobal statistics (e.g., modularity).\nRandom graphs (static).\nRandom graphs (dynamic).\nCoevolution of networks and behavior.\nAdvanced topics (sampling and conditional models)."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Home",
    "section": "About the author",
    "text": "About the author\nDr. George G. Vega Yon is an Assistant Professor of Research at the Division of Epidemiology at the University of Utah and a Lead Scientist at Booz Allen Hamilton. He studies Complex Systems using Statistical Computing. George has over ten years of experience developing scientific software focusing on high-performance computing, data visualization, and social network analysis. His training is in Public Policy (M.A. UAI, 2011), Economics (M.Sc. Caltech, 2015), and Biostatistics (Ph.D. USC, 2020).\nDr. Vega Yon obtained his Ph.D. in Biostatistics under the supervision of Prof. Paul Marjoram and Prof. Kayla de la Haye, with his dissertation titled “Essays on Bioinformatics and Social Network Analysis: Statistical and Computational Methods for Complex Systems.”"
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Home",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis is an ongoing project. The course is being developed and will be updated as we go. If you have any comments or suggestions, please let me know. The generation of the course materials was assisted by AI tools, namely, GitHub copilot."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Home",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that the networks-udd2024 project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms."
  },
  {
    "objectID": "01-overview.html#programming-with-r",
    "href": "01-overview.html#programming-with-r",
    "title": "2  Overview",
    "section": "2.1 Programming with R",
    "text": "2.1 Programming with R\nThe R programming language (R Core Team 2023) is the defacto language for social network analysis1. Furthermore, R homes the most comprehensive collection of packages implementing the methods we will cover here. Let’s start by the fundamentals"
  },
  {
    "objectID": "01-overview.html#getting-help",
    "href": "01-overview.html#getting-help",
    "title": "2  Overview",
    "section": "2.2 Getting help",
    "text": "2.2 Getting help\nUnlike other languages, R’s documentation is highly reliable. The Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN must pass a series of tests to ensure the quality of the code, including the documentation.\nTo get help on a function, we can use the help() function. For example, if we wanted to get help on the mean() function, we would do:\n\nhelp(\"mean\")"
  },
  {
    "objectID": "01-overview.html#naming-conventions",
    "href": "01-overview.html#naming-conventions",
    "title": "2  Overview",
    "section": "2.3 Naming conventions",
    "text": "2.3 Naming conventions\nR has a set of naming conventions that we should follow to avoid confusion. The most important ones are:\n\nUse lowercase letters (optional)\nUse underscores to separate words (optional)\nDo not start with a number\nDo not use special characters\nDo not use reserved words\n\n\n\n\n\n\n\nQuestion\n\n\n\nOf the following list, which are valid names and which are valid but to be avoided?\n_my.var\nmy.var\nmy_var\nmyVar\nmyVar1\n1myVar\nmy var\nmy-var"
  },
  {
    "objectID": "01-overview.html#assignment",
    "href": "01-overview.html#assignment",
    "title": "2  Overview",
    "section": "2.4 Assignment",
    "text": "2.4 Assignment\nIn R, we have two (four) ways of assigning values to objects: the &lt;- and = binary operators2. Although both are equivalent, the former is the preferred way of assigning values to objects since the latter can be confused with function arguments.\n\nx &lt;- 1\nx = 1\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the difference between the following two assignments? Use the help function to find out.\nx &lt;- 1\nx &lt;&lt;- 1\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat are other ways in which you can assign values to objects?"
  },
  {
    "objectID": "01-overview.html#using-functions-and-piping",
    "href": "01-overview.html#using-functions-and-piping",
    "title": "2  Overview",
    "section": "2.5 Using functions and piping",
    "text": "2.5 Using functions and piping\nIn R, we use functions to perform operations on objects. Functions are implemented as function_name ( argument_1 , argument_2 , ... ). For example, the mean() function takes a vector of numbers and returns the mean of the values:\n\nx &lt;- c(1, 2, 3) # The c() function creates a vector\nmean(x)\n## [1] 2\n\nFurthermore, we can use the pipe operator (|&gt;) to improve readability. The pipe operator takes the output of the left-hand side expression and passes it as the first argument of the right-hand side expression. Our previous example could be rewritten as:\n\nc(1, 2, 3) |&gt; mean()\n## [1] 2"
  },
  {
    "objectID": "01-overview.html#data-structures",
    "href": "01-overview.html#data-structures",
    "title": "2  Overview",
    "section": "2.6 Data structures",
    "text": "2.6 Data structures\nAtomic types are the minimal building blocks of R. They are logical, integer, double, character, complex, raw:\n\nx_logical   &lt;- TRUE\nx_integer   &lt;- 1L\nx_double    &lt;- 1.0\nx_character &lt;- \"a\"\nx_complex   &lt;- 1i\nx_raw       &lt;- charToRaw(\"a\")\n\nUnlike other languages, we do not need to declare the data type before creating the object; R will infer it from the value.\n\n\n\n\n\n\nPro-tip\n\n\n\nAdding the L suffix to the value is good practice when dealing with integers. Some R packages like data.table (Barrett, Dowle, and Srinivasan 2023) have internal checks that will throw an error if you are not explicit about the data type.\n\n\nThe next type is the vector. A vector is a collection of elements of the same type. The most common way to create a vector is with the c() function:\n\nx_integer &lt;- c(1, 2, 3)\nx_double  &lt;- c(1.0, 2.0, 3.0)\nx_logical &lt;- c(TRUE, FALSE, TRUE)\n## etc.\n\nR will coerce the data types to the most general type. For example, if we mix integers and doubles, R will coerce the integers into doubles. The coercion order is logical &lt; integer &lt; double &lt; character\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is the coercion order logical &lt; integer &lt; double &lt; character?\n\n\nThe next data structure is the list. A list is a collection of elements of any type. We can create a list with the list() function:\n\nx_list       &lt;- list(1, 2.0, TRUE, \"a\")\nx_list_named &lt;- list(a = 1, b = 2.0, c = TRUE, d = \"a\")\n\nTo access elements in a list, we have two options: by position or by name, the latter only if the elements are named:\n\nx_list[[1]]\n## [1] 1\nx_list_named[[\"a\"]]\n## [1] 1\nx_list_named$a\n## [1] 1\n\nAfter lists, we have matrices. A matrix is a collection of elements of the same type arranged in a two-dimensional grid. We can create a matrix with the matrix() function:\n\nx_matrix &lt;- matrix(1:9, nrow = 3, ncol = 3)\nx_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n\n## We can access elements in a matrix by row column, or position:\nx_matrix[1, 2]\n## [1] 4\nx_matrix[cbind(1, 2)]\n## [1] 4\nx_matrix[4]\n## [1] 4\n\n\n\n\n\n\n\nMatrix is a vector\n\n\n\nMatrices in R are vectors with dimensions. In base R, matrices are stored in column-major order. This means that the elements are stored column by column. This is important to know when we are accessing elements in a matrix\n\n\nThe two last data structures are arrays and data frames. An array is a collection of elements of the same type arranged in a multi-dimensional grid. We can create an array with the array() function:\n\nx_array &lt;- array(1:27, dim = c(3, 3, 3))\n\n## We can access elements in an array by row, column, and dimension, or\n## position:\nx_array[1, 2, 3]\n## [1] 22\nx_array[cbind(1, 2, 3)]\n## [1] 22\nx_array[22]\n## [1] 22\n\nData frames are the most common data structure in R. In principle, these objects are lists of vectors of the same length, each vector representing a column. Columns (lists) in data frames can be of different types, but elements in each column must be of the same type. We can create a data frame with the data.frame() function:\n\nx_data_frame &lt;- data.frame(\n  a = 1:3,\n  b = c(\"a\", \"b\", \"c\"),\n  c = c(TRUE, FALSE, TRUE)\n)\n\n## We can access elements in a data frame by row, column, or position:\nx_data_frame[1, 2]\n## [1] \"a\"\nx_data_frame[cbind(1, 2)]\n## [1] \"a\"\nx_data_frame$b[1]    # Like a list\n## [1] \"a\"\nx_data_frame[[2]][1] # Like a list too\n## [1] \"a\""
  },
  {
    "objectID": "01-overview.html#functions",
    "href": "01-overview.html#functions",
    "title": "2  Overview",
    "section": "2.7 Functions",
    "text": "2.7 Functions\nFunctions are the most important building blocks of R. A function is a set of instructions that takes one or more inputs and returns one or more outputs. We can create a function with the function() function:\n\n## This function has two arguments (y is optional)\nf &lt;- function(x, y = 1) {\n  x + 1\n}\n\nf(1)\n## [1] 2\n\nStarting with R 4, we can use the lambda syntax to create functions:\n\nf &lt;- \\(x, y) x + 1\n\nf(1)\n## [1] 2"
  },
  {
    "objectID": "01-overview.html#control-flow",
    "href": "01-overview.html#control-flow",
    "title": "2  Overview",
    "section": "2.8 Control flow",
    "text": "2.8 Control flow\nControl flow statements allow us to control the execution of the code. The most common control flow statements are if, for, while, and repeat. We can create a control flow statement with the if(), for(), while(), and repeat() functions:\n\n## if\nif (TRUE) {\n  \"a\"\n} else {\n  \"b\"\n}\n## [1] \"a\"\n\n## for\nfor (i in 1:3) {\n  cat(\"This is the number \", i, \"\\n\")\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n## while\ni &lt;- 1\nwhile (i &lt;= 3) {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3\n\n## repeat\ni &lt;- 1\nrepeat {\n  cat(\"This is the number \", i, \"\\n\")\n  i &lt;- i + 1\n  if (i &gt; 3) {\n    break\n  }\n}\n## This is the number  1 \n## This is the number  2 \n## This is the number  3"
  },
  {
    "objectID": "01-overview.html#r-packages",
    "href": "01-overview.html#r-packages",
    "title": "2  Overview",
    "section": "2.9 R packages",
    "text": "2.9 R packages\nR is so powerful because of its extensions. R extensions (different from other programming languages) are called packages. Packages are collections of functions, data, and documentation that provide additional functionality to R. Although anyone can create and distribute R packages to other users, the Comprehensive R Archive Network [CRAN] is the official repository of R packages. All packages posted on CRAN are thoroughly tested, so generally, their quality is high.\nTo install R packages, we use the install.packages() function; to load them, we use the library() function. For example, the following code chunk installs the ergm package and loads it:\n\ninstall.packages(\"ergm\")\nlibrary(ergm)"
  },
  {
    "objectID": "01-overview.html#statistics",
    "href": "01-overview.html#statistics",
    "title": "2  Overview",
    "section": "2.10 Statistics",
    "text": "2.10 Statistics\nGenerally, statistics are used for two purposes: to describe and to infer. We observe data samples in descriptive statistics, recording and reporting the mean, median, and standard deviation, among other statistics. Statistical inference, on the other hand, is used to infer the properties of a population from a sample, particularly, about population parameters.\n\n\n\nGrad view of statistics – Reproduced from “1.1: Basic Definitions and Concepts” (2014)\n\n\nFrom the perspective of network science, descriptive statistics are used to describe the properties of a network, such as the number of nodes and edges, the degree distribution, the clustering coefficient, etc. Statistical inference in network science has to do with addressing questions about the underlying properties of networked systems; some questions include the following:\n\nAre two networks different?\nIs the number of observed triangles in a network higher than expected by chance?\nAre individuals in a network more likely to be connected to individuals with similar characteristics?\netc.\n\nPart of statistical inference is hypothesis testing."
  },
  {
    "objectID": "01-overview.html#hypothesis-testing",
    "href": "01-overview.html#hypothesis-testing",
    "title": "2  Overview",
    "section": "2.11 Hypothesis testing",
    "text": "2.11 Hypothesis testing\nAccording to Wikipedia\n\nA statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis. More generally, hypothesis testing allows us to make probabilistic statements about population parameters. More informally, hypothesis testing is the processes of making decisions under uncertainty. Typically, hypothesis testing procedures involve a user selected tradeoff between false positives and false negatives. – Wiki\n\nIn a nutshell, hypothesis testing is performed by following these steps:\n\nState the null and alternative hypotheses. In general, the null hypothesis is a statement about the population parameter that challenges our research question; for example, given the question of whether two networks are different, the null hypothesis would be that the two networks are the same.\nCompute the corresponding test statistic. It is a data function that reduces the information to a single number.\nCompare the observed test statistic with the distribution of the test statistic under the null hypothesis. The sometimes infamous p-value: ``[…] the probability that the chosen test statistic would have been at least as large as its observed value if every model assumption were correct, including the test hypothesis.’’ (Greenland et al. 2016) 3\n\n\nReport the observed effect and p-value, i.e., \\Pr(t \\in H_0)\n\nWe usually say that we either reject the null hypothesis or fail to reject it (we never accept the null hypothesis,) but, in my view, it is always better to talk about it in terms of “suggests evidence for” or “suggests evidence against.”\nWe will illustrate statistical concepts more concretely in the next section."
  },
  {
    "objectID": "01-overview.html#statistical-programming",
    "href": "01-overview.html#statistical-programming",
    "title": "2  Overview",
    "section": "2.12 Statistical programming",
    "text": "2.12 Statistical programming\nStatistical programming (or computing) is the science of leveraging modern computing power to solve statistical problems. The R programming language is the defacto language for statistical programming, and so it has an extensive collection of packages implementing statistical methods and functions."
  },
  {
    "objectID": "01-overview.html#probability-distributions",
    "href": "01-overview.html#probability-distributions",
    "title": "2  Overview",
    "section": "2.13 Probability distributions",
    "text": "2.13 Probability distributions\nR has a standard way of naming probability functions. The naming structure is [type of function][distribution], where [type of function] can be d for density, p for cumulative distribution function, q for quantile function, and r for random generation. For example, the normal distribution has the following functions:\n\ndnorm(0, mean = 0, sd = 1)\n## [1] 0.3989423\npnorm(0, mean = 0, sd = 1)\n## [1] 0.5\nqnorm(0.5, mean = 0, sd = 1)\n## [1] 0\n\nNow, if we wanted to know what is the probability of observing a value smaller than -2 comming from a standard normal distribution, we would do:\n\npnorm(-2, mean = 0, sd = 1)\n## [1] 0.02275013\n\nCurrently, R has a wide range of probability distributions implemented.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many probability distributions are implemented in R’s stats package?"
  },
  {
    "objectID": "01-overview.html#random-number-generation",
    "href": "01-overview.html#random-number-generation",
    "title": "2  Overview",
    "section": "2.14 Random number generation",
    "text": "2.14 Random number generation\nRandom numbers, and more precisely, pseudo-random numbers, are a vital component of statistical programming. Pure randomness is hard to come by, and so we rely on pseudo-random number generators (PRNGs) to generate random numbers. These generators are deterministic algorithms that produce sequences of numbers we can then use to generate random samples from probability distributions. Because of the latter, PRNGs need a starting point called the seed. As a statistical computing program, R has a variety of PRNGs. As suggested in the previous subsection, we can generate random numbers from a probability distribution with the r function. In what follows, we will draw random numbers from a few distributions and plot histograms of the results:\n\nset.seed(1)\n\n## Saving the current graphical parameters\nop &lt;- par(mfrow = c(2,2))\nrnorm(1000) |&gt; hist(main = \"Normal distribution\")\nrunif(1000) |&gt; hist(main = \"Uniform distribution\")\nrpois(1000, lambda = 1) |&gt; hist(main = \"Poisson distribution\")\nrbinom(1000, size = 10, prob = 0.1) |&gt; hist(main = \"Binomial distribution\")\n\n\n\npar(op)"
  },
  {
    "objectID": "01-overview.html#simulations-and-sampling",
    "href": "01-overview.html#simulations-and-sampling",
    "title": "2  Overview",
    "section": "2.15 Simulations and sampling",
    "text": "2.15 Simulations and sampling\nSimulations are front and center in statistical programming. We can use them to test the properties of statistical methods, generate data, and perform statistical inference. The following example uses the sample function in R to compute the bootstrap standard error of the mean (see Casella and Berger 2021):\n\nset.seed(1)\nx &lt;- rnorm(1000)\n\n## Bootstrap standard error of the mean\nn &lt;- length(x)\nB &lt;- 1000\n\n## We will store the results in a vector\nres &lt;- numeric(B)\n\nfor (i in 1:B) {\n  # Sample with replacement\n  res[i] &lt;- sample(x, size = n, replace = TRUE) |&gt;\n    mean()\n}\n\n## Plot the results\nhist(res, main = \"Bootstrap standard error of the mean\")\n\n\n\n\nSince the previous example is rather extensive, let us review it in detail.\n\nset.seed(1) sets the seed of the PRNG to 1. It ensures we get the same results every time we run the code.\nrnorm() generates a sample of 1,000 standard-normal values.\nn &lt;- length(x) stores the length of the vector in the n variable.\nB &lt;- 1000 stores the number of bootstrap samples in the B variable.\nres &lt;- numeric(B) creates a vector of length B to store the results.\nfor (i in 1:B) is a for loop that iterates from 1 to B.\nres[i] &lt;- sample(x, size = n, replace = TRUE) |&gt; mean() samples n values from x with replacement and computes the mean of the sample.\nThe pipe operator (|&gt;) passes the output of the left-hand side expression as the first argument of the right-hand side expression.\nhist(res, main = \"Bootstrap standard error of the mean\") plots the histogram of the results.\n\n\n\n\n\n“1.1: Basic Definitions and Concepts.” 2014. Statistics LibreTexts. https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/01%3A_Introduction_to_Statistics/1.01%3A_Basic_Definitions_and_Concepts.\n\n\nBarrett, Tyson, Matt Dowle, and Arun Srinivasan. 2023. Data.table: Extension of ‘Data.frame‘. https://r-datatable.com.\n\n\nCasella, George, and Roger L. Berger. 2021. Statistical Inference. Cengage Learning.\n\n\nGelman, Andrew. 2018. “The Failure of Null Hypothesis Significance Testing When Studying Incremental Changes, and What to Do About It.” Personality and Social Psychology Bulletin 44 (1): 16–23. https://doi.org/10.1177/0146167217729162.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. 2016. “Statistical Tests, P Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/."
  },
  {
    "objectID": "01-overview.html#footnotes",
    "href": "01-overview.html#footnotes",
    "title": "2  Overview",
    "section": "",
    "text": "Although, not for network science in general.↩︎\nIn mathematics and computer science, a binary operator is a function that takes two arguments. In R, binary operators are implemented as variable 1 [operator] variable 2. For example, 1 + 2 is a binary operation.↩︎\nThe discussion about interpreting p-values and hypothesis testing is vast and relevant. Although we will not review this here, I recommend looking into the work of Andrew Gelman Gelman (2018).↩︎"
  },
  {
    "objectID": "02-random-graphs.html",
    "href": "02-random-graphs.html",
    "title": "3  Random graphs",
    "section": "",
    "text": "4 Motif discovery\nOne important application of random graphs is motif discovery. The principle is simple: we can use random graphs to generate null distributions of observed statistics/motifs. Usually the process is as follows:"
  },
  {
    "objectID": "02-random-graphs.html#introduction",
    "href": "02-random-graphs.html#introduction",
    "title": "3  Random graphs",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this section, we will focus on reviewing the most common random graph models, how these are used, and what things are important to consider when using them. Later on in the course, we will focus on Exponential-Family Random Graph Models [ERGMs], which are a generalization of the models we will discuss here."
  },
  {
    "objectID": "02-random-graphs.html#erdősrényi-model",
    "href": "02-random-graphs.html#erdősrényi-model",
    "title": "3  Random graphs",
    "section": "3.2 Erdős–Rényi model",
    "text": "3.2 Erdős–Rényi model\nThe Erdős–Rényi model is the simplest random graph model. It is defined by two parameters: n and p. The parameter n is the number of nodes in the graph, and p is the probability that any two nodes are connected by an edge. The model is named after Paul Erdős and Alfréd Rényi, who first introduced it in 1959.\nFormally, we can describe the ER model as follows: (V, E) where V = \\{1, \\ldots, n\\} and E is a set of edges, where each edge is included with probability p.\n\n\n\n\n\n\nTip\n\n\n\nComputing note: In the case of large networks, sampling ER graphs can be done effectively in a two-step process. First, we sample the number of edges in the graph from a binomial distribution. Then, we sample the edges uniformly at random from the set of all possible edges. This is much more efficient than sampling each edge independently since the number of possible edges is much smaller than the number of possible graphs.\n\n\nMost of the time, the ER is used as a reference distribution for studying real-world networks. Nevertheless, using the ER model as a null model for a real-world network is not always a good idea, as it may inflate the type two error rate.\n\n3.2.1 Code example\n\n## Model parameters\nn &lt;- 40\np &lt;- 0.1\n\n## Generating the graph, version 1\nset.seed(3312)\ng &lt;- matrix(as.integer(runif(n * n) &lt; p), nrow = n, ncol = n)\ndiag(g) &lt;- 0\n\n## Visualizing the network\nlibrary(igraph)\nlibrary(netplot)\nnplot(graph_from_adjacency_matrix(g))"
  },
  {
    "objectID": "02-random-graphs.html#watts-strogatz-model",
    "href": "02-random-graphs.html#watts-strogatz-model",
    "title": "3  Random graphs",
    "section": "3.3 Watts-Strogatz model",
    "text": "3.3 Watts-Strogatz model\nThe second model in our list is the small-world model, introduced by Duncan Watts and Steven Strogatz in 1998. The model is defined by three parameters: n, k, and p. The parameter n is the number of nodes in the graph, k is the number of neighbors each node is connected to, and p is the probability that an edge is rewired. As its name suggests, the networks sampled from this model hold the small-world property, which means that the average distance between any two nodes is small.\nNetworks from the WS model are generated as follows:\n\nStart with a ring of n nodes, where each node is connected to its k nearest neighbors.\nFor each edge (u, v), rewire it with probability p by replacing it with a random edge (u, w), where w is chosen uniformly at random from the set of all nodes.\n\nChallenge: How would you generate a WS graph using the two-step process described above?\n\n3.3.1 Code example\n\n## Creating a ring\nn &lt;- 10\nV &lt;- 1:n\nk &lt;- 3\np &lt;- .2\n\nE &lt;- NULL\nfor (i in 1:k) {\n  E &lt;- rbind(E, cbind(V, c(V[-c(1:i)], V[1:i])))\n}\n\n## Generating the ring layout\nlo &lt;- layout_in_circle(graph_from_edgelist(E))\n\n## Plotting with netplot\nnplot(\n  graph_from_edgelist(E),\n  layout = lo\n  )\n\n\n\n## Rewiring\nids &lt;- which(runif(nrow(E)) &lt; p)\nE[ids, 2] &lt;- sample(V, length(ids), replace = TRUE)\nnplot(\n  graph_from_edgelist(E),\n  layout = lo\n  )"
  },
  {
    "objectID": "02-random-graphs.html#scale-free-networks",
    "href": "02-random-graphs.html#scale-free-networks",
    "title": "3  Random graphs",
    "section": "3.4 Scale-free networks",
    "text": "3.4 Scale-free networks\nScale-free networks are networks where the degree distribution follows a power-law distribution. The power-law distribution is a heavy-tailed distribution, which means that it has a long tail of high-degree nodes. The power-law distribution is defined as follows:\n\np(k) = C k^{-\\gamma}\n\nwhere C is a normalization constant and \\gamma is the power-law exponent. The power-law exponent is usually between 2 and 3, but it can be any value larger than 2. The power-law distribution is a special case of the more general class of distributions called the Pareto distribution.\nScale-free networks are generated using the Barabási–Albert model, which was introduced by Albert-László Barabási and Réka Albert in 1999. The model is defined by two parameters: n and m. The parameter n is the number of nodes in the graph, and m is the number of edges added at each time step. The model is generated as follows:\n\nStart with a graph of m nodes, where each node is connected to all other nodes.\nAt each time step, add a new node to the graph and connect it to m existing nodes. The probability that a new node is connected to an existing node i is proportional to the degree of i.\n\n\n3.4.1 Code example\n\n## Model parameters\nn &lt;- 500\nm &lt;- 2\n\n## Generating the graph\nset.seed(3312)\ng &lt;- matrix(0, nrow = n, ncol = n)\ng[1:m, 1:m] &lt;- 1\ndiag(g) &lt;- 0\n\n## Adding nodes\nfor (i in (m + 1):n) {\n\n  # Selecting the nodes to connect to\n  ids &lt;- sample(\n    x       = 1:(i-1), # Up to i-1\n    size    = m,       # m nodes\n    replace = FALSE,   # No replacement\n    # Probability proportional to the degree\n    prob    = colSums(g[, 1:(i-1), drop = FALSE])\n    )\n\n  # Adding the edges\n  g[i, ids] &lt;- 1\n  g[ids, i] &lt;- 1\n\n}\n\n## Visualizing the degree distribution\nlibrary(ggplot2)\ndata.frame(degree = colSums(g)) |&gt;\n  ggplot(aes(degree)) +\n  geom_histogram() +\n  scale_x_log10() +\n  labs(\n    x = \"Degree\\n(log10 scale)\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "02-random-graphs.html#code-example-2",
    "href": "02-random-graphs.html#code-example-2",
    "title": "3  Random graphs",
    "section": "4.1 Code example",
    "text": "4.1 Code example\n\n# Loading the R package\nlibrary(netdiffuseR)\n\n# A graph with known structure (see Milo 2004)\nn &lt;- 5\nx &lt;- matrix(0, ncol=n, nrow=n)\nx &lt;- as(x, \"dgCMatrix\")\nx[1,c(-1,-n)] &lt;- 1\nx[c(-1,-n),n] &lt;- 1\nx\n\n5 x 5 sparse Matrix of class \"dgCMatrix\"\n              \n[1,] . 1 1 1 .\n[2,] . . . . 1\n[3,] . . . . 1\n[4,] . . . . 1\n[5,] . . . . .\n\n# Simulations (increase the number for more precision)\nset.seed(8612)\nnsim &lt;- 1e4\nw &lt;- sapply(seq_len(nsim), function(y) {\n # Creating the new graph\n g &lt;- rewire_graph(x,p=nlinks(x)*100, algorithm = \"swap\")\n # Categorizing (tag of the generated structure)\n paste0(as.vector(g), collapse=\"\")\n})\n# Counting\ncoded &lt;- as.integer(as.factor(w))\nplot(table(coded)/nsim*100, type=\"p\", ylab=\"Frequency %\", xlab=\"Class of graph\", pch=3,\n main=\"Distribution of classes generated by rewiring\")\n# Marking the original structure\nbaseline &lt;- paste0(as.vector(x), collapse=\"\")\npoints(x=7,y=table(as.factor(w))[baseline]/nsim*100, pch=3, col=\"red\")"
  },
  {
    "objectID": "03-behavior-and-coevolution.html#introduction",
    "href": "03-behavior-and-coevolution.html#introduction",
    "title": "4  Behavior and coevolution",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThis section focuses on inference involving network and a secondary outcome. While there are many ways of studying the coevolution or dependence between network and behavior, this section focuses on two classes of analysis: When the network is fixed and when both network and behavior influence each other.\nWhether we treat the network as given or endogenous sets the complexity of conducting statistical inference. Data analysis becomes much more straightforward if our research focuses on individual-level outcomes embedded in a network and not on the network itself. Here, we will deal with three particular cases: (1) when network effects are lagged, (2) egocentric networks, and (3) when network effects are contemporaneous."
  },
  {
    "objectID": "03-behavior-and-coevolution.html#lagged-exposure",
    "href": "03-behavior-and-coevolution.html#lagged-exposure",
    "title": "4  Behavior and coevolution",
    "section": "4.2 Lagged exposure",
    "text": "4.2 Lagged exposure\nIf we assume that network influence in the form of exposure is lagged, we have one of the most straightforward cases for network inference (Haye et al. 2019; Valente and Vega Yon 2020; Valente, Wipfli, and Vega Yon 2019). Here, instead of dealing with convoluted statistical models, the problem reduces to estimating a simple linear regression model. Generally, lagged exposure effects look like this:\n\ny_{it} = \\rho \\left(\\sum_{j\\neq i}X_{ij}\\right)^{-1}\\left(\\sum_{j\\neq i}y_{jt-1} X_{ij}\\right) + {\\bm{{\\theta}}}^{\\mathbf{t}}\\bm{{w_i}} + \\varepsilon,\\quad \\varepsilon \\sim \\text{N}(0, \\sigma^2)\n\nwhere y_{it} is the outcome of individual i at time t, X_{ij} is the ij-th entry of the adjacency matrix, \\bm{{\\theta}} is a vector of coefficients, \\bm{{w_i}} is a vector of features/covariates of individual i, and \\varepsilon_i is a normally distributed error. Here, the key component is \\rho: the coefficient associated with the network exposure effect.\nThe exposure statistic, \\left(\\sum_{j\\neq i}X_{ij}\\right)^{-1}\\left(\\sum_{j\\neq i}y_{jt-1} X_{ij}\\right), is the weighted average of i’s neighbors’ outcomes at time t-1."
  },
  {
    "objectID": "03-behavior-and-coevolution.html#code-example-lagged-exposure",
    "href": "03-behavior-and-coevolution.html#code-example-lagged-exposure",
    "title": "4  Behavior and coevolution",
    "section": "4.3 Code example: Lagged exposure",
    "text": "4.3 Code example: Lagged exposure\nThe following code example shows how to estimate a lagged exposure effect using the glm function in R. The model we will simulate and estimate features a Bernoulli graph with 1,000 nodes and a density of 0.01.\n\ny_{it} = \\theta_1 + \\rho \\text{Exposure}_{it} + \\theta_2 w_i + \\varepsilon\n\nwhere \\text{Exposure}_{it} is the exposure statistic defined above, and w_i is a vector of covariates.\n\n## Simulating data\nn &lt;- 1000\ntime &lt;- 2\ntheta &lt;- c(-1, 3)\n\n## Sampling a bernoilli network\nset.seed(3132)\np &lt;- 0.01\nX &lt;- matrix(rbinom(n^2, 1, p), nrow = n)\ndiag(X) &lt;- 0\n\n## Covariate\nW &lt;- matrix(rnorm(n), nrow = n)\n\n## Simulating the outcome\nrho &lt;- 0.5\nY0 &lt;- cbind(rnorm(n))\n\n## The lagged exposure\nexpo &lt;- (X %*% Y0)/rowSums(X)\nY1 &lt;- theta[1] + rho * expo + W * theta[2] + rnorm(n)\n\nNow we fit the model using GLM, in this case, linear Regression\n\nfit &lt;- glm(Y1 ~ expo + W, family = \"gaussian\")\nsummary(fit)\n\n\nCall:\nglm(formula = Y1 ~ expo + W, family = \"gaussian\")\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.07187    0.03284 -32.638  &lt; 2e-16 ***\nexpo         0.61170    0.10199   5.998  2.8e-09 ***\nW            3.00316    0.03233  92.891  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.071489)\n\n    Null deviance: 10319.3  on 999  degrees of freedom\nResidual deviance:  1068.3  on 997  degrees of freedom\nAIC: 2911.9\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "03-behavior-and-coevolution.html#egocentric-networks",
    "href": "03-behavior-and-coevolution.html#egocentric-networks",
    "title": "4  Behavior and coevolution",
    "section": "4.4 Egocentric networks",
    "text": "4.4 Egocentric networks\nGenerally, when we use egocentric networks and egos’ outcomes, we are thinking in a model where one observation is the pair (y_i, X_i), this is, the outcome of individual i and the egocentric network of individual i. When such is the case, since (a) networks are independent across egos and (b) the networks are fixed, like the previous case, a simple linear regression model is enough to conduct the analyses. A typical model looks like this:\n\n\\bm{{y}} = \\bm{{\\theta}}_{x}^{\\mathbf{t}}s(\\bm{{X}}) + \\bm{{\\theta}}^{\\mathbf{t}}\\bm{{w}} + \\varepsilon,\\quad \\varepsilon \\sim \\text{N}(0, \\sigma^2)\n\nWhere \\bm{{y}} is a vector of outcomes, \\bm{{X}} is a matrix of egocentric networks, \\bm{{w}} is a vector of covariates, \\bm{{\\theta}} is a vector of coefficients, and \\varepsilon is a vector of errors. The key component here is s(\\bm{{X}}), which is a vector of sufficient statistics of the egocentric networks. For example, if we are interested in the number of ties, s(\\bm{{X}}) is a vector of the number of ties of each ego."
  },
  {
    "objectID": "03-behavior-and-coevolution.html#code-example-egocentric-networks",
    "href": "03-behavior-and-coevolution.html#code-example-egocentric-networks",
    "title": "4  Behavior and coevolution",
    "section": "4.5 Code example: Egocentric networks",
    "text": "4.5 Code example: Egocentric networks\nFor this example, we will simulate a stream of 1,000 Bernoulli graphs looking into the probability of school dropout. Each network will have between 4 and 10 nodes and have a density of 0.4. The data-generating process is as follows:\n\n{\\Pr{}}_{\\bm{{\\theta}}}\\left(Y_i=1\\right) = \\text{logit}^{-1}\\left(\\bm{{\\theta}}_x s(\\bm{{X}}_i) \\right)\n\nWhere s(X) \\equiv \\left(\\text{density}, \\text{n mutual ties}\\right), and \\bm{{\\theta}}_x = (0.5, -1). This model only features sufficient statistics. We start by simulating the networks\n\nset.seed(331)\nn &lt;- 1000\nsizes &lt;- sample(4:10, n, replace = TRUE)\n\n## Simulating the networks\nX &lt;- lapply(sizes, function(x) matrix(rbinom(x^2, 1, 0.4), nrow = x))\nX &lt;- lapply(X, \\(x) {diag(x) &lt;- 0; x})\n\n## Inspecting the first 5\nhead(X, 5)\n\n[[1]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    1    1    0\n[2,]    0    0    0    0    0\n[3,]    0    1    0    0    0\n[4,]    0    0    0    0    0\n[5,]    1    0    0    1    0\n\n[[2]]\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    1    0    1    0\n\n[[3]]\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    0    1    0    1    0    0\n[2,]    0    0    0    0    0    0\n[3,]    0    1    0    0    0    1\n[4,]    0    0    0    0    1    0\n[5,]    0    0    0    0    0    0\n[6,]    0    0    0    0    0    0\n\n[[4]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    1    0    1    0\n[2,]    0    0    0    0    1\n[3,]    0    1    0    0    0\n[4,]    0    1    1    0    1\n[5,]    1    0    1    0    0\n\n[[5]]\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    1    0    0    0    0\n[3,]    1    0    0    0    0\n[4,]    0    0    0    0    0\n[5,]    1    0    0    0    0\n\n\nUsing the ergm R package (Handcock et al. 2023; Hunter et al. 2008), we can extract the associated sufficient statistics of the egocentric networks:\n\nlibrary(ergm)\nstats &lt;- lapply(X, \\(x) summary_formula(x ~ density + mutual))\n\n## Turning the list into a matrix\nstats &lt;- do.call(rbind, stats)\n\n## Inspecting the first 5\nhead(stats, 5)\n\n       density mutual\n[1,] 0.3000000      0\n[2,] 0.1666667      0\n[3,] 0.1666667      0\n[4,] 0.4500000      0\n[5,] 0.1500000      0\n\n\nWe now simulate the outcomes\n\ny &lt;- rbinom(n, 1, plogis(stats %*% c(0.5, -1)))\nglm(y ~ stats, family = binomial(link = \"logit\")) |&gt;\n  summary()\n\n\nCall:\nglm(formula = y ~ stats, family = binomial(link = \"logit\"))\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.07319    0.41590   0.176    0.860    \nstatsdensity  0.42568    1.26942   0.335    0.737    \nstatsmutual  -1.14804    0.12166  -9.436   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 768.96  on 999  degrees of freedom\nResidual deviance: 518.78  on 997  degrees of freedom\nAIC: 524.78\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "03-behavior-and-coevolution.html#network-effects-are-endogenous",
    "href": "03-behavior-and-coevolution.html#network-effects-are-endogenous",
    "title": "4  Behavior and coevolution",
    "section": "4.6 Network effects are endogenous",
    "text": "4.6 Network effects are endogenous\nHere we have two different approaches: Spatial Autocorrelation [SAR], and Autologistic actor attribute model [ALAAM] (Robins, Pattison, and Elliott 2001). The first one is a generalization of the linear regression model that accounts for spatial dependence. The second is a close relative of ERGMs that treats covariates as endogenous and network as exogenous. Overall, ALAAMs are more flexible than SARs, but SARs are easier to estimate.\nSAR Formally, SAR models (see LeSage 2008) can be used to estimate network exposure effects. The general form is:\n\n\\bm{{y}} = \\rho \\bm{{W}} \\bm{{y}} + \\bm{{\\theta}}^{\\mathbf{t}} \\bm{{X}} + \\epsilon,\\quad \\epsilon \\sim \\text{MVN}(0, \\Sigma)\n\nwhere \\bm{{y}}\\equiv \\{y_i\\} is a vector of outcomes the outcome, \\rho is an autocorrelation coefficient, \\bm{{W}} \\in \\{w_{ij}\\} is a row-stochastic square matrix of size n, \\bm{{\\theta}} is a vector of model parameters, \\bm{{X}} is the corresponding matrix with exogenous variables, and \\epsilon is a vector of errors that distributes multivariate normal with mean 0 and covariance make\\Sigma.[^notation] The SAR model is a generalization of the linear regression model that accounts for spatial dependence. The SAR model can be estimated using the spatialreg package in R (Roger Bivand 2022).\n\n\n\n\n\n\nTip\n\n\n\nWhat is the appropriate network to use in the SAR model? According to LeSage and Pace (2014), it is not very important. Since (I_n - \\rho \\mathbf{W})^{-1} = \\rho \\mathbf{W} + \\rho^2 \\mathbf{W}^2 + \\dots.\n\n\nAlthough the SAR model was developed for spatial data, it is easy to apply it to network data. Furthermore, each entry of the vector \\bm{{Wy}} has the same definition as network exposure, namely\n\n\\bm{{Wy}} \\equiv \\left\\{\\sum_{j}y_j w_{ij}\\right\\}_i\n\nSince \\bm{{W}} is row-stochastic, \\bm{{Wy}} is a weighted average of the outcome of the neighbors of i, i.e., a vector of network exposures.\nALAAM The simplest way we can think of this class of models is as if a given covariate switched places with the network in an ERGM, so the network is now fixed and the covariate is the random variable. While ALAAMs can also estimate network exposure effects, we can use them to build more complex models beyond exposure. The general form is:\n\n\\Pr\\left(\\bm{{Y}} = \\bm{{y}}|\\bm{{W}},\\bm{{X}}\\right) = \\exp{\\left(\\bm{{\\theta}}^{\\mathbf{t}}s(\\bm{{y}},\\bm{{W}}, \\bm{{X}})\\right)}\\times\\eta(\\bm{{\\theta}})^{-1}\n\n\n\\eta(\\bm{{\\theta}}) = \\sum_{\\bm{{y}}}\\exp{\\left(\\bm{{\\theta}}^{\\mathbf{t}}s(\\bm{{y}},\\bm{{W}}, \\bm{{X}})\\right)}\n\nWhere \\bm{{Y}}\\equiv \\{y_i \\in (0, 1)\\} is a vector of binary individual outcomes, \\bm{{W}} denotes the social network, \\bm{{X}} is a matrix of exogenous variables, \\bm{{\\theta}} is a vector of model parameters, s(\\bm{{y}},\\bm{{W}}, \\bm{{X}}) is a vector of sufficient statistics, and \\eta(\\bm{{\\theta}}) is a normalizing constant."
  },
  {
    "objectID": "03-behavior-and-coevolution.html#code-example-sar",
    "href": "03-behavior-and-coevolution.html#code-example-sar",
    "title": "4  Behavior and coevolution",
    "section": "4.7 Code example: SAR",
    "text": "4.7 Code example: SAR\nSimulation of SAR models can be done using the following observation: Although the outcome shows on both sides of the equation, we can isolate it in one side and solve for it; formally:\n\n\\bm{{y}} = \\rho \\bm{{X}} \\bm{{y}} + \\bm{{\\theta}}^{\\mathbf{t}}\\bm{{W}} + \\varepsilon \\implies \\bm{{y}} = \\left(\\bm{{I}} - \\rho \\bm{{X}}\\right)^{-1}\\bm{{\\theta}}^{\\mathbf{t}}\\bm{{W}} + \\left(\\bm{{I}} - \\rho \\bm{{X}}\\right)^{-1}\\varepsilon\n\nThe following code chunk simulates a SAR model with a Bernoulli graph with 1,000 nodes and a density of 0.01. The data-generating process is as follows:\n\nset.seed(4114)\nn &lt;- 1000\n\n## Simulating the network\np &lt;- 0.01\nX &lt;- matrix(rbinom(n^2, 1, p), nrow = n)\n\n## Covariate\nW &lt;- matrix(rnorm(n), nrow = n)\n\n## Simulating the outcome\nrho &lt;- 0.5\nlibrary(MASS) # For the mvrnorm function\n\n## Identity minus rho * X\nX_rowstoch &lt;- X / rowSums(X)\nI &lt;- diag(n) - rho * X_rowstoch\n\n## The outcome\nY &lt;- solve(I) %*% (2 * W) + solve(I) %*% mvrnorm(1, rep(0, n), diag(n))\n\nUsing the spatialreg R package, we can fit the model using the lagsarlm function:\n\nlibrary(spdep) # for the function mat2listw\nlibrary(spatialreg)\nfit &lt;- lagsarlm(\n  Y ~ W,\n  data  = as.data.frame(X),\n  listw = mat2listw(X_rowstoch)\n  )\n\n\n## Using texreg to get a pretty print\ntexreg::screenreg(fit, single.row = TRUE)\n\n\n========================================\n                     Model 1            \n----------------------------------------\n(Intercept)             -0.01 (0.03)    \nW                        1.97 (0.03) ***\nrho                      0.54 (0.04) ***\n----------------------------------------\nNum. obs.             1000              \nParameters               4              \nLog Likelihood       -1373.02           \nAIC (Linear model)    2920.37           \nAIC (Spatial model)   2754.05           \nLR test: statistic     168.32           \nLR test: p-value         0.00           \n========================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nThe interpretation of this model is almost the same as a linear regression, with the difference that we have the autocorrelation effect (rho). As expected, the model got an estimate close enough to the population parameter: \\rho = 0.5."
  },
  {
    "objectID": "03-behavior-and-coevolution.html#code-example-alaam",
    "href": "03-behavior-and-coevolution.html#code-example-alaam",
    "title": "4  Behavior and coevolution",
    "section": "4.8 Code example: ALAAM",
    "text": "4.8 Code example: ALAAM\nTo date, there is no R package implementing the ALAAM framework. Nevertheless, you can fit ALAAMs using the PNet software developed by the Melnet group at the University of Melbourne (click here).\nBecause of the similarities, ALAAMs can be implemented using ERGMs. Because of the novelty of it, the coding example will be left as a potential class project. We will post a fully-featured example after the workshop."
  },
  {
    "objectID": "03-behavior-and-coevolution.html#coevolution",
    "href": "03-behavior-and-coevolution.html#coevolution",
    "title": "4  Behavior and coevolution",
    "section": "4.9 Coevolution",
    "text": "4.9 Coevolution\nFinally, we discuss coevolution when both network and behavior are embedded in a feedback loop. Coevolution should be the default assumption when dealing with social networks. Nevertheless, models capable of capturing coevolution are hard to estimate. Here, we will discuss two of such models: Stochastic Actor-Oriented Models (or Siena Models) (first introduced in T. a. B. Snijders (1996); see also T. A. B. Snijders (2017)) and Exponential-family Random Exponential-family Random Network Models [ERNMs,] a generalization of ERGMs (Wang, Fellows, and Handcock 2023; Fellows 2012).\nSiena Stochastic Actor-Oriented Models [SOAMs] or Siena Models are dynamic models of network and behavior that describe the transition of a network system within two or more time points. \nERNMs This model is closely related to ERGMs, with the difference that they incorporate a vertex-level output. Conceptually, it is moving from a having a random network, to a model where a given vertex feature and network are random:\n\nP_{\\mathcal{Y}, \\bm{{\\theta}}}(\\bm{{Y}}=\\bm{{y}}|\\bm{{X}}=\\bm{{x}}) \\to P_{\\mathcal{Y}, \\bm{{\\theta}}}(\\bm{{Y}}=\\bm{{y}}, \\bm{{X}}=\\bm{{x}})"
  },
  {
    "objectID": "03-behavior-and-coevolution.html#code-example-siena",
    "href": "03-behavior-and-coevolution.html#code-example-siena",
    "title": "4  Behavior and coevolution",
    "section": "4.10 Code example: Siena",
    "text": "4.10 Code example: Siena\nThis example was adapted from the RSiena R package (see ?sienaGOF-auxiliary page).We start by loading the package and taking a look at the data we will use:\n\nlibrary(RSiena)\n\n## Visualizing the adjacency matrix & behavior\nop &lt;- par(mfrow=c(2, 2))\nimage(s501, main = \"Net: s501\")\nimage(s502, main = \"Net: s502\")\nhist(s50a[,1], main = \"Beh1\")\nhist(s50a[,2], main = \"Beh2\")\n\n\n\npar(op)\n\nThe next step is the data preparation process. RSiena does not receive raw data as is. We need to explicitly declare the networks and outcome variable. Siena models can also model network changes\n\n## Initializing the dependent variable (network)\nmynet1 &lt;- sienaDependent(array(c(s501, s502), dim=c(50, 50, 2)))\nmynet1\n\nType         oneMode             \nObservations 2                   \nNodeset      Actors (50 elements)\n\nmybeh  &lt;- sienaDependent(s50a[,1:2], type=\"behavior\")\nmybeh\n\nType         behavior            \nObservations 2                   \nNodeset      Actors (50 elements)\n\n## Node-level covariates (artificial)\nmycov  &lt;- c(rep(1:3,16),1,2)\n\n## Edge-level covariates (also artificial)\nmydycov &lt;- matrix(rep(1:5, 500), 50, 50) \n\n\n## Creating the data object\nmydata &lt;- sienaDataCreate(mynet1, mybeh)\n\n## Adding the effects (first get them!)\nmyeff &lt;- getEffects(mydata)\n\n## Notice that Siena adds some default effects\nmyeff\n##   name   effectName                  include fix   test  initialValue parm\n## 1 mynet1 basic rate parameter mynet1 TRUE    FALSE FALSE    4.69604   0   \n## 2 mynet1 outdegree (density)         TRUE    FALSE FALSE   -1.48852   0   \n## 3 mynet1 reciprocity                 TRUE    FALSE FALSE    0.00000   0   \n## 4 mybeh  rate mybeh period 1         TRUE    FALSE FALSE    0.70571   0   \n## 5 mybeh  mybeh linear shape          TRUE    FALSE FALSE    0.32247   0   \n## 6 mybeh  mybeh quadratic shape       TRUE    FALSE FALSE    0.00000   0\n\n## Adding a few extra effects (automatically prints them out)\nmyeff &lt;- includeEffects(myeff, transTies, cycle3)\n##   effectName      include fix   test  initialValue parm\n## 1 3-cycles        TRUE    FALSE FALSE          0   0   \n## 2 transitive ties TRUE    FALSE FALSE          0   0\n\nTo add more effects, first, call the function effectsDocumentation(myeff). It will show you explicitly how to add a particular effect. For instance, if we wanted to add network exposure (avExposure,) under the documentation of effectsDocumentation(myeff) we need to pass the following arguments:\n\n## And now, exposure effect\nmyeff &lt;- includeEffects(\n  myeff,\n  avExposure,\n  # These last three are specified by effectsDocum...\n  name         = \"mybeh\",\n  interaction1 = \"mynet1\",\n  type         = \"rate\"\n  )\n\n  effectName                            include fix   test  initialValue parm\n1 average exposure effect on rate mybeh TRUE    FALSE FALSE          0   0   \n\n\nThe next step involves creating the model with (sienaAlgorithmCreate,) where we specify all the parameters for fitting the model (e.g., MCMC steps.) Here, we modified the values of n3 and nsub to half of the default values to reduce the time it would take to fit the model; yet this degrades the quality of the fit.\n\n## Shorter phases 2 and 3, just for example:\nmyalgorithm &lt;- sienaAlgorithmCreate(\n  nsub = 2, n3 = 500, seed = 122, projname = NULL\n  )\n## If you use this algorithm object, siena07 will create/use an output file Siena.txt .\n \n## Fitting and printing the model\nans &lt;- siena07(\n  myalgorithm,\n  data = mydata, effects = myeff,\n  returnDeps = TRUE, batch = TRUE\n  )\n## \n## Start phase 0 \n## theta:  4.696 -1.489  0.000  0.000  0.000  0.706  0.000  0.322  0.000 \n## \n## Start phase 1 \n## Phase 1 Iteration 1 Progress: 0%\n## Phase 1 Iteration 2 Progress: 0%\n## Phase 1 Iteration 3 Progress: 0%\n## Phase 1 Iteration 4 Progress: 0%\n## Phase 1 Iteration 5 Progress: 0%\n## Phase 1 Iteration 10 Progress: 1%\n## Phase 1 Iteration 15 Progress: 1%\n## Phase 1 Iteration 20 Progress: 1%\n## Phase 1 Iteration 25 Progress: 2%\n## Phase 1 Iteration 30 Progress: 2%\n## Phase 1 Iteration 35 Progress: 2%\n## Phase 1 Iteration 40 Progress: 3%\n## Phase 1 Iteration 45 Progress: 3%\n## Phase 1 Iteration 50 Progress: 3%\n## theta:  5.380 -1.734  0.481  0.147  0.166  1.168 -0.340  0.250  0.140 \n## \n## Start phase 2.1\n## Phase 2 Subphase 1 Iteration 1 Progress: 33%\n## Phase 2 Subphase 1 Iteration 2 Progress: 33%\n## theta  6.044 -1.877  0.840  0.300  0.473  1.096 -0.332  0.112  0.191 \n## ac  0.375 -0.183  3.835  4.574 23.412  0.922  0.908  0.626  3.302 \n## Phase 2 Subphase 1 Iteration 3 Progress: 33%\n## Phase 2 Subphase 1 Iteration 4 Progress: 33%\n## theta  6.1075 -2.0372  1.2512  0.0341  0.5207  1.2593 -0.1534  0.3018  0.1928 \n## ac  0.9859  0.2280 -0.0703 -2.2973 -2.7455  1.1518  1.0749  0.8732  0.5399 \n## Phase 2 Subphase 1 Iteration 5 Progress: 33%\n## Phase 2 Subphase 1 Iteration 6 Progress: 33%\n## theta  6.8650 -2.3185  1.7577  0.2694  0.7636  1.1997 -0.0433  0.3000  0.1762 \n## ac  0.4789  0.4883  0.0199 -1.9449 -2.2609  1.1444  1.0397  0.9282  0.6470 \n## Phase 2 Subphase 1 Iteration 7 Progress: 33%\n## Phase 2 Subphase 1 Iteration 8 Progress: 33%\n## theta  6.801140 -2.485273  1.966471  0.301197  0.817324  1.228010  0.000627  0.377719  0.072274 \n## ac  0.4538  0.4933 -0.0158 -1.9326 -2.2588  1.1102  1.0395  0.9239  0.6905 \n## Phase 2 Subphase 1 Iteration 9 Progress: 33%\n## Phase 2 Subphase 1 Iteration 10 Progress: 33%\n## theta  6.1258 -2.5124  1.8704  0.1206  0.6054  1.4538 -0.0145  0.5091 -0.0671 \n## ac  0.472  0.103 -0.330 -1.625 -1.991  1.152  1.090  0.767  0.514 \n## Phase 2 Subphase 1 Iteration 1 Progress: 33%\n## Phase 2 Subphase 1 Iteration 2 Progress: 33%\n## Phase 2 Subphase 1 Iteration 3 Progress: 33%\n## Phase 2 Subphase 1 Iteration 4 Progress: 33%\n## Phase 2 Subphase 1 Iteration 5 Progress: 33%\n## Phase 2 Subphase 1 Iteration 6 Progress: 34%\n## Phase 2 Subphase 1 Iteration 7 Progress: 34%\n## Phase 2 Subphase 1 Iteration 8 Progress: 34%\n## Phase 2 Subphase 1 Iteration 9 Progress: 34%\n## Phase 2 Subphase 1 Iteration 10 Progress: 34%\n## theta  6.4976 -2.5900  1.9265  0.2750  0.8543  1.0420  0.0446  0.3234 -0.0686 \n## ac -0.212 -0.697 -0.818 -0.831 -0.765 -0.442 -0.268 -0.240 -0.267 \n## theta:  6.4976 -2.5900  1.9265  0.2750  0.8543  1.0420  0.0446  0.3234 -0.0686 \n## \n## Start phase 2.2\n## Phase 2 Subphase 2 Iteration 1 Progress: 48%\n## Phase 2 Subphase 2 Iteration 2 Progress: 48%\n## Phase 2 Subphase 2 Iteration 3 Progress: 48%\n## Phase 2 Subphase 2 Iteration 4 Progress: 48%\n## Phase 2 Subphase 2 Iteration 5 Progress: 48%\n## Phase 2 Subphase 2 Iteration 6 Progress: 48%\n## Phase 2 Subphase 2 Iteration 7 Progress: 49%\n## Phase 2 Subphase 2 Iteration 8 Progress: 49%\n## Phase 2 Subphase 2 Iteration 9 Progress: 49%\n## Phase 2 Subphase 2 Iteration 10 Progress: 49%\n## theta  6.7988 -2.5810  1.9335  0.3941  0.7843  1.1111  0.0358  0.3252 -0.0415 \n## ac -0.0757 -0.3672 -0.4025 -0.3676 -0.4157 -0.0166  0.0222 -0.0874  0.0651 \n## theta:  6.7988 -2.5810  1.9335  0.3941  0.7843  1.1111  0.0358  0.3252 -0.0415 \n## \n## Start phase 3 \n## Phase 3 Iteration 500 Progress 100%\n\nans\n## Estimates, standard errors and convergence t-ratios\n## \n##                                                 Estimate   Standard   Convergence \n##                                                              Error      t-ratio   \n## Network Dynamics \n##   1. rate basic rate parameter mynet1            6.7988  ( 1.2496   )   -0.0460   \n##   2. eval outdegree (density)                   -2.5810  ( 0.1505   )    0.0113   \n##   3. eval reciprocity                            1.9335  ( 0.2627   )    0.0205   \n##   4. eval 3-cycles                               0.3941  ( 0.2742   )   -0.0658   \n##   5. eval transitive ties                        0.7843  ( 0.2379   )   -0.0014   \n## \n## Behavior Dynamics\n##   6. rate rate mybeh period 1                    1.1111  ( 1.3096   )   -0.0199   \n##   7. rate average exposure effect on rate mybeh  0.0358  ( 0.4373   )   -0.0190   \n##   8. eval mybeh linear shape                     0.3252  ( 0.2329   )   -0.0048   \n##   9. eval mybeh quadratic shape                 -0.0415  ( 0.1139   )    0.0992   \n## \n## Overall maximum convergence ratio:    0.2163 \n## \n## \n## Total of 940 iteration steps.\n\nAs a rule of thumb, absolute t-values below 0.1 show good convergence, below 0.2 we say “reasonably well,” and above is no convergence. Let’s highlight two of the effects we have in our model\n\nTransitive ties (number five) are positive 0.78 with a t-value of smaller than 0.01. Thus, we say that the network has a tendency towards transitivity (balance) that is significant.\nExposure effect (number seven) is also positive, but small, 0.03, but still significant (t-value of -0.01)\n\n:Like with ERGMs, we also do goodness-of-fit:\n\nsienaGOF(ans, OutdegreeDistribution, varName=\"mynet1\") |&gt;\n  plot()\n\n\n\nsienaGOF(ans, BehaviorDistribution, varName = \"mybeh\") |&gt; \n  plot()\n\nNote: some statistics are not plotted because their variance is 0.\nThis holds for the statistic: 5."
  },
  {
    "objectID": "03-behavior-and-coevolution.html#code-example-ernm",
    "href": "03-behavior-and-coevolution.html#code-example-ernm",
    "title": "4  Behavior and coevolution",
    "section": "4.11 Code example: ERNM",
    "text": "4.11 Code example: ERNM\nTo date, there is no CRAN release for the ERNM model. The only implementation I am aware of is one of the leading authors, which is available on GitHub: https://github.com/fellstat/ernm. Unfortunately, the current version of the package seems to be broken.\nJust like the ALAAM case, as ERNMs are closely related to ERGMS, building an example using the ERGM package could be a great opportunity for a class project!\n\n\n\n\nFellows, Ian E. 2012. “Exponential Family Random Network Models.” ProQuest Dissertations and Theses. PhD thesis. https://login.ezproxy.lib.utah.edu/login?url=https://www.proquest.com/dissertations-theses/exponential-family-random-network-models/docview/1221548720/se-2.\n\n\nHandcock, Mark S., David R. Hunter, Carter T. Butts, Steven M. Goodreau, Pavel N. Krivitsky, and Martina Morris. 2023. Ergm: Fit, Simulate and Diagnose Exponential-Family Models for Networks. The Statnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.\n\n\nHaye, Kayla de la, Heesung Shin, George G. Vega Yon, and Thomas W. Valente. 2019. “Smoking Diffusion Through Networks of Diverse, Urban American Adolescents over the High School Period.” Journal of Health and Social Behavior. https://doi.org/10.1177/0022146519870521.\n\n\nHunter, David R., Mark S. Handcock, Carter T. Butts, Steven M. Goodreau, and Martina Morris. 2008. “ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.” Journal of Statistical Software 24 (3): 1–29. https://doi.org/10.18637/jss.v024.i03.\n\n\nLeSage, James P. 2008. “An Introduction to Spatial Econometrics.” Revue d’économie Industrielle 123 (123): 19–44. https://doi.org/10.4000/rei.3887.\n\n\nLeSage, James P., and R. Kelley Pace. 2014. “The Biggest Myth in Spatial Econometrics.” Econometrics 2 (4): 217–49. https://doi.org/10.2139/ssrn.1725503.\n\n\nLusher, Dean, Johan Koskinen, and Garry Robins. 2013. Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications. Cambridge University Press.\n\n\nRobins, Garry, Philippa Pattison, and Peter Elliott. 2001. “Network Models for Social Influence Processes.” Psychometrika 66 (2): 161–89. https://doi.org/10.1007/BF02294834.\n\n\nRoger Bivand. 2022. “R Packages for Analyzing Spatial Data: A Comparative Case Study with Areal Data.” Geographical Analysis 54 (3): 488–518. https://doi.org/10.1111/gean.12319.\n\n\nSnijders, Tom a B. 1996. “Stochastic Actor-Oriented Models for Network Change.” The Journal of Mathematical Sociology 21 (1-2): 149–72. https://doi.org/10.1080/0022250X.1996.9990178.\n\n\nSnijders, Tom A. B. 2017. “Stochastic Actor-Oriented Models for Network Dynamics.” Annual Review of Statistics and Its Application 4 (1): 343–63. https://doi.org/10.1146/annurev-statistics-060116-054035.\n\n\nValente, Thomas W., and George G. Vega Yon. 2020. “Diffusion/Contagion Processes on Social Networks.” Health Education & Behavior 47 (2): 235–48. https://doi.org/10.1177/1090198120901497.\n\n\nValente, Thomas W., Heather Wipfli, and George G. Vega Yon. 2019. “Network Influences on Policy Implementation: Evidence from a Global Health Treaty.” Social Science and Medicine. https://doi.org/10.1016/j.socscimed.2019.01.008.\n\n\nWang, Zeyi, Ian E. Fellows, and Mark S. Handcock. 2023. “Understanding Networks with Exponential-Family Random Network Models.” Social Networks, August, S0378873323000497. https://doi.org/10.1016/j.socnet.2023.07.003."
  },
  {
    "objectID": "04-lab-1.html#types-of-problems",
    "href": "04-lab-1.html#types-of-problems",
    "title": "5  Lab 1",
    "section": "5.1 Types of problems",
    "text": "5.1 Types of problems\n\nModeling with networks Of the following cases, which ones can be treated with “regular statistical methods”? Justify your answers by identifying what is the unit of analysis in your “regression”/“test” and specify whether the units are independent or not.\n\n\nEgocentric study 1: Analyze how the network structure affects health outcomes.\nEgocentric study 2: Investigate what network structures are more prevalent in a population sample.\nEgocentric study 3: Elucidate whether the prevalence of a given network structure is higher in one population than in another.\nCountry-level networks: Analyze whether neighboring countries tend to adopt international treaties at the same time.\nPhylogenomics: Study a given phenotype in a population of organisms related by a phylogenetic tree."
  },
  {
    "objectID": "04-lab-1.html#programming",
    "href": "04-lab-1.html#programming",
    "title": "5  Lab 1",
    "section": "5.2 Programming",
    "text": "5.2 Programming\n\nSimulating convolutions: Using what you have learned about statistical functions in R, simulate the convolution of two normal distributions, one with (\\mu, \\sigma^2) = (-3, 1) and the other with (\\mu, \\sigma^2) = (2, 2). Plot the histogram of the results. Draw 1,000 samples.\n\n\nBimodal distribution: Using the previous two normal distributions, simulate a bimodal distribution where the probability of sampling from the first distribution is 0.3 and the probability of sampling from the second distribution is 0.7. Plot the histogram of the results. (Hint: use a combination of runif() and ifelse())."
  },
  {
    "objectID": "04-lab-1.html#random-graphs",
    "href": "04-lab-1.html#random-graphs",
    "title": "5  Lab 1",
    "section": "5.3 Random graphs",
    "text": "5.3 Random graphs\n\nWrite a function to simulate Bernoulli graph, but instead of evaluating each (i,j) tie individually like in (er-code?), use a binomial random number to identify how many edges the graph will have, and randomly pick which will those be from the set V\\times V\nUsing a Generalized-Linear-Model [GLM], estimate the density parameter of the previous graph."
  },
  {
    "objectID": "05-intro-to-ergms.html#introduction",
    "href": "05-intro-to-ergms.html#introduction",
    "title": "6  Introduction to ERGMs",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nExponential-Family Random Graph Models, known as ERGMs, ERG Models, or p^* models (Holland and Leinhardt (1981), Frank and Strauss (1986), Wasserman and Pattison (1996), Snijders et al. (2006), Robins et al. (2007), and many others) are a large family of statistical distributions for random graphs. In ERGMs, the focus is on the processes that give origin to the network rather than the individual ties.1\nThe most common representation of ERGMs is the following:\n\nP_{\\mathcal{Y}, \\bm{{\\theta}}}(\\bm{{Y}}=\\bm{{y}}) =  \\exp\\left(\\bm{{\\theta}}^{\\mathbf{t}} s(y)\\right)\\kappa(\\bm{{\\theta}})^{-1}\n\nwhere \\bm{{Y}} is a random graph, \\bm{{y}}\\in \\mathcal{Y} is a particular realization of Y, \\bm{{\\theta}} is a vector of parameters, s(\\bm{{y}}) is a vector of sufficient statistics, and \\kappa(\\bm{{\\theta}}) is a normalizing constant. The normalizing constant is defined as:\n\n\\kappa(\\bm{{\\theta}}) = \\sum_{\\bm{{y}} \\in \\mathcal{Y}} \\exp\\left(\\bm{{\\theta}}^{\\mathbf{t}} s(\\bm{{y}})\\right)\n\nFrom the statistical point of view, the normalizing constant makes this model attractive; only cases where \\mathcal{Y} is small enough to be enumerated are feasible (Vega Yon, Slaughter, and Haye 2021). Because of that reason, estimating ERGMs is a challenging task.\nIn more simple terms, ERG models resemble Logistic regressions. The term \\bm{{\\theta}}^{\\mathbf{t}} s(\\bm{{y}}) is simply the sum of the product between the parameters and the statistics (like you would see in a Logistic regression,) and its key component is the composition of the vector of sufficient statistics, s(\\bm{{y}}). The latter is what defines the model.\nThe vector of sufficient statistics dictates the data-generating process of the network. Like the mean and variance characterize the normal distribution, sufficient statistics (and corresponding parameters) characterize the ERG distribution. Figure 6.1 shows some examples of sufficient statistics with their corresponding parametrizations.\n\n\n\nFigure 6.1: Example ERGM terms from Vega Yon, Slaughter, and Haye (2021)\n\n\nThe ergm package has many terms we can use to fit these models. You can explore the available terms in the manual ergm.terms in the ergm package. Take a moment to explore the manual and see the different terms available."
  },
  {
    "objectID": "05-intro-to-ergms.html#dyadic-independence-p1",
    "href": "05-intro-to-ergms.html#dyadic-independence-p1",
    "title": "6  Introduction to ERGMs",
    "section": "6.2 Dyadic independence (p1)",
    "text": "6.2 Dyadic independence (p1)\nThe simplest ERG model we can fit is a Bernoulli (Erdos-Renyi) model. Here, the only statistic is the edgecount. Now, if we can write the function computing sufficient statistics as a sum over all edges, i.e., s(\\bm{{y}}) = \\sum_{ij}s'(y_{ij}), the model reduces to a Logistic regression.\n\nlibrary(ergm)\n\n## Simulating a Bernoulli network\nY &lt;- matrix(rbinom(100^2, 1, 0.1), 100, 100)\ndiag(Y) &lt;- NA\n\ncbind(\n  logit = glm(as.vector(Y) ~ 1, family = binomial) |&gt; coef(),\n  ergm  = ergm(Y ~ edges) |&gt; coef(),\n  avg   = mean(Y, na.rm = TRUE) |&gt; qlogis()\n)\n##                 logit      ergm       avg\n## (Intercept) -2.170574 -2.170574 -2.170574\n\nWhen we see this, we say that dyads are independent of each other; this is also called p1 models (Holland and Leinhardt 1981). As a second example, imagine that we are interested in assessing whether gender homophily (the tendency between individuals of the same gender to connect) is present in a network. ERG models are the right tool for this task. Moreover, if we assume that gender homophily is the only mechanism that governs the network, the problem reduces to a Logistic regression:\n\nP_{\\mathcal{Y}, \\bm{{\\theta}}}(y_{ij} = 1) = \\text{Logit}^{-1}\\left(\\theta_{edges} + \\theta_{homophily}\\mathbb{1}\\left(X_i = X_j\\right)\\right)\n\nwhere \\mathbb{1}\\left(\\cdot\\right) is the indicator function, and X_i equals one if the ith individual is female, and zero otherwise. To see this, let’s simulate some data. We will use the simulate_formula function in the ergm package. All we need is a network, a model, and some arbitrary coefficients. We start with an indicator vector for gender (1: female, male otherwise):\n\n## Simulating data\nset.seed(7731)\nX &lt;- rbinom(100, 1, 0.5)\nhead(X)\n\n[1] 0 1 0 0 1 0\n\n\nNext, we use the functions from the ergm and network packages to simulate a network with homophily:\n\n## Simulating the network with homophily\nlibrary(ergm)\n\n## A network with 100 vertices\nY &lt;- network(100) \n\n## Adding the vertex attribute with the %v% operator\nY %v% \"X\" &lt;- X\n\n## Simulating the network\nY &lt;- ergm::simulate_formula(\n  # The model\n  Y ~ edges + nodematch(\"X\"),\n  # The coefficients\n  coef = c(-3, 2)\n  )\n\nIn this case, the network tends to be sparse (negative edges coefficient) and present homophilic ties (positive nodematch coefficient). Using the sna package (also from the statnet suite), we can plot the network:\n\nlibrary(sna)\ngplot(Y, vertex.col = c(\"green\", \"red\")[Y %v% \"X\" + 1])\n\n\n\n\nUsing the ERGM package, we can fit the model using code very similar to the one used to simulate the network:\n\nfit_homophily &lt;- ergm(Y ~ edges + nodematch(\"X\"))\n\nWe will check the results later and compare them against the following: the Logit model. The following code chunk implements the logistic regression model for this particular model. This example only proves that ERGMs are a generalization of Logistic regression.\n\n\n\n\n\n\nOnly to learn!\n\n\n\nThis example was created only for learning purposes. In practice, you should always use the ergm package or PNet software to fit ERGMs.\n\n\n\n\nCode\nn &lt;- 100\nsstats &lt;- summary_formula(Y ~ edges + nodematch(\"X\"))\nY_mat &lt;- as.matrix(Y)\ndiag(Y_mat) &lt;- NA\n\n## To speedup computations, we pre-compute this value\nX_vec &lt;- outer(X, X, \"==\") |&gt; as.numeric()\n\n## Function to compute the negative loglikelihood\nobj &lt;- \\(theta) {\n\n  # Compute the probability according to the value of Y\n  p &lt;- ifelse(\n    Y_mat == 1,\n\n    # If Y = 1\n    plogis(theta[1] + X_vec * theta[2]),\n\n    # If Y = 0\n    1 - plogis(theta[1] + X_vec * theta[2])\n  )\n\n  # The -loglikelihood\n  -sum(log(p[!is.na(p)]))\n  \n\n}\n\n\nAnd, using the optim function, we can fit the model:\n\n## Fitting the model\nfit_homophily_logit &lt;- optim(c(0,0), obj, hessian = TRUE)\n\nNow that we have the values let’s compare them:\n\n## The coefficients\ncbind(\n  theta_ergm  = coef(fit_homophily),\n  theta_logit = fit_homophily_logit$par,\n  sd_ergm     = vcov(fit_homophily) |&gt; diag() |&gt; sqrt(),\n  sd_logit    = sqrt(diag(solve(fit_homophily_logit$hessian)))\n)\n\n            theta_ergm theta_logit    sd_ergm   sd_logit\nedges        -2.973331   -2.973829 0.06659648 0.06661194\nnodematch.X   1.926039    1.926380 0.07395580 0.07397025\n\n\nAs you can see, both models yielded the same estimates because they are the same! Before continuing, let’s review a couple of important results in ERGMs."
  },
  {
    "objectID": "05-intro-to-ergms.html#the-most-important-results",
    "href": "05-intro-to-ergms.html#the-most-important-results",
    "title": "6  Introduction to ERGMs",
    "section": "6.3 The most important results",
    "text": "6.3 The most important results\nIf we were able to say what two of the most important results in ERGMs are, I would say the following: (a) conditioning on the rest of the graph, the probability of a tie distributes Logistic (Bernoulli), and (b) the ratio between two loglikelihoods can be approximated through simulation."
  },
  {
    "objectID": "05-intro-to-ergms.html#the-logistic-distribution",
    "href": "05-intro-to-ergms.html#the-logistic-distribution",
    "title": "6  Introduction to ERGMs",
    "section": "6.4 The logistic distribution",
    "text": "6.4 The logistic distribution\nLet’s start by stating the result: Conditioning on all graphs that are not y_{ij}, the probability of a tie Y_{ij} is distributed Logistic; formally:\n\nP_{\\mathcal{Y}, \\bm{{\\theta}}}(Y_{ij}=1 | \\bm{{y}}_{-ij}) = \\frac{1}{1 + \\exp \\left(\\bm{{\\theta}}^{\\mathbf{t}}\\delta_{ij}(\\bm{{y}}){}\\right)},\n\nwhere \\delta_{ij}(\\bm{{y}}){}\\equiv s_{ij}^+(\\bm{{y}}) - s_{ij}^-(\\bm{{y}}) is the change statistic, and s_{ij}^+(\\bm{{y}}) and s_{ij}^-(\\bm{{y}}) are the statistics of the graph with and without the tie Y_{ij}, respectively.\nThe importance of this result is two-fold: (a) we can use this equation to interpret fitted models in the context of a single graph (like using odds,) and (b) we can use this equation to simulate from the model, without touching the normalizing constant."
  },
  {
    "objectID": "05-intro-to-ergms.html#the-ratio-of-loglikelihoods",
    "href": "05-intro-to-ergms.html#the-ratio-of-loglikelihoods",
    "title": "6  Introduction to ERGMs",
    "section": "6.5 The ratio of loglikelihoods",
    "text": "6.5 The ratio of loglikelihoods\nThe second significant result is that the ratio of loglikelihoods can be approximated through simulation. It is based on the following observation by Geyer and Thompson (1992):\n\n\\frac{\\kappa(\\bm{{\\theta}})}{\\kappa(\\bm{{\\theta}}_0)} = \\mathbb{E}_{\\mathcal{Y}, \\bm{{\\theta}}_0}\\left((\\bm{{\\theta}} - \\bm{{\\theta}}_0)s(\\bm{{y}})^{\\mathbf{t}}\\right),\n\nUsing the latter, we can approximate the following loglikelihood ratio:\n\\begin{align*}\nl(\\bm{{\\theta}}) - l(\\bm{{\\theta}}_0) = & (\\bm{{\\theta}} - \\bm{{\\theta}}_0)^{\\mathbf{t}}s(\\bm{{y}}) - \\log\\left[\\frac{\\kappa(\\bm{{\\theta}})}{\\kappa(\\bm{{\\theta}}_0)}\\right]\\\\\n\\approx & (\\bm{{\\theta}} - \\bm{{\\theta}}_0)^{\\mathbf{t}}s(\\bm{{y}}) - \\log\\left[M^{-1}\\sum_{\\bm{{y}}^{(m)}} (\\bm{{\\theta}} - \\bm{{\\theta}}_0)^{\\mathbf{t}}s(\\bm{{y}}^{(m)})\\right]\n\\end{align*}\nWhere \\bm{{\\theta}}_0 is an arbitrary vector of parameters, and \\bm{{y}}^{(m)} are sampled from the distribution P_{\\mathcal{Y}, \\bm{{\\theta}}_0}. In the words of Geyer and Thompson (1992), “[…] it is possible to approximate \\bm{{\\theta}} by using simulations from one distribution P_{\\mathcal{Y}, \\bm{{\\theta}}_0} no matter which \\bm{{\\theta}}_0 in the parameter space is.”"
  },
  {
    "objectID": "05-intro-to-ergms.html#start-to-finish-example",
    "href": "05-intro-to-ergms.html#start-to-finish-example",
    "title": "6  Introduction to ERGMs",
    "section": "6.6 Start to finish example",
    "text": "6.6 Start to finish example\nThere is no silver bullet to fit ERGMs. However, the following steps are a good starting point:\n\nInspect the data: Ensure the network you work with is processed correctly. Some common mistakes include isolates being excluded, vertex attributes not being adequately aligned (mismatch), etc.\nStart with endogenous effects first: Before jumping into vertex/edge-covariates, try fitting models that only include structural terms such as edgecount, triangles (or their equivalent,) stars, reciprocity, etc.\nAfter structure is controlled: You can add vertex/edge-covariates. The most common ones are homophily, nodal-activity, etc.\nEvaluate your results: Once you have a model you are happy with, the last couple of steps are (a) assess convergence (which is usually done automagically by the ergm package,) and (b) assess goodness-of-fit, which in this context means how good was our model to capture (not-controlled for) properties of the network.\n\nAlthough we could go ahead and use an existing dataset to play with, instead, we will simulate a directed graph with the following properties:\n\n100 nodes.\nHomophily on Music taste.\nGender heterophily.\nReciprocity\n\nThe following code chunk illustrates how to do this in R. Notice that, like the previous example, we need to create a network with the vertex attributes needed to simulate homophily.\n\n## Simulating the covariates (vertex attribute)\nset.seed(1235)\n\n## Simulating the data\nY &lt;- network(100, directed = TRUE)\nY %v% \"fav_music\" &lt;- sample(c(\"rock\", \"jazz\", \"pop\"), 100, replace = TRUE)\nY %v% \"female\"    &lt;- rbinom(100, 1, 0.5) \n\nNow that we have a starting point for the simulation, we can use the simulate_formula function to get our network:\n\nY &lt;- ergm::simulate_formula(\n  Y ~\n    edges +\n    nodematch(\"fav_music\") + \n    nodematch(\"female\") +\n    mutual,\n  coef = c(-4, 1, -1, 2)\n  )\n\n## And visualize it!\ngplot(Y, vertex.col = c(\"green\", \"red\")[Y %v% \"female\" + 1])\n\n\n\n\nThis figure is precisely why we need ERGMs (and why many Sunbelt talks don’t include a network visualization!). We know the graph has structure (it’s not random), but visually, it is hard to see."
  },
  {
    "objectID": "05-intro-to-ergms.html#inspect-the-data",
    "href": "05-intro-to-ergms.html#inspect-the-data",
    "title": "6  Introduction to ERGMs",
    "section": "6.7 Inspect the data",
    "text": "6.7 Inspect the data\nFor the sake of time, we will not take the time to investigate our network properly. However, you should always do so. Make sure you do descriptive statistics (density, centrality, modularity, etc.), check missing values, isolates (disconnected nodes), and inspect your data visually through “notepad” and visualizations before jumping into your ERG model."
  },
  {
    "objectID": "05-intro-to-ergms.html#start-with-endogenous-effects-first",
    "href": "05-intro-to-ergms.html#start-with-endogenous-effects-first",
    "title": "6  Introduction to ERGMs",
    "section": "6.8 Start with endogenous effects first",
    "text": "6.8 Start with endogenous effects first\nThe step is to check whether we can fit an ERGM or not. We can do so with the Bernoulli graph:\n\nmodel_1 &lt;- ergm(Y ~ edges)\nsummary(model_1)\n\nCall:\nergm(formula = Y ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.78885    0.06833      0  -55.45   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 13724  on 9900  degrees of freedom\n Residual Deviance:  2102  on 9899  degrees of freedom\n \nAIC: 2104  BIC: 2112  (Smaller is better. MC Std. Err. = 0)\n\n\nIt is rare to see a model in which the edgecount is not significant. The next term we will add is reciprocity (mutual in the ergm package)\n\nmodel_2 &lt;- ergm(Y ~ edges + mutual) \nsummary(model_2)\n## Call:\n## ergm(formula = Y ~ edges + mutual)\n## \n## Monte Carlo Maximum Likelihood Results:\n## \n##        Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \n## edges  -3.93277    0.07833      0 -50.205   &lt;1e-04 ***\n## mutual  2.15152    0.31514      0   6.827   &lt;1e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 13724  on 9900  degrees of freedom\n##  Residual Deviance:  2064  on 9898  degrees of freedom\n##  \n## AIC: 2068  BIC: 2083  (Smaller is better. MC Std. Err. = 0.8083)\n\nAs expected, reciprocity is significant (we made it like this!.) Notwithstanding, there is a difference between this model and the previous one. This model was not fitted using MLE. Instead, since the reciprocity term involves more than one tie, the model cannot be reduced to a Logistic regression, so it needs to be estimated using one of the other available estimation methods in the ergm package.\nThe model starts gaining complexity as we add higher-order terms involving more ties. An infamous example is the number of triangles. Although highly important for social sciences, including triangle terms in your ERGMs results in a degenerate model (when the MCMC chain jumps between empty and fully connected graphs). One exception is if you deal with small networks. To address this, Snijders et al. (2006) and Hunter (2007) introduced various new terms that significantly reduce the risk of degeneracy. Here, we will illustrate the use of the term “geometrically weighted dyadic shared partner” (gwdsp,) which Prof. David Hunter proposed. The gwdsp term is akin to triadic closure but reduces the chances of degeneracy.\n\n## Fitting two more models (output message suppressed)\nmodel_3 &lt;- ergm(Y ~ edges + mutual + gwdsp(.5, fixed = TRUE))\n## model_4 &lt;- ergm(Y ~ edges + triangles) # bad idea\n\nRight after fitting a model, we want to inspect the results. An excellent tool for this is the R package texreg (Leifeld 2013):\n\nlibrary(texreg)\nscreenreg(list(model_1, model_2, model_3))\n\n\n=============================================================\n                     Model 1       Model 2       Model 3     \n-------------------------------------------------------------\nedges                   -3.79 ***     -3.93 ***     -3.84 ***\n                        (0.07)        (0.08)        (0.21)   \nmutual                                 2.15 ***      2.14 ***\n                                      (0.32)        (0.29)   \ngwdsp.OTP.fixed.0.5                                 -0.02    \n                                                    (0.05)   \n-------------------------------------------------------------\nAIC                   2104.43       2068.22       2071.94    \nBIC                   2111.63       2082.62       2093.54    \nLog Likelihood       -1051.22      -1032.11      -1032.97    \n=============================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nSo far, model_2 is winning. We will continue with this model."
  },
  {
    "objectID": "05-intro-to-ergms.html#lets-add-a-little-bit-of-structure",
    "href": "05-intro-to-ergms.html#lets-add-a-little-bit-of-structure",
    "title": "6  Introduction to ERGMs",
    "section": "6.9 Let’s add a little bit of structure",
    "text": "6.9 Let’s add a little bit of structure\nNow that we only have a model featuring endogenous terms, we can add vertex/edge-covariates. Starting with \"fav_music\", there are a couple of different ways to use this node feature:\n\nDirectly through homophily (assortative mixing): Using the nodematch term, we can control for the propensity of individuals to connect based on shared music taste.\nHomophily (v2): We could activate the option diff = TRUE using the same term. By doing this, the homophily term is operationalized differently, adding as many terms as options in the vertex attribute.\nMixing: We can use the term nodemix for individuals’ tendency to mix between musical tastes.\n\n\n\n\n\n\n\nQuestion\n\n\n\nIn this context, what would be the different hypotheses behind each decision?\n\n\n\nmodel_4 &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\"))\nmodel_5 &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\", diff = TRUE))\nmodel_6 &lt;- ergm(Y ~ edges + mutual + nodemix(\"fav_music\"))\n\nNow, let’s inspect what we have so far:\n\nscreenreg(list(`2` = model_2, `4` = model_4, `5` = model_5, `6` = model_6))\n\n\n================================================================================\n                          2             4             5             6           \n--------------------------------------------------------------------------------\nedges                        -3.93 ***     -4.29 ***     -4.29 ***     -3.56 ***\n                             (0.08)        (0.11)        (0.11)        (0.24)   \nmutual                        2.15 ***      1.99 ***      2.00 ***      2.02 ***\n                             (0.32)        (0.30)        (0.30)        (0.30)   \nnodematch.fav_music                         0.85 ***                            \n                                           (0.14)                               \nnodematch.fav_music.jazz                                  0.74 **               \n                                                         (0.25)                 \nnodematch.fav_music.pop                                   0.82 ***              \n                                                         (0.18)                 \nnodematch.fav_music.rock                                  0.87 ***              \n                                                         (0.17)                 \nmix.fav_music.pop.jazz                                                 -0.54    \n                                                                       (0.34)   \nmix.fav_music.rock.jazz                                                -0.75 *  \n                                                                       (0.38)   \nmix.fav_music.jazz.pop                                                 -0.60    \n                                                                       (0.35)   \nmix.fav_music.pop.pop                                                   0.10    \n                                                                       (0.27)   \nmix.fav_music.rock.pop                                                 -0.50    \n                                                                       (0.31)   \nmix.fav_music.jazz.rock                                                -1.40 ** \n                                                                       (0.44)   \nmix.fav_music.pop.rock                                                 -0.86 *  \n                                                                       (0.33)   \nmix.fav_music.rock.rock                                                 0.15    \n                                                                       (0.27)   \n--------------------------------------------------------------------------------\nAIC                        2068.22       2030.85       2033.57       2036.37    \nBIC                        2082.62       2052.45       2069.57       2108.38    \nLog Likelihood            -1032.11      -1012.42      -1011.79      -1008.19    \n================================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nAlthough model 5 has a higher loglikelihood, using AIC or BIC suggests model 4 is a better candidate. For the sake of time, we will jump ahead and add nodematch(\"female\") as the last term of our model. The next step is to assess (a) convergence and (b) goodness-of-fit.\n\nmodel_final &lt;- ergm(Y ~ edges + mutual + nodematch(\"fav_music\") + nodematch(\"female\"))\n\n## Printing the pretty table\nscreenreg(list(`2` = model_2, `4` = model_4, `Final` = model_final))\n\n\n============================================================\n                     2             4             Final      \n------------------------------------------------------------\nedges                   -3.93 ***     -4.29 ***    -3.95 ***\n                        (0.08)        (0.11)       (0.12)   \nmutual                   2.15 ***      1.99 ***     1.86 ***\n                        (0.32)        (0.30)       (0.33)   \nnodematch.fav_music                    0.85 ***     0.81 ***\n                                      (0.14)       (0.14)   \nnodematch.female                                   -0.74 ***\n                                                   (0.15)   \n------------------------------------------------------------\nAIC                   2068.22       2030.85      2002.18    \nBIC                   2082.62       2052.45      2030.98    \nLog Likelihood       -1032.11      -1012.42      -997.09    \n============================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05"
  },
  {
    "objectID": "05-intro-to-ergms.html#more-about-ergms",
    "href": "05-intro-to-ergms.html#more-about-ergms",
    "title": "6  Introduction to ERGMs",
    "section": "6.10 More about ERGMs",
    "text": "6.10 More about ERGMs\nWe have shown here just a glimpse of what ERG models are. A large, active, collaborative community of social network scientists is working on new extensions and advances. If you want to learn more about ERGMs, I recommend the following resources:\n\nThe Statnet website (link).\nThe book “Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications” by Lusher, Koskinen, and Robins (2013).\nMelnet’s PNEt website (link).\n\n\n\n\n\nFrank, O, and David Strauss. 1986. “Markov graphs.” Journal of the American Statistical Association 81 (395): 832–42. https://doi.org/10.2307/2289017.\n\n\nGeyer, Charles J., and Elizabeth A. Thompson. 1992. “Constrained Monte Carlo Maximum Likelihood for Dependent Data.” Journal of the Royal Statistical Society. Series B (Methodological) 54 (3): 657–99. https://www.jstor.org/stable/2345852.\n\n\nHolland, Paul W., and Samuel Leinhardt. 1981. “An exponential family of probability distributions for directed graphs.” Journal of the American Statistical Association 76 (373): 33–50. https://doi.org/10.2307/2287037.\n\n\nHunter, David R. 2007. “Curved Exponential Family Models for Social Networks.” Social Networks 29 (2): 216–30. https://doi.org/10.1016/j.socnet.2006.08.005.\n\n\nLeifeld, Philip. 2013. “Texreg : Conversion of Statistical Model Output in R to L A T E X and HTML Tables.” Journal of Statistical Software 55 (8). https://doi.org/10.18637/jss.v055.i08.\n\n\nLusher, Dean, Johan Koskinen, and Garry Robins. 2013. Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications. Cambridge University Press.\n\n\nRobins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007. “An introduction to exponential random graph (p*) models for social networks.” Social Networks 29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002.\n\n\nSnijders, Tom A B, Philippa E Pattison, Garry L Robins, and Mark S Handcock. 2006. “New specifications for exponential random graph models.” Sociological Methodology 36 (1): 99–153. https://doi.org/10.1111/j.1467-9531.2006.00176.x.\n\n\nVega Yon, George G., Andrew Slaughter, and Kayla de la Haye. 2021. “Exponential Random Graph Models for Little Networks.” Social Networks 64 (August 2020): 225–38. https://doi.org/10.1016/j.socnet.2020.07.005.\n\n\nWasserman, Stanley, and Philippa Pattison. 1996. “Logit models and logistic regressions for social networks: I. An introduction to Markov graphs and p*.” Psychometrika 61 (3): 401–25. https://doi.org/10.1007/BF02294547."
  },
  {
    "objectID": "05-intro-to-ergms.html#footnotes",
    "href": "05-intro-to-ergms.html#footnotes",
    "title": "6  Introduction to ERGMs",
    "section": "",
    "text": "While ERG Models can be used to predict individual ties (which is another way of describing them), the focus is on the processes that give origin to the network.↩︎"
  },
  {
    "objectID": "06-ergms-cont.html#convergence",
    "href": "06-ergms-cont.html#convergence",
    "title": "7  Convergence on ERGMs",
    "section": "7.1 Convergence",
    "text": "7.1 Convergence\nAs our model was fitted using MCMC, we must ensure the chains converged. We can use the mcmc.diagnostics function from the ergm package to check model convergence. This function looks at the last set of simulations of the MCMC model and generates various diagnostics for the user.\nUnder the hood, the fitting algorithm generates a stream of networks based on those parameters for each new proposed set of model parameters. The last stream of networks is thus simulated using the final state of the model. The mcmc.diagnostics function takes that stream of networks and plots the sequence of the sufficient statistics included in the model. A converged model should show a stationary statistics sequence, moving around a fixed value without (a) becoming stuck at any point and (b) chaining the tendency. This model shows both:\n\nmcmc.diagnostics(model_final, which   = c(\"plots\"))\n\n\n\n\n\n\n\n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nNow that we know our model was good enough to represent the observed statistics (sample them, actually,) let’s see how good it is at capturing other features of the network that were not included in the model."
  },
  {
    "objectID": "06-ergms-cont.html#goodness-of-fit",
    "href": "06-ergms-cont.html#goodness-of-fit",
    "title": "7  Convergence on ERGMs",
    "section": "7.2 Goodness-of-fit",
    "text": "7.2 Goodness-of-fit\nThis would be the last step in the sequence of steps to fit an ERGM. As we mentioned before, the idea of Goodness-of-fit [GOF] in ERG models is to see how well our model captures other properties of the graph that were not included in the model. By default, the gof function from the ergm package computes GOF for:\n\nThe model statistics.\nThe outdegree distribution.\nThe indegree distribution.\nThe distribution of edge-wise shared partners.\nThe distribution of the geodesic distances (shortest path).\n\nThe process of evaluating GOF is relatively straightforward. Using samples from the posterior distribution, we check whether the observed statistics from above are covered (fall within the CI) of our model. If they do, we say that the model has a good fit. Otherwise, if we observe significant anomalies, we return to the bench and try to improve our model.\nAs with all simulated data, our gof() call shows that our selected model was an excellent choice for the observed graph:\n\ngof_final &lt;- gof(model_final)\nprint(gof_final)\n## \n## Goodness-of-fit for in-degree \n## \n##           obs min  mean max MC p-value\n## idegree0   11   3 10.93  20       1.00\n## idegree1   22  16 24.80  34       0.58\n## idegree2   23  17 26.54  36       0.52\n## idegree3   30   9 19.41  31       0.02\n## idegree4   10   4 10.94  20       0.94\n## idegree5    3   1  4.97  11       0.46\n## idegree6    1   0  1.68   5       0.96\n## idegree7    0   0  0.58   3       1.00\n## idegree8    0   0  0.14   1       1.00\n## idegree10   0   0  0.01   1       1.00\n## \n## Goodness-of-fit for out-degree \n## \n##           obs min  mean max MC p-value\n## odegree0   10   3 10.71  18       1.00\n## odegree1   30  13 24.67  35       0.28\n## odegree2   23  18 27.50  40       0.38\n## odegree3   17  12 19.01  28       0.74\n## odegree4   10   3 10.72  18       1.00\n## odegree5    8   1  4.84  10       0.12\n## odegree6    2   0  1.79   6       1.00\n## odegree7    0   0  0.57   3       1.00\n## odegree8    0   0  0.15   2       1.00\n## odegree9    0   0  0.03   1       1.00\n## odegree10   0   0  0.01   1       1.00\n## \n## Goodness-of-fit for edgewise shared partner \n## \n##          obs min   mean max MC p-value\n## esp.OTP0 212 178 209.21 241       0.78\n## esp.OTP1   7   3  10.47  22       0.44\n## esp.OTP2   0   0   0.40   3       1.00\n## \n## Goodness-of-fit for minimum geodesic distance \n## \n##      obs min    mean  max MC p-value\n## 1    219 190  220.08  255       0.94\n## 2    449 326  459.25  619       0.98\n## 3    790 499  842.85 1183       0.90\n## 4   1151 708 1252.11 1960       0.80\n## 5   1245 775 1396.75 2266       0.66\n## 6   1043 656 1181.38 1680       0.44\n## 7    745 499  802.34 1211       0.72\n## 8    434 194  466.85  791       0.86\n## 9    198  57  244.78  569       0.72\n## 10    80  11  121.93  408       0.76\n## 11    10   1   60.50  285       0.42\n## 12     1   0   30.83  251       0.46\n## 13     0   0   15.96  186       0.74\n## 14     0   0    8.42  133       1.00\n## 15     0   0    5.02   96       1.00\n## 16     0   0    3.03   65       1.00\n## 17     0   0    1.83   60       1.00\n## 18     0   0    1.05   47       1.00\n## 19     0   0    0.54   23       1.00\n## 20     0   0    0.18    7       1.00\n## 21     0   0    0.07    3       1.00\n## 22     0   0    0.01    1       1.00\n## Inf 3535 776 2784.24 4680       0.38\n## \n## Goodness-of-fit for model statistics \n## \n##                     obs min   mean max MC p-value\n## edges               219 190 220.08 255       0.94\n## mutual               16   9  16.36  25       1.00\n## nodematch.fav_music 123  97 123.55 152       1.00\n## nodematch.female     72  49  71.53  90       0.90\n\nIt is easier to see the results using the plot function:\n\n## Plotting the result (5 figures)\nop &lt;- par(mfrow = c(3,2))\nplot(gof_final)\npar(op)"
  },
  {
    "objectID": "07-odd-balls.html#non-parametric-statistics",
    "href": "07-odd-balls.html#non-parametric-statistics",
    "title": "8  Odd balls",
    "section": "8.1 Non-parametric statistics",
    "text": "8.1 Non-parametric statistics\nNon-parametric statistics are statistical methods in which parametric assumptions such as probability function or model parameters are not made. In other words, non-parametric statistics are distribution-free methods. With many non-parametric methods, simulation-based methods are the most commonly used. The following papers show a couple of examples using non-parametric methods applied to network inference problems."
  },
  {
    "objectID": "07-odd-balls.html#case-1-social-mimicry",
    "href": "07-odd-balls.html#case-1-social-mimicry",
    "title": "8  Odd balls",
    "section": "8.2 Case 1: Social mimicry",
    "text": "8.2 Case 1: Social mimicry\nIn Bell et al. (2019), we investigated the prevalence of social mimicry in families while eating. Social mimicry is a widely studied phenomenon. And while some statistical methods try to measure it, we approach the problem differently. Instead of assuming mimicry and measuring it right away, we started by asking if there was mimicry in the first place.\nThe study featured an experiment that consisted of the following:\n\nFamilies were invited to a lab and were asked to eat a meal together.\nUsing accelerometers, we time-stamped the action of taking a bite.\nData was then cleaned and processed to obtain the time stamps of each bite (verified using video recordings of the event).\n\nThe null hypothesis was then: “The sequence of bites for Person i and j are independent (i.e., no mimicry).” To test this hypothesis, we used a permutation test. The test consisted of the following:\n\nFor each pair of persons i and j, we calculated the average time gap between their bites.\nWe then generated a null distribution by shuffling the time blocks of each person, i.e., for each dyad:\n\nFor each individual, we computed the sequence of time gaps between bites (time between bites each person took).\nUsing that sequence, we then simulated a new sequence of bites for each individual, fixing the individual level time gap distribution.\nOnce shuffled, we then computed the average time gap per dyad.\n\nFinally, we used the generated null distribution to assess the prevalence of social mimicry. The next figure shows the results for one dyad:\n\n\n\n\nNull distribution of average time gap between person i and person j – reproduced from Bell et al. (2019)"
  },
  {
    "objectID": "07-odd-balls.html#case-2-imaginary-motifs",
    "href": "07-odd-balls.html#case-2-imaginary-motifs",
    "title": "8  Odd balls",
    "section": "8.3 Case 2: Imaginary motifs",
    "text": "8.3 Case 2: Imaginary motifs\nIn Tanaka and Vega Yon (2024), we study the prevalence of perception-based network motifs. While the ERGM framework would be a natural choice, as a first approach, we used non-parametric tests for hypothesis testing. The rest of this section is a reproduction of the methods section of the paper:\n\n\nThe process can be described as follows:\n\\begin{equation}\n    \\text{Pr}\\left(\\left.\\vphantom{t_{ij} = 1}p_{ij} = 1\\right|t_{ij} = 1\\right) = \\left\\{\\begin{array}{ll}%\n    |\\sum_{\\oplus(k)}t_{nm}|^{-1}\\sum_{\\oplus(k)}p_{nm}t_{nm} &\\text{if}\\; k \\in \\{i, j\\} \\\\\n    |\\sum_{\\oplus(k)^{\\complement}}t_{nm}|^{-1}\\sum_{\\oplus(k)^\\complement}p_{nm}t_{nm} &\\text{otherwise},\n    \\end{array}\\right.\n\\end{equation}\n\\begin{equation}\n    \\text{Pr}\\left(\\left.\\vphantom{t_{ij} = 0}p_{ij} = 0\\right|t_{ij} = 0\\right) = \\left\\{\\begin{array}{ll}%\n    |\\sum_{\\oplus(k)}(1-t_{nm})|^{-1}\\sum_{\\oplus(k)}(1-p_{nm}t_{nm}) &\\text{if}\\; k \\in \\{i, j\\} \\\\\n    |\\sum_{\\oplus(k)^{\\complement}}(1-t_{nm})|^{-1}\\sum_{\\oplus(k)^\\complement}(1-p_{nm}t_{nm}) &\\text{otherwise},\n    \\end{array}\\right.\n\\end{equation}\nwhere, \\oplus(k) \\equiv \\left\\{(m,n) : (m = k) \\oplus (n = k)\\right\\} is the set of ties involving k, and \\oplus(k)^\\complement is its complement.\nEach sampled perceived graph was \\mathbf{P}^b_l–-th sampled graph for each individual . We then counted the dyadic imaginary network motifs, \\tau_l^b \\equiv s\\left(\\mathbf{T}, \\mathbf{P}^b\\right); the same was done to the observed graphs, \\hat{\\tau}_l \\equiv s\\left(\\mathbf{T}, \\mathbf{P}\\right). Finally, we calculated a -value using the equal-tail nonparametric test drawing B = 2,000 samples recommend 1,000 minimum) from the null distribution, comparing the observed imaginary motif counts, \\hat\\tau_l, and what we would expect to see by chance, \\{\\tau^b_l\\}_{b=1}^B:\n\\begin{equation}\n    \\hat{p}(\\hat{\\tau_l}) = 2\\times \\min\\left[\\frac{1}{B}\\sum_{b}I(\\tau^b_l \\leq \\hat{\\tau_l}), \\frac{1}{B}\\sum_{b}I(\\tau^b_l &gt; \\hat{\\tau_l})\\right]\n\\end{equation}\n\n\n\n\n\nBell, Brooke M., Donna Spruijt-Metz, George G. Vega Yon, Abu S. Mondol, Ridwan Alam, Meiyi Ma, Ifat Emi, John Lach, John A. Stankovic, and Kayla De La Haye. 2019. “Sensing Eating Mimicry Among Family Members.” Translational Behavioral Medicine. https://doi.org/10.1093/tbm/ibz051.\n\n\nTanaka, Kyosuke, and George G. Vega Yon. 2024. “Imaginary Network Motifs: Structural Patterns of False Positives and Negatives in Social Networks.” Social Networks 78 (July): 65–80. https://doi.org/10.1016/j.socnet.2023.11.005."
  },
  {
    "objectID": "08-lab-2.html",
    "href": "08-lab-2.html",
    "title": "9  Lab II",
    "section": "",
    "text": "The ergm package comes with a handful of vignettes (extended R examples) that you can use to learn more about the package. The vignette with the same name as the package shows an example fitting a model to the network faux.mesa.high. Address the following questions:\n\nWhat type of ERG model is this?\nLooking at the plot in the vignette, what other term(s) could you consider worth adding to the model? Why?\nThe vignette did not include a goodness-of-fit analysis. Can you add one? What do you find?\nChallenge: without using ergm to fit the model, estimate the following p1 model: ~ edges + nodefactor(\"Race\"). Compare your results with what you get using ergm."
  },
  {
    "objectID": "09-advanced-ergms.html#introduction",
    "href": "09-advanced-ergms.html#introduction",
    "title": "10  Advanced ERGMs: Constraints",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nFor this section, we will dive in into ERGM constranints. Using constraints, you will be able to modify the sampling space of the model to things such as:\n\nPool (multilevel) models.\nAccounting for data generating process.\nMake your model behave (with caution.)\nEven fit Discrete-Exponential Family Models (DEFM.)\n\nWe will start with formally understanding what constraining the space means and then continue with some examples."
  },
  {
    "objectID": "09-advanced-ergms.html#constraining-ergms",
    "href": "09-advanced-ergms.html#constraining-ergms",
    "title": "10  Advanced ERGMs: Constraints",
    "section": "10.2 Constraining ERGMs",
    "text": "10.2 Constraining ERGMs\nExponential Random Graph Models [ERGMs] can represent a variety of network classes. We often look at “regular” social networks like school students, colleagues in the workplace, or families. Nonetheless, some social networks we study have features that restrict how connections can occur. Typical examples are bi-partite graphs and multilevel networks. There are two classes of vertices in bi-partite networks, and ties can only occur between classes. On the other hand, multilevel networks may feature multiple classes with inter-class ties that are somewhat restricted. Structural constraints exist in both cases, meaning some configurations may not be plausible.\nMathematically, what we are trying to do is, instead of assuming that all network configurations are possible:\n\n\\left\\{\\bm{{y}} \\in \\mathcal{Y}: y_{ij} = 0, \\forall i = j\\right\\}\n\nwe want to go a bit further avoiding loops, namely:\n\n\\left\\{\\bm{{y}} \\in \\mathcal{Y}: y_{ij} = 0, \\forall i = j; \\mathbf{y} \\in C\\right\\},\n\nwhere C is a constraint, for example, only networks with no triangles. The ergm R package has built-in capabilities to deal with some of these cases. Nonetheless, we can specify models with arbitrary structural constraints built into the model. The key is in using offset terms."
  },
  {
    "objectID": "09-advanced-ergms.html#example-1-pool-block-diagonalmultilevel-ergm",
    "href": "09-advanced-ergms.html#example-1-pool-block-diagonalmultilevel-ergm",
    "title": "10  Advanced ERGMs: Constraints",
    "section": "10.3 Example 1: Pool (block-diagonal/multilevel) ERGM",
    "text": "10.3 Example 1: Pool (block-diagonal/multilevel) ERGM\nNowadays, multi-network studies featuring a large sample of networks are easy to see. When we deal with multiple networks, there are usually two approaches to analyzing them: (a) estimate separate ERGMs and do a meta-analysis like in [ANN CITATION], or (b) fit a pooled ERGM. Here, we will follow the latter.\nPooled ERGMs resemble typical statistical methods. Assuming that the networks are independent, we can estimate a model like the following:\n\nP_{\\mathcal{Y}, \\bm{{\\theta}}}(\\bm{{Y}}_1=\\bm{{y}}_1, \\bm{{Y}}_2=\\bm{{y}}_2, \\dots, \\bm{{Y}}_1=\\bm{{y}}_L) = \\prod_l^L \\exp\\left(\\bm{{\\theta}}^{\\mathbf{t}} s(y_l)\\right)\\kappa_l(\\bm{{\\theta}})^{-1}\n\nIn this case, we assume that all networks have the same data-generating process, meaning they all come from the same \\mathcal{Y}. If that’s not the case and, like in any regression analysis, we could write a hierarchical model where model parameters come from the same distribution or fit a random effects ERGM as described in Slaughter and Koehly (2016).\nWe have a handful of different ways of fitting pooled ERGMs. If you are dealing with small networks (about six nodes if directed and eight if undirected,) you can use the ergmito R package (G. G. Vega Yon, Slaughter, and Haye 2021; G. Vega Yon 2020). The ergm package can still assist you with simple models if you have larger networks. But if you want to leverage the power of having multiple networks, the recently published ergm.multi R package should be your choice (Krivitsky, Coletti, and Hens 2023; Krivitsky 2023). Since we are learning to use constraints, we will use the ergm package. Let’s start with an example from the ergmito R package:\n\nlibrary(sna)\nlibrary(ergm)\nlibrary(ergmito)\n\n## Loading five artificial networks (5 networks)\ndata(fivenets)\n\n## Taking a look at the first two \nfivenets[1:2]\n## [[1]]\n##  Network attributes:\n##   vertices = 4 \n##   directed = TRUE \n##   hyper = FALSE \n##   loops = FALSE \n##   multiple = FALSE \n##   bipartite = FALSE \n##   total edges= 2 \n##     missing edges= 0 \n##     non-missing edges= 2 \n## \n##  Vertex attribute names: \n##     female name \n## \n## No edge attributes\n## \n## [[2]]\n##  Network attributes:\n##   vertices = 4 \n##   directed = TRUE \n##   hyper = FALSE \n##   loops = FALSE \n##   multiple = FALSE \n##   bipartite = FALSE \n##   total edges= 7 \n##     missing edges= 0 \n##     non-missing edges= 7 \n## \n##  Vertex attribute names: \n##     female name \n## \n## No edge attributes\n\nWe can also visualize these five networks:\n\n## Quickly visualizing fine networks\nop &lt;- par(mfrow = c(2, 3), mar = c(1,1,1,1)/2)\nres &lt;- lapply(fivenets, \\(x) {gplot(x); box()})\npar(op)\n\n\n\n\nThese five networks were generated using an edges+gender homophily model. Since all the graphs in the list are small (five vertices each,) we can fit a pooled ERGM using the ergmito function from the package of the same name:\n\nsummary(ergmito(fivenets ~ edges + nodematch(\"female\")))\n\n\nERGMito estimates (MLE)\nThis model includes 5 networks.\n\nformula:\n  fivenets ~ edges + nodematch(\"female\")\n\n                 Estimate Std. Error z value Pr(&gt;|z|)   \nedges            -1.70475    0.54356 -3.1363 0.001711 **\nnodematch.female  1.58697    0.64305  2.4679 0.013592 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAIC: 73.34109    BIC: 77.52978    (Smaller is better.) \n\n\n\n\n\n\n\n\nWho cares about small networks?\n\n\n\nYou may be wondering why someone would care about small networks. Well, the truth is that small networks are the fabric of societies (think about family nucleous,) and are more complicated than you think. A directed graph with five vertices has about 1,000,000 possible configurations.\n\n\n\n\n\n\n\n\nHPC in small networks\n\n\n\nThe key of ERGMitos is that, with a small enough support size, we can skip using MCMC and simulation-based approximations and directly calculate likelihoods for pooled models. This is extremely powerful as it opens the door to diverse statistical methods.\n\n\nThe question is: How can we use constraints to fit the same model using the ergm package? We have to follow these steps:\n\nWe must combine all the networks into a single network object.\nSpecify the constraint via the constraint argument in the ergm function. In this case, use the blockdiag constraint (check out the manual ?ergm.constraints to learn more about what’s available with the tool).\nFit the model!\n\nThe ergmito package has a convenient function that will take any list of network objects (or matrices) and turn it into a single network. The passed networks don’t need to have the same size. After combining the networks, the function creates a new vertex attribute that holds the id of the original group.\n\n## Combining the networks\nbd &lt;- blockdiagonalize(fivenets)\nbd\n\n Network attributes:\n  vertices = 20 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 20 \n    missing edges= 0 \n    non-missing edges= 20 \n\n Vertex attribute names: \n    block female name_original vertex.names \n\nNo edge attributes\n\n\nJust out of curiosity, what would we get if we fitted a regular ERGM to this data? Here is the answer:\n\nergm(bd ~ edges + nodematch(\"female\"))\n\n\nCall:\nergm(formula = bd ~ edges + nodematch(\"female\"))\n\nMaximum Likelihood Coefficients:\n           edges  nodematch.female  \n          -3.882             1.542  \n\n\ni.e.*, biased estimates. Although the homophily term is very similar to the original estimate, the edges parameter is twice the size. The estimate is inflated because the support of this model includes networks where there are ties between the groups, yet these networks cannot be connected one another.\nTo solve this issue, as we said, we use the constraints:\n\nergm(bd ~ edges + nodematch(\"female\"), constraints = ~ blockdiag(\"block\"))\n\n\nCall:\nergm(formula = bd ~ edges + nodematch(\"female\"), constraints = ~blockdiag(\"block\"))\n\nMaximum Likelihood Coefficients:\n           edges  nodematch.female  \n          -1.705             1.587  \n\n\nSuccess! This model is now correctly specified. Although it seems like we have a single network, by imposing the blockdiagonal constraint, we are fitting a pooled model. The ergm package comes with more alternatives to restricting the sample space of your model, but there may be a case where you need more. For those cases, the offset comes in handy."
  },
  {
    "objectID": "09-advanced-ergms.html#example-2-interlocking-egos-and-disconnected-alterslaura",
    "href": "09-advanced-ergms.html#example-2-interlocking-egos-and-disconnected-alterslaura",
    "title": "10  Advanced ERGMs: Constraints",
    "section": "10.4 Example 2: Interlocking egos and disconnected alters1",
    "text": "10.4 Example 2: Interlocking egos and disconnected alters1\nImagine that we have two sets of vertices. The first, group E, are egos in an egocentric study. The second group, called A, is composed of people mentioned by egos in E but were not surveyed. Assume that individuals in A can only connect to individuals in E; moreover, individuals in E have no restrictions on connecting. In other words, only two types of ties exist: E-E and A-E. The question is now, how can we enforce such a constraint in an ERGM?\nUsing offsets, and in particular, setting coefficients to -Inf provides an easy way to restrict the support set of ERGMs. For example, if we wanted to constrain the support to include networks with no triangles, we would add the term offset(triangle) and use the option offset.coef = -Inf to indicate that realizations including triangles are not possible. Using R:\nergm(net ~ edges + offset(triangle), offset.coef = -Inf)\nIn this model, a Bernoulli graph, we reduce the sample space to networks with no triangles. In our example, such a statistic should only take non-zero values whenever ties within the A class happen. We can use the nodematch() term to do that. Formally\n\n\\text{NodeMatch}(x) = \\sum_{i,j} y_{ij} \\mathbf{1}({x_{i} = x_{j}})\n\nThis statistic will sum over all ties in which source (i) and target (j)’s X attribute is equal. One way to make this happen is by creating an auxiliary variable that equals, e.g., 0 for all vertices in A, and a unique value different from zero otherwise. For example, if we had 2 As and three Es, the data would look like \\{0,0,1,2,3\\}. The following code block creates an empty graph with 50 nodes, 10 of which are in group E (ego).\n\nlibrary(ergm, quietly =  TRUE)\nlibrary(sna, quietly =  TRUE)\n\nn &lt;- 50\nn_egos &lt;- 10\nnet &lt;- as.network(matrix(0, ncol = n, nrow = n), directed = TRUE)\n\n## Let's assing the groups\nnet %v% \"is.ego\" &lt;- c(rep(TRUE, n_egos), rep(FALSE, n - n_egos))\nnet %v% \"is.ego\"\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE\n\ngplot(net, vertex.col = net %v% \"is.ego\")\n\n\n\n\nTo create the auxiliary variable, we will use the following function:\n\n## Function that creates an aux variable for the ergm model\nmake_aux_var &lt;- function(my_net, is_ego_dummy) {\n  \n  n_vertex &lt;- length(my_net %v% is_ego_dummy)\n  n_ego_   &lt;- sum(my_net %v% is_ego_dummy)\n  \n  # Creating an auxiliary variable to identify the non-informant non-informant ties\n  my_net %v% \"aux_var\" &lt;- ifelse(\n    !my_net %v% is_ego_dummy, 0, 1:(n_vertex - n_ego_)\n    )\n\n  my_net\n}\n\nCalling the function in our data results in the following:\n\nnet &lt;- make_aux_var(net, \"is.ego\")\n\n## Taking a look over the first 15 rows of data\ncbind(\n  Is_Ego = net %v% \"is.ego\",\n  Aux    = net %v% \"aux_var\"  \n) |&gt; head(n = 15)\n\n      Is_Ego Aux\n [1,]      1   1\n [2,]      1   2\n [3,]      1   3\n [4,]      1   4\n [5,]      1   5\n [6,]      1   6\n [7,]      1   7\n [8,]      1   8\n [9,]      1   9\n[10,]      1  10\n[11,]      0   0\n[12,]      0   0\n[13,]      0   0\n[14,]      0   0\n[15,]      0   0\n\n\nWe can now use this data to simulate a network in which ties between A-class vertices are not possible:\n\nset.seed(2828)\nnet_sim &lt;- simulate(net ~ edges + nodematch(\"aux_var\"), coef = c(-3.0, -Inf))\ngplot(net_sim, vertex.col = net_sim %v% \"is.ego\")\n\n\n\n\nAs you can see, this network has only ties of the type E-E and A-E. We can double-check by (i) looking at the counts and (ii) visualizing each induced-subgraph separately:\n\nsummary(net_sim ~ edges + nodematch(\"aux_var\"))\n\n            edges nodematch.aux_var \n               49                 0 \n\nnet_of_alters &lt;- get.inducedSubgraph(\n  net_sim, which((net_sim %v% \"aux_var\") == 0)\n  )\n\nnet_of_egos &lt;- get.inducedSubgraph(\n  net_sim, which((net_sim %v% \"aux_var\") != 0)\n  )\n\n## Counts\nsummary(net_of_alters ~ edges + nodematch(\"aux_var\"))\n\n            edges nodematch.aux_var \n                0                 0 \n\nsummary(net_of_egos ~ edges + nodematch(\"aux_var\"))\n\n            edges nodematch.aux_var \n                1                 0 \n\n## Figures\nop &lt;- par(mfcol = c(1, 2))\ngplot(net_of_alters, vertex.col = net_of_alters %v% \"is.ego\", main = \"A\")\ngplot(net_of_egos, vertex.col = net_of_egos %v% \"is.ego\", main = \"E\")\n\n\n\npar(op)\n\nNow, to fit an ERGM with this constraint, we simply need to make use of the offset terms. Here is an example:\n\nans &lt;- ergm(\n  net_sim ~ edges + offset(nodematch(\"aux_var\")), # The model (notice the offset)\n  offset.coef = -Inf                              # The offset coefficient\n  )\n## Starting maximum pseudolikelihood estimation (MPLE):\n## Obtaining the responsible dyads.\n## Evaluating the predictor and response matrix.\n## Maximizing the pseudolikelihood.\n## Finished MPLE.\n## Evaluating log-likelihood at the estimate.\nsummary(ans)\n## Call:\n## ergm(formula = net_sim ~ edges + offset(nodematch(\"aux_var\")), \n##     offset.coef = -Inf)\n## \n## Maximum Likelihood Results:\n## \n##                           Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \n## edges                       -2.843      0.147      0  -19.34   &lt;1e-04 ***\n## offset(nodematch.aux_var)     -Inf      0.000      0    -Inf   &lt;1e-04 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 1233.8  on 890  degrees of freedom\n##  Residual Deviance:  379.4  on 888  degrees of freedom\n##  \n## AIC: 381.4  BIC: 386.2  (Smaller is better. MC Std. Err. = 0)\n## \n##  The following terms are fixed by offset and are not estimated:\n##   offset(nodematch.aux_var)\n\nThis ERGM model–which by the way only featured dyadic-independent terms, and thus can be reduced to a logistic regression–restricts the support by excluding all networks in which ties within the class A exists. To finalize, let’s look at a few simulations based on this model:\n\nset.seed(1323)\nop &lt;- par(mfcol = c(2,2), mar = rep(1, 4))\nfor (i in 1:4) {\n  gplot(simulate(ans), vertex.col = net %v% \"is.ego\", vertex.cex = 2)\n  box()\n}\n\n\n\npar(op)\n\nAll networks with no ties between A nodes."
  },
  {
    "objectID": "09-advanced-ergms.html#example-3-bi-partite-networks",
    "href": "09-advanced-ergms.html#example-3-bi-partite-networks",
    "title": "10  Advanced ERGMs: Constraints",
    "section": "10.5 Example 3: Bi-partite networks",
    "text": "10.5 Example 3: Bi-partite networks\nIn the case of bipartite networks (sometimes called affiliation networks,) we can use ergm’s terms for bipartite graphs to corroborate what we discussed here. For example, the two-star term. Let’s start simulating a bipartite network using the edges and two-star parameters. Since the k-star term is usually complex to fit (tends to generate degenerate models,) we will take advantage of the Log() transformation function in the ergm package to smooth the term.2\nThe bipartite network that we will be simulating will have 100 actors and 50 entities. Actors, which we will map to the first level of the ergm terms, this is, b1star b1nodematch, etc. will send ties to the entities, the second level of the bipartite ERGM. To create a bipartite network, we will create an empty matrix of size nactors x nentitites; thus, actors are represented by rows and entities by columns.\n\n## Parameters for the simulation\nnactors   &lt;- 100\nnentities &lt;- floor(nactors/2)\nn         &lt;- nactors + nentities\n\n## Creating an empty bipartite network (baseline)\nnet_b &lt;- network(\n  matrix(0, nrow = nactors, ncol = nentities), bipartite = TRUE\n)\n\n## Simulating the bipartite ERGM,\nnet_b &lt;- simulate(net_b ~ edges + Log(~b1star(2)), coef = c(-3, 1.5), seed = 55)\n\nLet’s see what we got here:\n\nsummary(net_b ~ edges + Log(~b1star(2)))\n\n      edges Log~b1star2 \n 245.000000    5.746203 \n\nnetplot::nplot(net_b, vertex.col = (1:n &lt;= nactors) + 1)\n\n\n\n\nNotice that the first nactors vertices in the network are the actors, and the remaining are the entities. Now, although the ergm package features bipartite network terms, we can still fit a bipartite ERGM without explicitly declaring the graph as such. In such case, the b1star(2) term of a bipartite network is equivalent to an ostar(2) in a directed graph. Likewise, b2star(2) in a bipartite graph matches the istar(2) term in a directed graph. This information will be relevant when fitting the ERGM. Let’s transform the bipartite network into a directed graph. The following code block does so:\n\n## Identifying the edges\nnet_not_b &lt;- which(as.matrix(net_b) != 0, arr.ind = TRUE)\n\n## We need to offset the endpoint of the ties by nactors\n## so that the ids go from 1 through (nactors + nentitites)\nnet_not_b[,2] &lt;- net_not_b[,2] + nactors\n\n## The resulting graph is a directed network\nnet_not_b &lt;- network(net_not_b, directed = TRUE)\n\nNow we are almost done. As before, we need to use node-level covariates to put the constraints in our model. For this ERGM to reflect an ERGM on a bipartite network, we need two constraints:\n\nOnly ties from actors to entities are allowed, and\nentities can only receive ties.\n\nThe corresponding offset terms for this model are: nodematch(\"is.actor\") ~ -Inf, and nodeocov(\"isnot.actor\") ~ -Inf. Mathematically:\n\\begin{align*}\n\\text{NodeMatch(x = \"is.actor\")} &= \\sum_{i&lt;j} y_{ij}\\mathbb{1}\\left(x_i = x_j\\right) \\\\\n\\text{NodeOCov(x = \"isnot.actor\")} &= \\sum_{i} x_i \\times \\sum_{j&lt;i} y_{ij}\n\\end{align*}\nIn other words, we are setting that ties between nodes of the same class are forbidden, and outgoing ties are forbidden for entities. Let’s create the vertex attributes needed to use the aforementioned terms:\n\nnet_not_b %v% \"is.actor\" &lt;- as.integer(1:n &lt;= nactors)\nnet_not_b %v% \"isnot.actor\" &lt;- as.integer(1:n &gt; nactors)\n\nFinally, to make sure we have done all well, let’s look how both networks–bipartite and unimodal–look side by side:\n\n## First, let's get the layout\nfig &lt;- netplot::nplot(net_b, vertex.col = (1:n &lt;= nactors) + 1)\ngridExtra::grid.arrange(\n  fig,\n  netplot::nplot(\n    net_not_b, vertex.col = (1:n &lt;= nactors) + 1,\n    layout = fig$.layout\n     ),\n  ncol = 2, nrow = 1\n)\n\n\n\n## Looking at the counts\nsummary(net_b ~ edges + b1star(2) + b2star(2))\n\n  edges b1star2 b2star2 \n    245     313     645 \n\nsummary(net_not_b ~ edges + ostar(2) + istar(2))\n\n edges ostar2 istar2 \n   245    313    645 \n\n\nWith the two networks matching, we can now fit the ERGMs with and without offset terms and compare the results between the two models:\n\n## ERGM with a bipartite graph\nres_b     &lt;- ergm(\n  # Main formula\n  net_b ~ edges + Log(~b1star(2)),\n\n  # Control parameters\n  control = control.ergm(seed = 1)\n  )\n\n## ERGM with a digraph with constraints\nres_not_b &lt;- ergm(\n  # Main formula\n  net_not_b ~ edges + Log(~ostar(2)) +\n\n  # Offset terms \n  offset(nodematch(\"is.actor\")) + offset(nodeocov(\"isnot.actor\")),\n  offset.coef = c(-Inf, -Inf),\n\n  # Control parameters\n  control = control.ergm(seed = 1)\n  )\n\nHere are the estimates (using the texreg R package for a prettier output):\n\ntexreg::screenreg(list(Bipartite = res_b, Directed = res_not_b))\n\n\n======================================================================\n                              Bipartite    Directed                   \n----------------------------------------------------------------------\nedges                           -3.14 ***                    -3.11 ***\n                                (0.15)                       (0.14)   \nLog~b1star2                     21.89                                 \n                               (17.13)                                \nLog~ostar2                                                   19.66    \n                                                            (16.75)   \noffset(nodematch.is.actor)                                    -Inf    \n                                                                      \noffset(nodeocov.isnot.actor)                                  -Inf    \n                                                                      \n----------------------------------------------------------------------\nAIC                           1958.00      -2134192392498170112.00    \nBIC                           1971.03      -2134192392498170112.00    \nLog Likelihood                -977.00       1067096196249085056.00    \n======================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nAs expected, both models yield the “same” estimate. The minor differences observed between the models are how the ergm package performs the sampling. In particular, in the bipartite case, ergm has special routines for making the sampling more efficient, having a higher acceptance rate than that of the model in which the bipartite graph was not explicitly declared. We can tell this by inspecting rejection rates:\n\ndata.frame(\n  Bipartite = coda::rejectionRate(res_b$sample[[1]]) * 100,\n  Directed  = coda::rejectionRate(res_not_b$sample[[1]][, -c(3,4)]) * 100\n) |&gt; knitr::kable(digits = 2, caption = \"Rejection rate (percent)\")\n\n\nRejection rate (percent)\n\n\n\nBipartite\nDirected\n\n\n\n\nedges\n2.48\n3.67\n\n\nLog~b1star2\n1.24\n2.04\n\n\n\n\n\nThe ERGM fitted with the offset terms has a much higher rejection rate than that of the ERGM fitted with the bipartite ERGM.\nFinally, the fact that we can fit ERGMs using offset does not mean that we need to use it ALL the time. Unless there is a very good reason to go around ergm’s capabilities, I wouldn’t recommend fitting bipartite ERGMs as we just did, as the authors of the package have included (MANY) features to make our job easier.\n\n\n\n\nKrivitsky, Pavel N. 2023. Ergm.multi: Fit, Simulate and Diagnose Exponential-Family Models for Multiple or Multilayer Networks. The Statnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.multi.\n\n\nKrivitsky, Pavel N., Pietro Coletti, and Niel Hens. 2023. “A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks.” Journal of the American Statistical Association 0 (0): 1–21. https://doi.org/10.1080/01621459.2023.2242627.\n\n\nSlaughter, Andrew J., and Laura M. Koehly. 2016. “Multilevel Models for Social Networks: Hierarchical Bayesian Approaches to Exponential Random Graph Modeling.” Social Networks 44: 334–45. https://doi.org/10.1016/j.socnet.2015.11.002.\n\n\nVega Yon, George. 2020. ergmito: Exponential Random Graph Models for Small Networks. CRAN. https://cran.r-project.org/package=ergmito.\n\n\nVega Yon, George G., Andrew Slaughter, and Kayla de la Haye. 2021. “Exponential Random Graph Models for Little Networks.” Social Networks 64 (August 2020): 225–38. https://doi.org/10.1016/j.socnet.2020.07.005."
  },
  {
    "objectID": "09-advanced-ergms.html#footnotes",
    "href": "09-advanced-ergms.html#footnotes",
    "title": "10  Advanced ERGMs: Constraints",
    "section": "",
    "text": "Thanks to Laura Koehly, who devised this complicated model.↩︎\nAfter writing this example, it became apparent the use of the Log() transformation function may not be ideal. Since many terms used in ERGMs can be zero, e.g., triangles, the term Log(~ ostar(2)) is undefined when ostar(2) = 0. In practice, the ERGM package sets a lower limit for the log of 0, so, instead of having Log(0) ~ -Inf, they set it to be a really large negative number. This causes all sorts of issues to the estimates; in our example, an overestimation of the population parameter and a positive log-likelihood. Therefore, I wouldn’t recommend using this transformation too often.↩︎"
  },
  {
    "objectID": "10-new-topics.html#overview",
    "href": "10-new-topics.html#overview",
    "title": "11  New topics in network modeling",
    "section": "11.1 Overview",
    "text": "11.1 Overview\n\nWith more data and computing resources, the things that we can ask and do with networks are becoming increasingly (even more) exciting and complex.\nIn this section, I will introduce some of the latest advancements and forthcoming topics in network modeling."
  },
  {
    "objectID": "10-new-topics.html#part-i-new-models-and-extensions",
    "href": "10-new-topics.html#part-i-new-models-and-extensions",
    "title": "11  New topics in network modeling",
    "section": "11.2 Part I: New models and extensions",
    "text": "11.2 Part I: New models and extensions"
  },
  {
    "objectID": "10-new-topics.html#mutli-ergms",
    "href": "10-new-topics.html#mutli-ergms",
    "title": "11  New topics in network modeling",
    "section": "11.3 Mutli-ERGMs",
    "text": "11.3 Mutli-ERGMs\n\nIn Krivitsky, Coletti, and Hens (2023a), the authors present a start-to-finish pooled ERGM example featuring heterogeneous data sources.\nThey increase power and allow exploring heterogeneous effects across types/classes of networks."
  },
  {
    "objectID": "10-new-topics.html#statistical-power-of-soam",
    "href": "10-new-topics.html#statistical-power-of-soam",
    "title": "11  New topics in network modeling",
    "section": "11.4 Statistical power of SOAM",
    "text": "11.4 Statistical power of SOAM\n\n\nSOAM Stadtfeld et al. (2020) proposes ways to perform power analysis for Siena models. At the center of their six-step approach is simulation."
  },
  {
    "objectID": "10-new-topics.html#bayesian-alaam",
    "href": "10-new-topics.html#bayesian-alaam",
    "title": "11  New topics in network modeling",
    "section": "11.5 Bayesian ALAAM",
    "text": "11.5 Bayesian ALAAM\nEver wondered how to model influence exclusively?\n\nThe Auto-Logistic Actor Attribute Model [ALAAM] is a model that allows us to do just that.\nKoskinen and Daraganova (2022) extends the ALAAM model to a Bayesian framework.\nIt provides greater flexibility to accommodate more complicated models and add extensions such as hierarchical models.\n\n\n\n\nFigure 1 reproduced from A. D. Stivala et al. (2020)"
  },
  {
    "objectID": "10-new-topics.html#relational-event-models",
    "href": "10-new-topics.html#relational-event-models",
    "title": "11  New topics in network modeling",
    "section": "11.6 Relational Event Models",
    "text": "11.6 Relational Event Models\n\nREMs are great for modeling sequences of ties (instead of panel or cross-sectional.)\nButts et al. (2023) provides a general overview of Relational Event Models [REMs,] new methods, and future steps.\n\n\n\n\nFigure 3 reproduced from Brandenberger (2020)"
  },
  {
    "objectID": "10-new-topics.html#big-ergms",
    "href": "10-new-topics.html#big-ergms",
    "title": "11  New topics in network modeling",
    "section": "11.7 Big ERGMs",
    "text": "11.7 Big ERGMs\n\nERGMs In A. Stivala, Robins, and Lomi (2020), a new method is proposed to estimate large ERGMs (featuring millions of nodes).\n\n\n\n\nPartial map of the Internet based on the January 15, 2005 data found on opte.org. – Wiki"
  },
  {
    "objectID": "10-new-topics.html#exponential-random-network-models",
    "href": "10-new-topics.html#exponential-random-network-models",
    "title": "11  New topics in network modeling",
    "section": "11.8 Exponential Random Network Models",
    "text": "11.8 Exponential Random Network Models\n\nWang, Fellows, and Handcock recently published a re-introduction of the ERNM framework (Wang, Fellows, and Handcock 2023).\nERNMs generalize ERGMs to incorporate behavior and are the cross-sectional causing of SIENA models.\n\n\\begin{align*}\n\\text{ER\\textbf{G}M}: & P_{\\mathcal{Y}, \\bm{{\\theta}}}(\\bm{{Y}}=\\bm{{y}} | \\bm{{X}}=\\bm{{x}}) \\\\\n\\text{ER\\textbf{N}M}: & P_{\\mathcal{Y}, \\bm{{\\theta}}}(\\bm{{Y}}=\\bm{{y}}, \\bm{{X}}=\\bm{{x}})\n\\end{align*}"
  },
  {
    "objectID": "10-new-topics.html#part-ii-shameless-self-promotion",
    "href": "10-new-topics.html#part-ii-shameless-self-promotion",
    "title": "11  New topics in network modeling",
    "section": "11.9 Part II: Shameless self-promotion",
    "text": "11.9 Part II: Shameless self-promotion"
  },
  {
    "objectID": "10-new-topics.html#ergmitos-small-ergms",
    "href": "10-new-topics.html#ergmitos-small-ergms",
    "title": "11  New topics in network modeling",
    "section": "11.10 ERGMitos: Small ERGMs",
    "text": "11.10 ERGMitos: Small ERGMs\n\nERGMitos1 (Vega Yon, Slaughter, and Haye 2021) leverage small network sizes to use exact statistics.\n\n\n\n\nFive small networks from the ergmito R package"
  },
  {
    "objectID": "10-new-topics.html#discrete-exponential-family-models",
    "href": "10-new-topics.html#discrete-exponential-family-models",
    "title": "11  New topics in network modeling",
    "section": "11.11 Discrete Exponential-family Models",
    "text": "11.11 Discrete Exponential-family Models\n\nERGMs are a particular case of Random Markov fields.\nWe can use the ERGM framework for modeling vectors of binary outcomes, e.g., the consumption of \\{tobacco, MJ, alcohol\\}"
  },
  {
    "objectID": "10-new-topics.html#power-analysis-in-ergms",
    "href": "10-new-topics.html#power-analysis-in-ergms",
    "title": "11  New topics in network modeling",
    "section": "11.12 Power analysis in ERGMs",
    "text": "11.12 Power analysis in ERGMs\n\nUsing conditional ERGMs (closely related to constrained), we can do power analysis for network samples (Vega Yon 2023).\n\n\n\n\nReproduced from Krivitsky, Coletti, and Hens (2023b)"
  },
  {
    "objectID": "10-new-topics.html#two-step-estimation-ergms",
    "href": "10-new-topics.html#two-step-estimation-ergms",
    "title": "11  New topics in network modeling",
    "section": "11.13 Two-step estimation ERGMs",
    "text": "11.13 Two-step estimation ERGMs\n\n\nConditioning the ERGM on an observed statistic “drops” the associated coefficient.\nHypothesis: As n increases, conditional ERGM estimates are consistent with the full model:\n\n\n\n\n\nSimulation study trying to demonstrate the concept (Work in progress)"
  },
  {
    "objectID": "10-new-topics.html#thanks",
    "href": "10-new-topics.html#thanks",
    "title": "11  New topics in network modeling",
    "section": "11.14 Thanks!",
    "text": "11.14 Thanks!"
  },
  {
    "objectID": "10-new-topics.html#bonus-track-why-network-scientists-dont-use-ergms",
    "href": "10-new-topics.html#bonus-track-why-network-scientists-dont-use-ergms",
    "title": "11  New topics in network modeling",
    "section": "11.15 Bonus track: Why network scientists don’t use ERGMs?",
    "text": "11.15 Bonus track: Why network scientists don’t use ERGMs?\n\nAttempts to overcome these problems by extending the blockmodel have focused particularly on the use of (more complicated) p^* or exponential random graph models, but while these are conceptually appealing, they quickly lose the analytic tractability of the original blockmodel as their complexity increases.\n– Karrer and Newman (2011)\n\n\n\n\n\nBrandenberger, Laurence. 2020. “Interdependencies in Conflict Dynamics: Analyzing Endogenous Patterns in Conflict Event Data Using Relational Event Models.” In Computational Conflict Research, edited by Emanuel Deutschmann, Jan Lorenz, Luis G. Nardin, Davide Natalini, and Adalbert F. X. Wilhelm, 67–80. Computational Social Sciences. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-29333-8_4.\n\n\nButts, Carter T., Alessandro Lomi, Tom A. B. Snijders, and Christoph Stadtfeld. 2023. “Relational Event Models in Network Science.” Network Science 11 (2): 175–83. https://doi.org/10.1017/nws.2023.9.\n\n\nKarrer, Brian, and M. E. J. Newman. 2011. “Stochastic Blockmodels and Community Structure in Networks.” Physical Review E 83 (1): 016107. https://doi.org/10.1103/PhysRevE.83.016107.\n\n\nKoskinen, Johan, and Galina Daraganova. 2022. “Bayesian Analysis of Social Influence.” Journal of the Royal Statistical Society Series A: Statistics in Society 185 (4): 1855–81. https://doi.org/10.1111/rssa.12844.\n\n\nKrivitsky, Pavel N., Pietro Coletti, and Niel Hens. 2023a. “A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks.” Journal of the American Statistical Association 0 (0): 1–21. https://doi.org/10.1080/01621459.2023.2242627.\n\n\n———. 2023b. “Rejoinder to Discussion of ‘A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’.” Journal of the American Statistical Association 118 (544): 2235–38. https://doi.org/10.1080/01621459.2023.2280383.\n\n\nStadtfeld, Christoph, Tom A. B. Snijders, Christian Steglich, and Marijtje van Duijn. 2020. “Statistical Power in Longitudinal Network Studies.” Sociological Methods & Research 49 (4): 1103–32. https://doi.org/10.1177/0049124118769113.\n\n\nStivala, Alex D., H. Colin Gallagher, David A. Rolls, Peng Wang, and Garry L. Robins. 2020. “Using Sampled Network Data With The Autologistic Actor Attribute Model.” arXiv. https://doi.org/10.48550/arXiv.2002.00849.\n\n\nStivala, Alex, Garry Robins, and Alessandro Lomi. 2020. “Exponential Random Graph Model Parameter Estimation for Very Large Directed Networks.” PLoS ONE 15 (1): 1–23. https://doi.org/10.1371/journal.pone.0227804.\n\n\nVega Yon, George G. 2023. “Power and Multicollinearity in Small Networks: A Discussion of ‘Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks’ by Krivitsky, Coletti & Hens.” Journal Of The American Statistical Association.\n\n\nVega Yon, George G., Andrew Slaughter, and Kayla de la Haye. 2021. “Exponential Random Graph Models for Little Networks.” Social Networks 64 (August 2020): 225–38. https://doi.org/10.1016/j.socnet.2020.07.005.\n\n\nWang, Zeyi, Ian E. Fellows, and Mark S. Handcock. 2023. “Understanding Networks with Exponential-Family Random Network Models.” Social Networks, August, S0378873323000497. https://doi.org/10.1016/j.socnet.2023.07.003."
  },
  {
    "objectID": "10-new-topics.html#footnotes",
    "href": "10-new-topics.html#footnotes",
    "title": "11  New topics in network modeling",
    "section": "",
    "text": "From the Spanish suffix meaning small.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "“1.1: Basic Definitions and\nConcepts.” 2014. Statistics LibreTexts.\nhttps://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/01%3A_Introduction_to_Statistics/1.01%3A_Basic_Definitions_and_Concepts.\n\n\nBarrett, Tyson, Matt Dowle, and Arun Srinivasan. 2023. Data.table:\nExtension of ‘Data.frame‘. https://r-datatable.com.\n\n\nBell, Brooke M., Donna Spruijt-Metz, George G. Vega Yon, Abu S. Mondol,\nRidwan Alam, Meiyi Ma, Ifat Emi, John Lach, John A. Stankovic, and Kayla\nDe La Haye. 2019. “Sensing Eating Mimicry Among Family\nMembers.” Translational Behavioral Medicine. https://doi.org/10.1093/tbm/ibz051.\n\n\nBrandenberger, Laurence. 2020. “Interdependencies in\nConflict Dynamics: Analyzing Endogenous\nPatterns in Conflict Event Data Using Relational Event\nModels.” In Computational Conflict\nResearch, edited by Emanuel Deutschmann, Jan Lorenz, Luis G.\nNardin, Davide Natalini, and Adalbert F. X. Wilhelm, 67–80.\nComputational Social Sciences. Cham:\nSpringer International Publishing. https://doi.org/10.1007/978-3-030-29333-8_4.\n\n\nButts, Carter T., Alessandro Lomi, Tom A. B. Snijders, and Christoph\nStadtfeld. 2023. “Relational Event Models in Network\nScience.” Network Science 11 (2): 175–83. https://doi.org/10.1017/nws.2023.9.\n\n\nCasella, George, and Roger L. Berger. 2021. Statistical\nInference. Cengage Learning.\n\n\nFellows, Ian E. 2012. “Exponential Family Random Network\nModels.” ProQuest Dissertations and Theses. PhD thesis.\nhttps://login.ezproxy.lib.utah.edu/login?url=https://www.proquest.com/dissertations-theses/exponential-family-random-network-models/docview/1221548720/se-2.\n\n\nFrank, O, and David Strauss. 1986. “Markov\ngraphs.” Journal of the American Statistical\nAssociation 81 (395): 832–42. https://doi.org/10.2307/2289017.\n\n\nGelman, Andrew. 2018. “The Failure of Null\nHypothesis Significance Testing When Studying Incremental\nChanges, and What to Do About\nIt.” Personality and Social Psychology Bulletin\n44 (1): 16–23. https://doi.org/10.1177/0146167217729162.\n\n\nGeyer, Charles J., and Elizabeth A. Thompson. 1992. “Constrained\nMonte Carlo Maximum Likelihood for Dependent\nData.” Journal of the Royal Statistical Society.\nSeries B (Methodological) 54 (3): 657–99. https://www.jstor.org/stable/2345852.\n\n\nGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin,\nCharles Poole, Steven N. Goodman, and Douglas G. Altman. 2016.\n“Statistical Tests, P Values, Confidence Intervals,\nand Power: A Guide to Misinterpretations.” European Journal\nof Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nHandcock, Mark S., David R. Hunter, Carter T. Butts, Steven M. Goodreau,\nPavel N. Krivitsky, and Martina Morris. 2023. Ergm: Fit, Simulate\nand Diagnose Exponential-Family Models for Networks. The Statnet\nProject (https://statnet.org). https://CRAN.R-project.org/package=ergm.\n\n\nHaye, Kayla de la, Heesung Shin, George G. Vega Yon, and Thomas W.\nValente. 2019. “Smoking Diffusion Through\nNetworks of Diverse, Urban American\nAdolescents over the High School Period.”\nJournal of Health and Social Behavior. https://doi.org/10.1177/0022146519870521.\n\n\nHolland, Paul W., and Samuel Leinhardt. 1981. “An exponential family of probability distributions for\ndirected graphs.” Journal of the American Statistical\nAssociation 76 (373): 33–50. https://doi.org/10.2307/2287037.\n\n\nHunter, David R. 2007. “Curved Exponential Family Models for\nSocial Networks.” Social Networks 29 (2): 216–30. https://doi.org/10.1016/j.socnet.2006.08.005.\n\n\nHunter, David R., Mark S. Handcock, Carter T. Butts, Steven M. Goodreau,\nand Martina Morris. 2008. “ergm: A\nPackage to Fit, Simulate and Diagnose Exponential-Family Models for\nNetworks.” Journal of Statistical Software 24 (3): 1–29.\nhttps://doi.org/10.18637/jss.v024.i03.\n\n\nKarrer, Brian, and M. E. J. Newman. 2011. “Stochastic Blockmodels\nand Community Structure in Networks.” Physical Review E\n83 (1): 016107. https://doi.org/10.1103/PhysRevE.83.016107.\n\n\nKoskinen, Johan, and Galina Daraganova. 2022. “Bayesian\nAnalysis of Social Influence.”\nJournal of the Royal Statistical Society Series A: Statistics in\nSociety 185 (4): 1855–81. https://doi.org/10.1111/rssa.12844.\n\n\nKrivitsky, Pavel N. 2023. Ergm.multi: Fit, Simulate and Diagnose\nExponential-Family Models for Multiple or Multilayer Networks. The\nStatnet Project (https://statnet.org). https://CRAN.R-project.org/package=ergm.multi.\n\n\nKrivitsky, Pavel N., Pietro Coletti, and Niel Hens. 2023a. “A\nTale of Two Datasets:\nRepresentativeness and Generalisability of\nInference for Samples of\nNetworks.” Journal of the American Statistical\nAssociation 0 (0): 1–21. https://doi.org/10.1080/01621459.2023.2242627.\n\n\n———. 2023b. “Rejoinder to Discussion of\n‘A Tale of Two Datasets:\nRepresentativeness and Generalisability of\nInference for Samples of\nNetworks’.” Journal of the American\nStatistical Association 118 (544): 2235–38. https://doi.org/10.1080/01621459.2023.2280383.\n\n\nLeifeld, Philip. 2013. “Texreg :\nConversion of Statistical Model Output in\nR to L A T E X and HTML\nTables.” Journal of Statistical Software 55 (8).\nhttps://doi.org/10.18637/jss.v055.i08.\n\n\nLeSage, James P. 2008. “An Introduction to\nSpatial Econometrics.” Revue\nd’économie Industrielle 123 (123): 19–44. https://doi.org/10.4000/rei.3887.\n\n\nLeSage, James P., and R. Kelley Pace. 2014. “The Biggest\nMyth in Spatial Econometrics.”\nEconometrics 2 (4): 217–49. https://doi.org/10.2139/ssrn.1725503.\n\n\nLusher, Dean, Johan Koskinen, and Garry Robins. 2013. Exponential\nRandom Graph Models for Social Networks:\nTheory, Methods, and\nApplications. Cambridge University Press.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRobins, Garry, Philippa Pattison, and Peter Elliott. 2001.\n“Network Models for Social Influence Processes.”\nPsychometrika 66 (2): 161–89. https://doi.org/10.1007/BF02294834.\n\n\nRobins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007.\n“An introduction to exponential random graph\n(p*) models for social networks.” Social Networks\n29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002.\n\n\nRoger Bivand. 2022. “R Packages for Analyzing Spatial Data: A\nComparative Case Study with Areal Data.” Geographical\nAnalysis 54 (3): 488–518. https://doi.org/10.1111/gean.12319.\n\n\nSlaughter, Andrew J., and Laura M. Koehly. 2016. “Multilevel\nModels for Social Networks: Hierarchical Bayesian Approaches to\nExponential Random Graph Modeling.” Social Networks 44:\n334–45. https://doi.org/10.1016/j.socnet.2015.11.002.\n\n\nSnijders, Tom a B. 1996. “Stochastic Actor-Oriented Models for\nNetwork Change.” The Journal of Mathematical Sociology\n21 (1-2): 149–72. https://doi.org/10.1080/0022250X.1996.9990178.\n\n\nSnijders, Tom A B, Philippa E Pattison, Garry L Robins, and Mark S\nHandcock. 2006. “New specifications for\nexponential random graph models.” Sociological\nMethodology 36 (1): 99–153. https://doi.org/10.1111/j.1467-9531.2006.00176.x.\n\n\nSnijders, Tom A. B. 2017. “Stochastic Actor-Oriented\nModels for Network Dynamics.” Annual\nReview of Statistics and Its Application 4 (1): 343–63. https://doi.org/10.1146/annurev-statistics-060116-054035.\n\n\nStadtfeld, Christoph, Tom A. B. Snijders, Christian Steglich, and\nMarijtje van Duijn. 2020. “Statistical Power in\nLongitudinal Network Studies.” Sociological\nMethods & Research 49 (4): 1103–32. https://doi.org/10.1177/0049124118769113.\n\n\nStivala, Alex D., H. Colin Gallagher, David A. Rolls, Peng Wang, and\nGarry L. Robins. 2020. “Using Sampled Network Data With The\nAutologistic Actor Attribute Model.” arXiv.\nhttps://doi.org/10.48550/arXiv.2002.00849.\n\n\nStivala, Alex, Garry Robins, and Alessandro Lomi. 2020.\n“Exponential Random Graph Model Parameter Estimation for Very\nLarge Directed Networks.” PLoS ONE 15 (1): 1–23. https://doi.org/10.1371/journal.pone.0227804.\n\n\nTanaka, Kyosuke, and George G. Vega Yon. 2024. “Imaginary Network\nMotifs: Structural Patterns of False Positives and\nNegatives in Social Networks.” Social Networks 78\n(July): 65–80. https://doi.org/10.1016/j.socnet.2023.11.005.\n\n\nValente, Thomas W., and George G. Vega Yon. 2020.\n“Diffusion/Contagion Processes on Social\nNetworks.” Health Education & Behavior 47\n(2): 235–48. https://doi.org/10.1177/1090198120901497.\n\n\nValente, Thomas W., Heather Wipfli, and George G. Vega Yon. 2019.\n“Network Influences on Policy Implementation:\nEvidence from a Global Health Treaty.” Social\nScience and Medicine. https://doi.org/10.1016/j.socscimed.2019.01.008.\n\n\nVega Yon, George. 2020. ergmito: Exponential\nRandom Graph Models for Small Networks. CRAN.\nhttps://cran.r-project.org/package=ergmito.\n\n\nVega Yon, George G. 2023. “Power and Multicollinearity in Small\nNetworks: A Discussion of ‘Tale of\nTwo Datasets: Representativeness and\nGeneralisability of Inference for\nSamples of Networks’ by\nKrivitsky, Coletti &\nHens.” Journal Of The American Statistical\nAssociation.\n\n\nVega Yon, George G., Andrew Slaughter, and Kayla de la Haye. 2021.\n“Exponential Random Graph Models for Little Networks.”\nSocial Networks 64 (August 2020): 225–38. https://doi.org/10.1016/j.socnet.2020.07.005.\n\n\nWang, Zeyi, Ian E. Fellows, and Mark S. Handcock. 2023.\n“Understanding Networks with Exponential-Family Random Network\nModels.” Social Networks, August, S0378873323000497. https://doi.org/10.1016/j.socnet.2023.07.003.\n\n\nWasserman, Stanley, and Philippa Pattison. 1996. “Logit models and logistic regressions for social\nnetworks: I. An introduction to Markov graphs and p*.”\nPsychometrika 61 (3): 401–25. https://doi.org/10.1007/BF02294547."
  }
]